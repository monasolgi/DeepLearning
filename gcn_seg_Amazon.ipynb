{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_seg_Amazon",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMeYYhZbyAdFsSW9R9we2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monasolgi/DeepLearning/blob/master/gcn_seg_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY7L10jJt1dd",
        "outputId": "594fbecd-93d3-4112-bb2a-38772b70dc6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "npz_data=np.load('/content/amazon_electronics_computers (1).npz')\n",
        "npz_data.files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ7zOlTHyRtR"
      },
      "source": [
        "#utils\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer, normalize\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),\n",
        "                                        shape=npz_data['attr_shape'])\n",
        "  \n",
        " adj= sp.coo_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),\n",
        "                                   shape=npz_data['adj_shape']))\n",
        "  \n",
        "\n",
        " labels=npz_data['labels']\n",
        " labels=binarize_labels(labels)\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        " idx_train = range(10000)\n",
        " idx_val = range(10000, 11500)\n",
        " idx_test = range(11500, 13381)\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " labels = torch.LongTensor(np.where(labels)[1])\n",
        " adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        " \n",
        " return adj, features, labels, idx_train, idx_val, idx_test\n",
        " \n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def binarize_labels(labels, sparse_output=False, return_classes=False):\n",
        "    if hasattr(labels[0], '__iter__'):  # labels[0] is iterable <=> multilabel format\n",
        "        binarizer = MultiLabelBinarizer(sparse_output=sparse_output)\n",
        "    else:\n",
        "        binarizer = LabelBinarizer(sparse_output=sparse_output)\n",
        "    label_matrix = binarizer.fit_transform(labels).astype(np.float32)\n",
        "    return (label_matrix, binarizer.classes_) if return_classes else label_matrix\n",
        " \n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQntzDnmOyUN"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "813otPOyFSLe",
        "outputId": "4be4efd0-fe19-4964-d62b-6b29a440356e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(indices=tensor([[    0,   507,  6551,  ..., 13019, 13121, 13751],\n",
              "                        [    0,     0,     0,  ..., 13751, 13751, 13751]]),\n",
              "        values=tensor([0.2000, 0.0070, 0.0032,  ..., 0.0081, 0.0370, 0.0333]),\n",
              "        size=(13752, 13752), nnz=505474, layout=torch.sparse_coo),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 1., 0.,  ..., 0., 1., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [1., 1., 0.,  ..., 1., 1., 0.]]),\n",
              " tensor([4, 4, 8,  ..., 8, 4, 0]),\n",
              " tensor([   0,    1,    2,  ..., 9997, 9998, 9999]),\n",
              " tensor([10000, 10001, 10002,  ..., 11497, 11498, 11499]),\n",
              " tensor([11500, 11501, 11502,  ..., 13378, 13379, 13380]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iarFyQvXDCp6"
      },
      "source": [
        "#layers\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter ,param haro cache mikone\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        \n",
        "        #Sparse matrix multiplication=spmm\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ') '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMeYJ5fQDGSN"
      },
      "source": [
        "#models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphConvolution\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        #self.gc2 = GraphConvolution(nhid, nhid2)\n",
        "        #self.gc3 = GraphConvolution(nhid2, nclass)\n",
        "        \n",
        "        #adding a 3th layer didn't outperform\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        #can also use tanh,not very different result\n",
        "        \n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        #return F.log_softmax(x, dim=1)\n",
        "        #print(x.size())\n",
        "    \n",
        "        x = self.gc2(x, adj)\n",
        "        \n",
        "        #x = self.gc3(x, adj)\n",
        "        #return x\n",
        "       # print(x.size())\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH8lnw8CDRwG",
        "outputId": "5b4c488f-df7b-4115-d32d-0be535a07be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#train\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torchsummary import summary\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "#import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#from utils import load_data, accuracy\n",
        "#from models import GCN\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "\n",
        "#parser.add_argument('--hidden2', type=int, default=8,\n",
        "                    #help='Number of hidden units.')\n",
        "\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=10,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "## to set cuda as your device if possible\n",
        "##training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "    \n",
        "    ## train:adjust the weights on the neural network\n",
        "    ## validation:used to minimize overfitting\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    \n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    \n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    ## Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "print(model)\n",
        "\n",
        "#summary(model,[(13381,767),(13381,13381)])\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 3.0389 acc_train: 0.0951 loss_val: 2.7131 acc_val: 0.1553 time: 0.1707s\n",
            "Epoch: 0002 loss_train: 2.7622 acc_train: 0.1112 loss_val: 2.3863 acc_val: 0.0353 time: 0.1699s\n",
            "Epoch: 0003 loss_train: 2.4400 acc_train: 0.0630 loss_val: 2.2620 acc_val: 0.3527 time: 0.1802s\n",
            "Epoch: 0004 loss_train: 2.3125 acc_train: 0.3289 loss_val: 2.1631 acc_val: 0.3820 time: 0.1693s\n",
            "Epoch: 0005 loss_train: 2.2410 acc_train: 0.3561 loss_val: 2.0329 acc_val: 0.3833 time: 0.1748s\n",
            "Epoch: 0006 loss_train: 2.0842 acc_train: 0.3627 loss_val: 1.9107 acc_val: 0.3853 time: 0.1744s\n",
            "Epoch: 0007 loss_train: 1.9773 acc_train: 0.3629 loss_val: 1.8489 acc_val: 0.3853 time: 0.1822s\n",
            "Epoch: 0008 loss_train: 1.8919 acc_train: 0.3632 loss_val: 1.8067 acc_val: 0.3913 time: 0.1675s\n",
            "Epoch: 0009 loss_train: 1.8323 acc_train: 0.3723 loss_val: 1.7782 acc_val: 0.4147 time: 0.1740s\n",
            "Epoch: 0010 loss_train: 1.8231 acc_train: 0.3910 loss_val: 1.7574 acc_val: 0.4847 time: 0.1712s\n",
            "Epoch: 0011 loss_train: 1.7823 acc_train: 0.4571 loss_val: 1.6961 acc_val: 0.4033 time: 0.1816s\n",
            "Epoch: 0012 loss_train: 1.7432 acc_train: 0.3796 loss_val: 1.6284 acc_val: 0.4133 time: 0.1792s\n",
            "Epoch: 0013 loss_train: 1.6679 acc_train: 0.3967 loss_val: 1.5685 acc_val: 0.4193 time: 0.1755s\n",
            "Epoch: 0014 loss_train: 1.6142 acc_train: 0.4119 loss_val: 1.5271 acc_val: 0.4360 time: 0.1516s\n",
            "Epoch: 0015 loss_train: 1.5764 acc_train: 0.4338 loss_val: 1.4950 acc_val: 0.4347 time: 0.1557s\n",
            "Epoch: 0016 loss_train: 1.5638 acc_train: 0.4348 loss_val: 1.4671 acc_val: 0.4427 time: 0.1519s\n",
            "Epoch: 0017 loss_train: 1.5313 acc_train: 0.4406 loss_val: 1.4175 acc_val: 0.4873 time: 0.1536s\n",
            "Epoch: 0018 loss_train: 1.4658 acc_train: 0.4784 loss_val: 1.3768 acc_val: 0.5800 time: 0.1535s\n",
            "Epoch: 0019 loss_train: 1.4215 acc_train: 0.5355 loss_val: 1.3456 acc_val: 0.5847 time: 0.1526s\n",
            "Epoch: 0020 loss_train: 1.3829 acc_train: 0.5612 loss_val: 1.3154 acc_val: 0.5727 time: 0.1499s\n",
            "Epoch: 0021 loss_train: 1.3622 acc_train: 0.5470 loss_val: 1.2840 acc_val: 0.6100 time: 0.1686s\n",
            "Epoch: 0022 loss_train: 1.3248 acc_train: 0.5945 loss_val: 1.2558 acc_val: 0.6267 time: 0.1572s\n",
            "Epoch: 0023 loss_train: 1.3108 acc_train: 0.6036 loss_val: 1.2270 acc_val: 0.6307 time: 0.1549s\n",
            "Epoch: 0024 loss_train: 1.2819 acc_train: 0.6075 loss_val: 1.1891 acc_val: 0.6367 time: 0.1573s\n",
            "Epoch: 0025 loss_train: 1.2367 acc_train: 0.6153 loss_val: 1.1619 acc_val: 0.6433 time: 0.1542s\n",
            "Epoch: 0026 loss_train: 1.2111 acc_train: 0.6201 loss_val: 1.1473 acc_val: 0.6407 time: 0.1500s\n",
            "Epoch: 0027 loss_train: 1.1884 acc_train: 0.6278 loss_val: 1.1259 acc_val: 0.6440 time: 0.1511s\n",
            "Epoch: 0028 loss_train: 1.1782 acc_train: 0.6265 loss_val: 1.1009 acc_val: 0.6427 time: 0.1568s\n",
            "Epoch: 0029 loss_train: 1.1448 acc_train: 0.6354 loss_val: 1.0724 acc_val: 0.6460 time: 0.1519s\n",
            "Epoch: 0030 loss_train: 1.1308 acc_train: 0.6316 loss_val: 1.0329 acc_val: 0.6533 time: 0.1522s\n",
            "Epoch: 0031 loss_train: 1.0875 acc_train: 0.6396 loss_val: 1.0121 acc_val: 0.6660 time: 0.1555s\n",
            "Epoch: 0032 loss_train: 1.0813 acc_train: 0.6487 loss_val: 0.9947 acc_val: 0.6613 time: 0.1488s\n",
            "Epoch: 0033 loss_train: 1.0577 acc_train: 0.6431 loss_val: 0.9717 acc_val: 0.6760 time: 0.1687s\n",
            "Epoch: 0034 loss_train: 1.0378 acc_train: 0.6572 loss_val: 0.9356 acc_val: 0.6967 time: 0.1731s\n",
            "Epoch: 0035 loss_train: 1.0273 acc_train: 0.6772 loss_val: 0.9143 acc_val: 0.7140 time: 0.1814s\n",
            "Epoch: 0036 loss_train: 0.9879 acc_train: 0.6929 loss_val: 0.9004 acc_val: 0.7107 time: 0.1714s\n",
            "Epoch: 0037 loss_train: 0.9650 acc_train: 0.6985 loss_val: 0.8818 acc_val: 0.7273 time: 0.1785s\n",
            "Epoch: 0038 loss_train: 0.9598 acc_train: 0.6929 loss_val: 0.8708 acc_val: 0.7247 time: 0.1710s\n",
            "Epoch: 0039 loss_train: 0.9309 acc_train: 0.6980 loss_val: 0.8485 acc_val: 0.7373 time: 0.1707s\n",
            "Epoch: 0040 loss_train: 0.9151 acc_train: 0.7097 loss_val: 0.8369 acc_val: 0.7333 time: 0.1698s\n",
            "Epoch: 0041 loss_train: 0.8982 acc_train: 0.7163 loss_val: 0.8107 acc_val: 0.7447 time: 0.1785s\n",
            "Epoch: 0042 loss_train: 0.8961 acc_train: 0.7180 loss_val: 0.7930 acc_val: 0.7480 time: 0.1761s\n",
            "Epoch: 0043 loss_train: 0.8813 acc_train: 0.7182 loss_val: 0.7798 acc_val: 0.7487 time: 0.1792s\n",
            "Epoch: 0044 loss_train: 0.8483 acc_train: 0.7278 loss_val: 0.7637 acc_val: 0.7553 time: 0.1683s\n",
            "Epoch: 0045 loss_train: 0.8454 acc_train: 0.7191 loss_val: 0.7576 acc_val: 0.7500 time: 0.1719s\n",
            "Epoch: 0046 loss_train: 0.8462 acc_train: 0.7223 loss_val: 0.7475 acc_val: 0.7567 time: 0.1727s\n",
            "Epoch: 0047 loss_train: 0.8081 acc_train: 0.7397 loss_val: 0.7332 acc_val: 0.7607 time: 0.1748s\n",
            "Epoch: 0048 loss_train: 0.8169 acc_train: 0.7347 loss_val: 0.7146 acc_val: 0.7727 time: 0.1768s\n",
            "Epoch: 0049 loss_train: 0.7972 acc_train: 0.7487 loss_val: 0.7057 acc_val: 0.7687 time: 0.1775s\n",
            "Epoch: 0050 loss_train: 0.7908 acc_train: 0.7401 loss_val: 0.6952 acc_val: 0.7747 time: 0.1702s\n",
            "Epoch: 0051 loss_train: 0.7823 acc_train: 0.7494 loss_val: 0.7044 acc_val: 0.7653 time: 0.1698s\n",
            "Epoch: 0052 loss_train: 0.7778 acc_train: 0.7416 loss_val: 0.6844 acc_val: 0.7700 time: 0.1710s\n",
            "Epoch: 0053 loss_train: 0.7515 acc_train: 0.7513 loss_val: 0.6748 acc_val: 0.7773 time: 0.1766s\n",
            "Epoch: 0054 loss_train: 0.7419 acc_train: 0.7546 loss_val: 0.6636 acc_val: 0.7713 time: 0.1739s\n",
            "Epoch: 0055 loss_train: 0.7408 acc_train: 0.7476 loss_val: 0.6594 acc_val: 0.7753 time: 0.1718s\n",
            "Epoch: 0056 loss_train: 0.7311 acc_train: 0.7542 loss_val: 0.6501 acc_val: 0.7760 time: 0.1685s\n",
            "Epoch: 0057 loss_train: 0.7220 acc_train: 0.7545 loss_val: 0.6425 acc_val: 0.7780 time: 0.1760s\n",
            "Epoch: 0058 loss_train: 0.7030 acc_train: 0.7610 loss_val: 0.6333 acc_val: 0.7853 time: 0.1713s\n",
            "Epoch: 0059 loss_train: 0.7028 acc_train: 0.7670 loss_val: 0.6283 acc_val: 0.7787 time: 0.1801s\n",
            "Epoch: 0060 loss_train: 0.6947 acc_train: 0.7628 loss_val: 0.6102 acc_val: 0.7933 time: 0.1762s\n",
            "Epoch: 0061 loss_train: 0.6887 acc_train: 0.7756 loss_val: 0.6025 acc_val: 0.8053 time: 0.1784s\n",
            "Epoch: 0062 loss_train: 0.6946 acc_train: 0.7733 loss_val: 0.6026 acc_val: 0.8007 time: 0.1537s\n",
            "Epoch: 0063 loss_train: 0.6753 acc_train: 0.7759 loss_val: 0.6115 acc_val: 0.7873 time: 0.1571s\n",
            "Epoch: 0064 loss_train: 0.6859 acc_train: 0.7664 loss_val: 0.5989 acc_val: 0.8080 time: 0.1520s\n",
            "Epoch: 0065 loss_train: 0.6818 acc_train: 0.7730 loss_val: 0.5889 acc_val: 0.7967 time: 0.1531s\n",
            "Epoch: 0066 loss_train: 0.6618 acc_train: 0.7759 loss_val: 0.5894 acc_val: 0.8000 time: 0.1540s\n",
            "Epoch: 0067 loss_train: 0.6468 acc_train: 0.7791 loss_val: 0.5853 acc_val: 0.8047 time: 0.1583s\n",
            "Epoch: 0068 loss_train: 0.6636 acc_train: 0.7803 loss_val: 0.5720 acc_val: 0.8153 time: 0.1664s\n",
            "Epoch: 0069 loss_train: 0.6483 acc_train: 0.7925 loss_val: 0.5864 acc_val: 0.8020 time: 0.1715s\n",
            "Epoch: 0070 loss_train: 0.6405 acc_train: 0.7880 loss_val: 0.5603 acc_val: 0.8160 time: 0.1716s\n",
            "Epoch: 0071 loss_train: 0.6271 acc_train: 0.7944 loss_val: 0.5669 acc_val: 0.8133 time: 0.1791s\n",
            "Epoch: 0072 loss_train: 0.6463 acc_train: 0.7910 loss_val: 0.5521 acc_val: 0.8240 time: 0.1697s\n",
            "Epoch: 0073 loss_train: 0.6230 acc_train: 0.7935 loss_val: 0.5496 acc_val: 0.8233 time: 0.1755s\n",
            "Epoch: 0074 loss_train: 0.6143 acc_train: 0.8048 loss_val: 0.5413 acc_val: 0.8453 time: 0.1761s\n",
            "Epoch: 0075 loss_train: 0.6234 acc_train: 0.8096 loss_val: 0.5379 acc_val: 0.8320 time: 0.1714s\n",
            "Epoch: 0076 loss_train: 0.6078 acc_train: 0.8047 loss_val: 0.5439 acc_val: 0.8187 time: 0.1726s\n",
            "Epoch: 0077 loss_train: 0.6001 acc_train: 0.7993 loss_val: 0.5243 acc_val: 0.8367 time: 0.1765s\n",
            "Epoch: 0078 loss_train: 0.5994 acc_train: 0.8060 loss_val: 0.5172 acc_val: 0.8493 time: 0.1807s\n",
            "Epoch: 0079 loss_train: 0.5811 acc_train: 0.8229 loss_val: 0.5252 acc_val: 0.8380 time: 0.1724s\n",
            "Epoch: 0080 loss_train: 0.5844 acc_train: 0.8161 loss_val: 0.5153 acc_val: 0.8480 time: 0.1732s\n",
            "Epoch: 0081 loss_train: 0.5834 acc_train: 0.8250 loss_val: 0.5048 acc_val: 0.8533 time: 0.1768s\n",
            "Epoch: 0082 loss_train: 0.5640 acc_train: 0.8315 loss_val: 0.5037 acc_val: 0.8467 time: 0.1736s\n",
            "Epoch: 0083 loss_train: 0.5620 acc_train: 0.8258 loss_val: 0.5140 acc_val: 0.8367 time: 0.1765s\n",
            "Epoch: 0084 loss_train: 0.5780 acc_train: 0.8137 loss_val: 0.5050 acc_val: 0.8513 time: 0.1868s\n",
            "Epoch: 0085 loss_train: 0.5774 acc_train: 0.8249 loss_val: 0.4969 acc_val: 0.8553 time: 0.1797s\n",
            "Epoch: 0086 loss_train: 0.5544 acc_train: 0.8266 loss_val: 0.5024 acc_val: 0.8493 time: 0.1510s\n",
            "Epoch: 0087 loss_train: 0.5572 acc_train: 0.8258 loss_val: 0.4882 acc_val: 0.8587 time: 0.1539s\n",
            "Epoch: 0088 loss_train: 0.5405 acc_train: 0.8299 loss_val: 0.4868 acc_val: 0.8580 time: 0.1528s\n",
            "Epoch: 0089 loss_train: 0.5562 acc_train: 0.8325 loss_val: 0.4964 acc_val: 0.8407 time: 0.1566s\n",
            "Epoch: 0090 loss_train: 0.5593 acc_train: 0.8169 loss_val: 0.4772 acc_val: 0.8620 time: 0.1588s\n",
            "Epoch: 0091 loss_train: 0.5484 acc_train: 0.8304 loss_val: 0.4765 acc_val: 0.8580 time: 0.1583s\n",
            "Epoch: 0092 loss_train: 0.5428 acc_train: 0.8348 loss_val: 0.5010 acc_val: 0.8353 time: 0.1693s\n",
            "Epoch: 0093 loss_train: 0.5575 acc_train: 0.8263 loss_val: 0.4809 acc_val: 0.8500 time: 0.1721s\n",
            "Epoch: 0094 loss_train: 0.5440 acc_train: 0.8292 loss_val: 0.4850 acc_val: 0.8593 time: 0.1766s\n",
            "Epoch: 0095 loss_train: 0.5562 acc_train: 0.8332 loss_val: 0.4749 acc_val: 0.8553 time: 0.1721s\n",
            "Epoch: 0096 loss_train: 0.5364 acc_train: 0.8321 loss_val: 0.4859 acc_val: 0.8480 time: 0.1743s\n",
            "Epoch: 0097 loss_train: 0.5322 acc_train: 0.8266 loss_val: 0.4686 acc_val: 0.8640 time: 0.1718s\n",
            "Epoch: 0098 loss_train: 0.5304 acc_train: 0.8349 loss_val: 0.4688 acc_val: 0.8607 time: 0.1706s\n",
            "Epoch: 0099 loss_train: 0.5300 acc_train: 0.8414 loss_val: 0.4788 acc_val: 0.8500 time: 0.1767s\n",
            "Epoch: 0100 loss_train: 0.5202 acc_train: 0.8347 loss_val: 0.4607 acc_val: 0.8600 time: 0.1870s\n",
            "Epoch: 0101 loss_train: 0.5105 acc_train: 0.8439 loss_val: 0.4558 acc_val: 0.8640 time: 0.1775s\n",
            "Epoch: 0102 loss_train: 0.5166 acc_train: 0.8406 loss_val: 0.4629 acc_val: 0.8547 time: 0.1827s\n",
            "Epoch: 0103 loss_train: 0.5166 acc_train: 0.8334 loss_val: 0.4620 acc_val: 0.8560 time: 0.1679s\n",
            "Epoch: 0104 loss_train: 0.5006 acc_train: 0.8422 loss_val: 0.4516 acc_val: 0.8680 time: 0.1542s\n",
            "Epoch: 0105 loss_train: 0.5118 acc_train: 0.8422 loss_val: 0.4495 acc_val: 0.8653 time: 0.1533s\n",
            "Epoch: 0106 loss_train: 0.5080 acc_train: 0.8391 loss_val: 0.4566 acc_val: 0.8567 time: 0.1547s\n",
            "Epoch: 0107 loss_train: 0.5129 acc_train: 0.8317 loss_val: 0.4489 acc_val: 0.8673 time: 0.1538s\n",
            "Epoch: 0108 loss_train: 0.5067 acc_train: 0.8377 loss_val: 0.4483 acc_val: 0.8680 time: 0.1558s\n",
            "Epoch: 0109 loss_train: 0.4978 acc_train: 0.8441 loss_val: 0.4499 acc_val: 0.8667 time: 0.1660s\n",
            "Epoch: 0110 loss_train: 0.4995 acc_train: 0.8513 loss_val: 0.4495 acc_val: 0.8640 time: 0.1691s\n",
            "Epoch: 0111 loss_train: 0.5060 acc_train: 0.8386 loss_val: 0.4411 acc_val: 0.8660 time: 0.1809s\n",
            "Epoch: 0112 loss_train: 0.4973 acc_train: 0.8510 loss_val: 0.4414 acc_val: 0.8660 time: 0.1777s\n",
            "Epoch: 0113 loss_train: 0.4911 acc_train: 0.8469 loss_val: 0.4400 acc_val: 0.8667 time: 0.1758s\n",
            "Epoch: 0114 loss_train: 0.4878 acc_train: 0.8466 loss_val: 0.4371 acc_val: 0.8700 time: 0.1757s\n",
            "Epoch: 0115 loss_train: 0.4806 acc_train: 0.8474 loss_val: 0.4509 acc_val: 0.8553 time: 0.1736s\n",
            "Epoch: 0116 loss_train: 0.4935 acc_train: 0.8445 loss_val: 0.4364 acc_val: 0.8673 time: 0.1781s\n",
            "Epoch: 0117 loss_train: 0.4837 acc_train: 0.8469 loss_val: 0.4373 acc_val: 0.8653 time: 0.1770s\n",
            "Epoch: 0118 loss_train: 0.4924 acc_train: 0.8411 loss_val: 0.4350 acc_val: 0.8667 time: 0.1780s\n",
            "Epoch: 0119 loss_train: 0.4877 acc_train: 0.8480 loss_val: 0.4387 acc_val: 0.8607 time: 0.1743s\n",
            "Epoch: 0120 loss_train: 0.4825 acc_train: 0.8485 loss_val: 0.4299 acc_val: 0.8687 time: 0.1803s\n",
            "Epoch: 0121 loss_train: 0.4850 acc_train: 0.8493 loss_val: 0.4385 acc_val: 0.8613 time: 0.1735s\n",
            "Epoch: 0122 loss_train: 0.4808 acc_train: 0.8486 loss_val: 0.4306 acc_val: 0.8667 time: 0.1769s\n",
            "Epoch: 0123 loss_train: 0.4852 acc_train: 0.8437 loss_val: 0.4305 acc_val: 0.8673 time: 0.1769s\n",
            "Epoch: 0124 loss_train: 0.4905 acc_train: 0.8510 loss_val: 0.4265 acc_val: 0.8720 time: 0.1736s\n",
            "Epoch: 0125 loss_train: 0.4736 acc_train: 0.8525 loss_val: 0.4321 acc_val: 0.8633 time: 0.1732s\n",
            "Epoch: 0126 loss_train: 0.4651 acc_train: 0.8477 loss_val: 0.4234 acc_val: 0.8707 time: 0.1851s\n",
            "Epoch: 0127 loss_train: 0.4740 acc_train: 0.8513 loss_val: 0.4232 acc_val: 0.8660 time: 0.1745s\n",
            "Epoch: 0128 loss_train: 0.4711 acc_train: 0.8539 loss_val: 0.4285 acc_val: 0.8653 time: 0.1775s\n",
            "Epoch: 0129 loss_train: 0.4654 acc_train: 0.8511 loss_val: 0.4233 acc_val: 0.8660 time: 0.1755s\n",
            "Epoch: 0130 loss_train: 0.4624 acc_train: 0.8566 loss_val: 0.4188 acc_val: 0.8700 time: 0.1717s\n",
            "Epoch: 0131 loss_train: 0.4666 acc_train: 0.8569 loss_val: 0.4286 acc_val: 0.8600 time: 0.1803s\n",
            "Epoch: 0132 loss_train: 0.4724 acc_train: 0.8423 loss_val: 0.4175 acc_val: 0.8700 time: 0.1807s\n",
            "Epoch: 0133 loss_train: 0.4703 acc_train: 0.8492 loss_val: 0.4194 acc_val: 0.8653 time: 0.1764s\n",
            "Epoch: 0134 loss_train: 0.4634 acc_train: 0.8550 loss_val: 0.4189 acc_val: 0.8627 time: 0.1700s\n",
            "Epoch: 0135 loss_train: 0.4603 acc_train: 0.8549 loss_val: 0.4177 acc_val: 0.8693 time: 0.1737s\n",
            "Epoch: 0136 loss_train: 0.4570 acc_train: 0.8582 loss_val: 0.4165 acc_val: 0.8733 time: 0.1781s\n",
            "Epoch: 0137 loss_train: 0.4584 acc_train: 0.8563 loss_val: 0.4137 acc_val: 0.8713 time: 0.1798s\n",
            "Epoch: 0138 loss_train: 0.4492 acc_train: 0.8588 loss_val: 0.4294 acc_val: 0.8567 time: 0.1707s\n",
            "Epoch: 0139 loss_train: 0.4557 acc_train: 0.8514 loss_val: 0.4147 acc_val: 0.8660 time: 0.1733s\n",
            "Epoch: 0140 loss_train: 0.4627 acc_train: 0.8591 loss_val: 0.4181 acc_val: 0.8653 time: 0.1752s\n",
            "Epoch: 0141 loss_train: 0.4518 acc_train: 0.8541 loss_val: 0.4113 acc_val: 0.8687 time: 0.1737s\n",
            "Epoch: 0142 loss_train: 0.4568 acc_train: 0.8572 loss_val: 0.4157 acc_val: 0.8633 time: 0.1730s\n",
            "Epoch: 0143 loss_train: 0.4612 acc_train: 0.8517 loss_val: 0.4097 acc_val: 0.8687 time: 0.1753s\n",
            "Epoch: 0144 loss_train: 0.4488 acc_train: 0.8620 loss_val: 0.4169 acc_val: 0.8633 time: 0.1836s\n",
            "Epoch: 0145 loss_train: 0.4579 acc_train: 0.8633 loss_val: 0.4158 acc_val: 0.8720 time: 0.1752s\n",
            "Epoch: 0146 loss_train: 0.4463 acc_train: 0.8513 loss_val: 0.4137 acc_val: 0.8707 time: 0.1710s\n",
            "Epoch: 0147 loss_train: 0.4571 acc_train: 0.8600 loss_val: 0.4126 acc_val: 0.8680 time: 0.1841s\n",
            "Epoch: 0148 loss_train: 0.4433 acc_train: 0.8557 loss_val: 0.4073 acc_val: 0.8667 time: 0.1689s\n",
            "Epoch: 0149 loss_train: 0.4567 acc_train: 0.8574 loss_val: 0.4099 acc_val: 0.8740 time: 0.1712s\n",
            "Epoch: 0150 loss_train: 0.4450 acc_train: 0.8657 loss_val: 0.4097 acc_val: 0.8700 time: 0.1526s\n",
            "Epoch: 0151 loss_train: 0.4428 acc_train: 0.8596 loss_val: 0.4142 acc_val: 0.8613 time: 0.1532s\n",
            "Epoch: 0152 loss_train: 0.4611 acc_train: 0.8508 loss_val: 0.4087 acc_val: 0.8727 time: 0.1517s\n",
            "Epoch: 0153 loss_train: 0.4447 acc_train: 0.8572 loss_val: 0.4098 acc_val: 0.8660 time: 0.1537s\n",
            "Epoch: 0154 loss_train: 0.4454 acc_train: 0.8609 loss_val: 0.4152 acc_val: 0.8593 time: 0.1585s\n",
            "Epoch: 0155 loss_train: 0.4469 acc_train: 0.8530 loss_val: 0.4019 acc_val: 0.8673 time: 0.1676s\n",
            "Epoch: 0156 loss_train: 0.4486 acc_train: 0.8614 loss_val: 0.3995 acc_val: 0.8693 time: 0.1582s\n",
            "Epoch: 0157 loss_train: 0.4329 acc_train: 0.8647 loss_val: 0.4154 acc_val: 0.8600 time: 0.1568s\n",
            "Epoch: 0158 loss_train: 0.4464 acc_train: 0.8537 loss_val: 0.4079 acc_val: 0.8680 time: 0.1590s\n",
            "Epoch: 0159 loss_train: 0.4518 acc_train: 0.8600 loss_val: 0.4322 acc_val: 0.8533 time: 0.1610s\n",
            "Epoch: 0160 loss_train: 0.4525 acc_train: 0.8506 loss_val: 0.3997 acc_val: 0.8693 time: 0.1506s\n",
            "Epoch: 0161 loss_train: 0.4317 acc_train: 0.8636 loss_val: 0.4222 acc_val: 0.8647 time: 0.1669s\n",
            "Epoch: 0162 loss_train: 0.4633 acc_train: 0.8606 loss_val: 0.4742 acc_val: 0.8360 time: 0.1711s\n",
            "Epoch: 0163 loss_train: 0.5005 acc_train: 0.8300 loss_val: 0.4138 acc_val: 0.8667 time: 0.1749s\n",
            "Epoch: 0164 loss_train: 0.4384 acc_train: 0.8623 loss_val: 0.4569 acc_val: 0.8540 time: 0.1744s\n",
            "Epoch: 0165 loss_train: 0.4943 acc_train: 0.8495 loss_val: 0.4922 acc_val: 0.8340 time: 0.1754s\n",
            "Epoch: 0166 loss_train: 0.5130 acc_train: 0.8312 loss_val: 0.4557 acc_val: 0.8480 time: 0.1706s\n",
            "Epoch: 0167 loss_train: 0.4784 acc_train: 0.8429 loss_val: 0.4607 acc_val: 0.8547 time: 0.1751s\n",
            "Epoch: 0168 loss_train: 0.4912 acc_train: 0.8492 loss_val: 0.3967 acc_val: 0.8720 time: 0.1507s\n",
            "Epoch: 0169 loss_train: 0.4289 acc_train: 0.8661 loss_val: 0.4475 acc_val: 0.8487 time: 0.1563s\n",
            "Epoch: 0170 loss_train: 0.4735 acc_train: 0.8393 loss_val: 0.4110 acc_val: 0.8627 time: 0.1484s\n",
            "Epoch: 0171 loss_train: 0.4424 acc_train: 0.8626 loss_val: 0.4006 acc_val: 0.8640 time: 0.1529s\n",
            "Epoch: 0172 loss_train: 0.4473 acc_train: 0.8658 loss_val: 0.4074 acc_val: 0.8660 time: 0.1506s\n",
            "Epoch: 0173 loss_train: 0.4425 acc_train: 0.8546 loss_val: 0.4151 acc_val: 0.8560 time: 0.1500s\n",
            "Epoch: 0174 loss_train: 0.4482 acc_train: 0.8480 loss_val: 0.3972 acc_val: 0.8693 time: 0.1616s\n",
            "Epoch: 0175 loss_train: 0.4341 acc_train: 0.8662 loss_val: 0.4016 acc_val: 0.8727 time: 0.1525s\n",
            "Epoch: 0176 loss_train: 0.4338 acc_train: 0.8676 loss_val: 0.4041 acc_val: 0.8653 time: 0.1519s\n",
            "Epoch: 0177 loss_train: 0.4358 acc_train: 0.8617 loss_val: 0.4063 acc_val: 0.8647 time: 0.1580s\n",
            "Epoch: 0178 loss_train: 0.4404 acc_train: 0.8554 loss_val: 0.3915 acc_val: 0.8740 time: 0.1492s\n",
            "Epoch: 0179 loss_train: 0.4262 acc_train: 0.8678 loss_val: 0.3934 acc_val: 0.8733 time: 0.1509s\n",
            "Epoch: 0180 loss_train: 0.4330 acc_train: 0.8671 loss_val: 0.3919 acc_val: 0.8713 time: 0.1583s\n",
            "Epoch: 0181 loss_train: 0.4246 acc_train: 0.8660 loss_val: 0.4022 acc_val: 0.8667 time: 0.1515s\n",
            "Epoch: 0182 loss_train: 0.4266 acc_train: 0.8643 loss_val: 0.3895 acc_val: 0.8653 time: 0.1537s\n",
            "Epoch: 0183 loss_train: 0.4239 acc_train: 0.8667 loss_val: 0.3906 acc_val: 0.8740 time: 0.1523s\n",
            "Epoch: 0184 loss_train: 0.4174 acc_train: 0.8674 loss_val: 0.3904 acc_val: 0.8707 time: 0.1503s\n",
            "Epoch: 0185 loss_train: 0.4231 acc_train: 0.8662 loss_val: 0.3919 acc_val: 0.8687 time: 0.1530s\n",
            "Epoch: 0186 loss_train: 0.4234 acc_train: 0.8662 loss_val: 0.3862 acc_val: 0.8767 time: 0.1488s\n",
            "Epoch: 0187 loss_train: 0.4179 acc_train: 0.8693 loss_val: 0.3886 acc_val: 0.8720 time: 0.1669s\n",
            "Epoch: 0188 loss_train: 0.4204 acc_train: 0.8717 loss_val: 0.3980 acc_val: 0.8693 time: 0.1506s\n",
            "Epoch: 0189 loss_train: 0.4275 acc_train: 0.8655 loss_val: 0.3859 acc_val: 0.8760 time: 0.1525s\n",
            "Epoch: 0190 loss_train: 0.4046 acc_train: 0.8724 loss_val: 0.3887 acc_val: 0.8667 time: 0.1550s\n",
            "Epoch: 0191 loss_train: 0.4252 acc_train: 0.8684 loss_val: 0.3829 acc_val: 0.8760 time: 0.1520s\n",
            "Epoch: 0192 loss_train: 0.4230 acc_train: 0.8661 loss_val: 0.3886 acc_val: 0.8707 time: 0.1520s\n",
            "Epoch: 0193 loss_train: 0.4153 acc_train: 0.8664 loss_val: 0.3863 acc_val: 0.8753 time: 0.1562s\n",
            "Epoch: 0194 loss_train: 0.4259 acc_train: 0.8685 loss_val: 0.3854 acc_val: 0.8687 time: 0.1540s\n",
            "Epoch: 0195 loss_train: 0.4104 acc_train: 0.8728 loss_val: 0.3854 acc_val: 0.8693 time: 0.1568s\n",
            "Epoch: 0196 loss_train: 0.4214 acc_train: 0.8647 loss_val: 0.3815 acc_val: 0.8773 time: 0.1511s\n",
            "Epoch: 0197 loss_train: 0.4118 acc_train: 0.8688 loss_val: 0.3844 acc_val: 0.8747 time: 0.1517s\n",
            "Epoch: 0198 loss_train: 0.4188 acc_train: 0.8693 loss_val: 0.3831 acc_val: 0.8753 time: 0.1516s\n",
            "Epoch: 0199 loss_train: 0.4060 acc_train: 0.8724 loss_val: 0.3909 acc_val: 0.8653 time: 0.1514s\n",
            "Epoch: 0200 loss_train: 0.4237 acc_train: 0.8640 loss_val: 0.3839 acc_val: 0.8693 time: 0.1585s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 33.4814s\n",
            "GCN(\n",
            "  (gc1): GraphConvolution (767 -> 16) \n",
            "  (gc2): GraphConvolution (16 -> 10) \n",
            ")\n",
            "Test set results: loss= 0.3956 accuracy= 0.8735\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}