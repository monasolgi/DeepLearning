{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_seg_Amazon",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17vScsjW7N-ZGco9xIjzPWg2eo-x6_xoj",
      "authorship_tag": "ABX9TyO22vrVSHjT+3MUYSe//K16",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monasolgi/DeepLearning/blob/master/gcn_seg_Amazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY7L10jJt1dd",
        "outputId": "a7377915-def7-4e9c-e50c-c9909752a2fb"
      },
      "source": [
        "import numpy as np\n",
        "npz_data=np.load('/content/drive/MyDrive/Datasets/amazon_electronics_computers (1).npz')\n",
        "npz_data.files\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adj_data',\n",
              " 'adj_indices',\n",
              " 'adj_indptr',\n",
              " 'adj_shape',\n",
              " 'attr_data',\n",
              " 'attr_indices',\n",
              " 'attr_indptr',\n",
              " 'attr_shape',\n",
              " 'labels',\n",
              " 'class_names']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLNtwqForWtv",
        "outputId": "450a7249-b248-4c2e-c371-89cb4e6f1855"
      },
      "source": [
        " npz_data['labels'].shape\r\n",
        " npz_data['class_names']\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Desktops', 'Data Storage', 'Laptops', 'Monitors',\n",
              "       'Computer Components', 'Video Projectors', 'Routers', 'Tablets',\n",
              "       'Networking Products', 'Webcams'], dtype='<U19')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2jDBa1vogos",
        "outputId": "18911483-9d64-422e-c449-9747337ebbb1"
      },
      "source": [
        "#utils\r\n",
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "import torch\r\n",
        "from scipy.sparse import coo_matrix, csr_matrix\r\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer, normalize\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),\r\n",
        "                                        shape=npz_data['attr_shape'])\r\n",
        "  \r\n",
        "adj= sp.coo_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),\r\n",
        "                                   shape=npz_data['adj_shape']))\r\n",
        "features[0]\r\n",
        "adj.toarray()[0]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7ZxuXmTNm3v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ7zOlTHyRtR"
      },
      "source": [
        "#utils\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer, normalize\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        " features = sp.csr_matrix((npz_data['attr_data'], npz_data['attr_indices'], npz_data['attr_indptr']),\n",
        "                                        shape=npz_data['attr_shape'])\n",
        "  \n",
        " adj= sp.coo_matrix(sp.csr_matrix((npz_data['adj_data'], npz_data['adj_indices'], npz_data['adj_indptr']),\n",
        "                                   shape=npz_data['adj_shape']))\n",
        "  \n",
        "\n",
        " labels=npz_data['labels']\n",
        " labels=binarize_labels(labels)\n",
        "\n",
        " adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        " adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        " idx_train = range(9100)\n",
        " idx_val = range(9100,11500)\n",
        " idx_test = range(11500, 13381)\n",
        "\n",
        " idx_train = torch.LongTensor(idx_train)\n",
        " idx_val = torch.LongTensor(idx_val)\n",
        " idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        " features = torch.FloatTensor(np.array(features.todense()))\n",
        " labels = torch.LongTensor(np.where(labels)[1])\n",
        " adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        " \n",
        " return adj, features, labels, idx_train, idx_val, idx_test\n",
        " \n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    #sum in every row \n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    # every sum to the power of -1 \n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    #diagonal matrice \n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def binarize_labels(labels, sparse_output=False, return_classes=False):\n",
        "    if hasattr(labels[0], '__iter__'):  # labels[0] is iterable <=> multilabel format\n",
        "        binarizer = MultiLabelBinarizer(sparse_output=sparse_output)\n",
        "    else:\n",
        "        binarizer = LabelBinarizer(sparse_output=sparse_output)\n",
        "    label_matrix = binarizer.fit_transform(labels).astype(np.float32)\n",
        "    return (label_matrix, binarizer.classes_) if return_classes else label_matrix\n",
        " \n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)    \n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iarFyQvXDCp6"
      },
      "source": [
        "#layers\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "#class parameter ,param haro cache mikone\n",
        "\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        \n",
        "        #Sparse matrix multiplication=spmm\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ') '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMeYJ5fQDGSN"
      },
      "source": [
        "#models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphConvolution\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid,nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "   \n",
        "        self.dropout = dropout\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EppThdy9xTB"
      },
      "source": [
        "#pip install ipdb\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wH8lnw8CDRwG",
        "outputId": "9c4382fb-7c73-4dfe-dbfd-f458a563cf43"
      },
      "source": [
        "#train\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torchsummary import summary\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt2\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from utils import load_data, accuracy\n",
        "#from models import GCN\n",
        "\n",
        "# Training settings\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=3000,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "#args = parser.parse_args()\n",
        "#error midad khate bala,khate paein jaigozin shod\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=10,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "\n",
        "## to set cuda as your device if possible\n",
        "##training on  GPU\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "    \n",
        "    ## train:adjust the weights on the neural network\n",
        "    ## validation:used to minimize overfitting\n",
        " \n",
        "def test():\n",
        "    ## Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "  # Computing the gradients necessary to adjust the weights\n",
        "  loss_train.backward()\n",
        "  # Updating the weights of the neural network\n",
        "  optimizer.step()\n",
        "  losses.append(loss_train.item())\n",
        "  acc.append(acc_train.item())\n",
        "\n",
        "  if not args.fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  losses_val.append(loss_val.item())\n",
        "  acc_valid.append(acc_val.item())\n",
        "\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "losses = []\n",
        "acc=[]\n",
        "losses_val = []\n",
        "acc_valid=[]\n",
        "t = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "\n",
        "plt.plot(np.array(losses),label ='loss_train Plot')#plotting loss_train_val\n",
        "plt.plot(np.array(losses_val),label ='loss_val Plot')\n",
        "plt.title('loss_train_validation Plot')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt2.plot(np.array(acc),label ='Accuracy_train Plot')#plotting acc_train_val\n",
        "plt2.plot(np.array(acc_valid),label ='Accuracy_val Plot')\n",
        "plt2.title('acc_train_validation Plot')\n",
        "plt2.xlabel('epoch')\n",
        "plt2.ylabel('value')\n",
        "plt2.legend()\n",
        "plt2.show()    \n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "print(model)\n",
        "#summary(model,[(13381,767),(13381,13381)])\n",
        "# Testing\n",
        "test()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 3.0367 acc_train: 0.0725 loss_val: 2.8156 acc_val: 0.1208 time: 0.2399s\n",
            "Epoch: 0002 loss_train: 2.8491 acc_train: 0.0968 loss_val: 2.5085 acc_val: 0.1125 time: 0.4803s\n",
            "Epoch: 0003 loss_train: 2.5390 acc_train: 0.1108 loss_val: 2.3669 acc_val: 0.3675 time: 0.7300s\n",
            "Epoch: 0004 loss_train: 2.3643 acc_train: 0.3526 loss_val: 2.2631 acc_val: 0.3596 time: 0.9843s\n",
            "Epoch: 0005 loss_train: 2.2676 acc_train: 0.3488 loss_val: 2.3088 acc_val: 0.1846 time: 1.2309s\n",
            "Epoch: 0006 loss_train: 2.3249 acc_train: 0.2059 loss_val: 2.0828 acc_val: 0.3746 time: 1.4713s\n",
            "Epoch: 0007 loss_train: 2.1159 acc_train: 0.3714 loss_val: 1.9507 acc_val: 0.3629 time: 1.7352s\n",
            "Epoch: 0008 loss_train: 1.9587 acc_train: 0.3605 loss_val: 1.9709 acc_val: 0.3713 time: 1.9779s\n",
            "Epoch: 0009 loss_train: 1.9890 acc_train: 0.3613 loss_val: 1.9931 acc_val: 0.3696 time: 2.2257s\n",
            "Epoch: 0010 loss_train: 1.9850 acc_train: 0.3602 loss_val: 1.9487 acc_val: 0.3688 time: 2.4744s\n",
            "Epoch: 0011 loss_train: 1.9506 acc_train: 0.3659 loss_val: 1.9072 acc_val: 0.4071 time: 2.7558s\n",
            "Epoch: 0012 loss_train: 1.9180 acc_train: 0.3964 loss_val: 1.8339 acc_val: 0.3937 time: 2.9835s\n",
            "Epoch: 0013 loss_train: 1.8494 acc_train: 0.3879 loss_val: 1.7601 acc_val: 0.3746 time: 3.2267s\n",
            "Epoch: 0014 loss_train: 1.7700 acc_train: 0.3733 loss_val: 1.7208 acc_val: 0.3879 time: 3.4774s\n",
            "Epoch: 0015 loss_train: 1.7475 acc_train: 0.3834 loss_val: 1.7230 acc_val: 0.3912 time: 3.7168s\n",
            "Epoch: 0016 loss_train: 1.7436 acc_train: 0.3920 loss_val: 1.7127 acc_val: 0.3929 time: 3.9744s\n",
            "Epoch: 0017 loss_train: 1.7458 acc_train: 0.3925 loss_val: 1.6662 acc_val: 0.3942 time: 4.2209s\n",
            "Epoch: 0018 loss_train: 1.6841 acc_train: 0.3938 loss_val: 1.6297 acc_val: 0.3925 time: 4.4737s\n",
            "Epoch: 0019 loss_train: 1.6429 acc_train: 0.3933 loss_val: 1.6218 acc_val: 0.3925 time: 4.7273s\n",
            "Epoch: 0020 loss_train: 1.6457 acc_train: 0.3912 loss_val: 1.5995 acc_val: 0.3946 time: 4.9757s\n",
            "Epoch: 0021 loss_train: 1.6050 acc_train: 0.3960 loss_val: 1.5545 acc_val: 0.3971 time: 5.2365s\n",
            "Epoch: 0022 loss_train: 1.5750 acc_train: 0.3979 loss_val: 1.5224 acc_val: 0.4071 time: 5.4844s\n",
            "Epoch: 0023 loss_train: 1.5415 acc_train: 0.4027 loss_val: 1.5076 acc_val: 0.4150 time: 5.7308s\n",
            "Epoch: 0024 loss_train: 1.5243 acc_train: 0.4170 loss_val: 1.4885 acc_val: 0.4200 time: 5.9776s\n",
            "Epoch: 0025 loss_train: 1.5106 acc_train: 0.4104 loss_val: 1.4565 acc_val: 0.4192 time: 6.2241s\n",
            "Epoch: 0026 loss_train: 1.4772 acc_train: 0.4192 loss_val: 1.4276 acc_val: 0.4333 time: 6.4638s\n",
            "Epoch: 0027 loss_train: 1.4501 acc_train: 0.4257 loss_val: 1.4150 acc_val: 0.4567 time: 6.7129s\n",
            "Epoch: 0028 loss_train: 1.4322 acc_train: 0.4499 loss_val: 1.3981 acc_val: 0.4746 time: 6.9633s\n",
            "Epoch: 0029 loss_train: 1.4154 acc_train: 0.4696 loss_val: 1.3685 acc_val: 0.4963 time: 7.2184s\n",
            "Epoch: 0030 loss_train: 1.3977 acc_train: 0.5054 loss_val: 1.3429 acc_val: 0.5175 time: 7.4636s\n",
            "Epoch: 0031 loss_train: 1.3613 acc_train: 0.5192 loss_val: 1.3240 acc_val: 0.5308 time: 7.6814s\n",
            "Epoch: 0032 loss_train: 1.3652 acc_train: 0.5149 loss_val: 1.3012 acc_val: 0.5483 time: 7.9352s\n",
            "Epoch: 0033 loss_train: 1.3349 acc_train: 0.5422 loss_val: 1.2800 acc_val: 0.5579 time: 8.1786s\n",
            "Epoch: 0034 loss_train: 1.3109 acc_train: 0.5469 loss_val: 1.2663 acc_val: 0.5758 time: 8.4214s\n",
            "Epoch: 0035 loss_train: 1.2777 acc_train: 0.5715 loss_val: 1.2511 acc_val: 0.5825 time: 8.6593s\n",
            "Epoch: 0036 loss_train: 1.2730 acc_train: 0.5811 loss_val: 1.2293 acc_val: 0.5962 time: 8.9089s\n",
            "Epoch: 0037 loss_train: 1.2549 acc_train: 0.5959 loss_val: 1.1906 acc_val: 0.6379 time: 9.1552s\n",
            "Epoch: 0038 loss_train: 1.2263 acc_train: 0.6253 loss_val: 1.1587 acc_val: 0.6846 time: 9.3791s\n",
            "Epoch: 0039 loss_train: 1.1896 acc_train: 0.6586 loss_val: 1.1320 acc_val: 0.7021 time: 9.6243s\n",
            "Epoch: 0040 loss_train: 1.1806 acc_train: 0.6688 loss_val: 1.1019 acc_val: 0.7025 time: 9.8790s\n",
            "Epoch: 0041 loss_train: 1.1591 acc_train: 0.6748 loss_val: 1.0820 acc_val: 0.6967 time: 10.0991s\n",
            "Epoch: 0042 loss_train: 1.1332 acc_train: 0.6722 loss_val: 1.0569 acc_val: 0.6937 time: 10.3526s\n",
            "Epoch: 0043 loss_train: 1.1042 acc_train: 0.6734 loss_val: 1.0219 acc_val: 0.6921 time: 10.5958s\n",
            "Epoch: 0044 loss_train: 1.0942 acc_train: 0.6659 loss_val: 0.9956 acc_val: 0.7029 time: 10.8251s\n",
            "Epoch: 0045 loss_train: 1.0551 acc_train: 0.6849 loss_val: 0.9726 acc_val: 0.7163 time: 11.0737s\n",
            "Epoch: 0046 loss_train: 1.0489 acc_train: 0.6844 loss_val: 0.9617 acc_val: 0.7142 time: 11.3188s\n",
            "Epoch: 0047 loss_train: 1.0240 acc_train: 0.6876 loss_val: 0.9441 acc_val: 0.7154 time: 11.5158s\n",
            "Epoch: 0048 loss_train: 1.0026 acc_train: 0.6922 loss_val: 0.9199 acc_val: 0.7192 time: 11.7595s\n",
            "Epoch: 0049 loss_train: 0.9878 acc_train: 0.6970 loss_val: 0.9026 acc_val: 0.7171 time: 12.0087s\n",
            "Epoch: 0050 loss_train: 0.9980 acc_train: 0.6914 loss_val: 0.8892 acc_val: 0.7175 time: 12.1915s\n",
            "Epoch: 0051 loss_train: 0.9675 acc_train: 0.6953 loss_val: 0.8785 acc_val: 0.7196 time: 12.4319s\n",
            "Epoch: 0052 loss_train: 0.9655 acc_train: 0.6919 loss_val: 0.8596 acc_val: 0.7233 time: 12.6754s\n",
            "Epoch: 0053 loss_train: 0.9418 acc_train: 0.7012 loss_val: 0.8432 acc_val: 0.7254 time: 12.9289s\n",
            "Epoch: 0054 loss_train: 0.9485 acc_train: 0.6936 loss_val: 0.8317 acc_val: 0.7317 time: 13.1719s\n",
            "Epoch: 0055 loss_train: 0.9331 acc_train: 0.7010 loss_val: 0.8235 acc_val: 0.7346 time: 13.4164s\n",
            "Epoch: 0056 loss_train: 0.9088 acc_train: 0.7080 loss_val: 0.8232 acc_val: 0.7242 time: 13.6647s\n",
            "Epoch: 0057 loss_train: 0.9023 acc_train: 0.7005 loss_val: 0.8127 acc_val: 0.7217 time: 13.9187s\n",
            "Epoch: 0058 loss_train: 0.8658 acc_train: 0.7131 loss_val: 0.8015 acc_val: 0.7154 time: 14.1627s\n",
            "Epoch: 0059 loss_train: 0.8810 acc_train: 0.7025 loss_val: 0.7913 acc_val: 0.7229 time: 14.4071s\n",
            "Epoch: 0060 loss_train: 0.8694 acc_train: 0.7122 loss_val: 0.7811 acc_val: 0.7433 time: 14.6604s\n",
            "Epoch: 0061 loss_train: 0.8552 acc_train: 0.7232 loss_val: 0.7766 acc_val: 0.7450 time: 14.9407s\n",
            "Epoch: 0062 loss_train: 0.8624 acc_train: 0.7187 loss_val: 0.7666 acc_val: 0.7483 time: 15.1893s\n",
            "Epoch: 0063 loss_train: 0.8428 acc_train: 0.7271 loss_val: 0.7538 acc_val: 0.7462 time: 15.4361s\n",
            "Epoch: 0064 loss_train: 0.8378 acc_train: 0.7276 loss_val: 0.7468 acc_val: 0.7425 time: 15.7009s\n",
            "Epoch: 0065 loss_train: 0.8265 acc_train: 0.7333 loss_val: 0.7400 acc_val: 0.7458 time: 15.9537s\n",
            "Epoch: 0066 loss_train: 0.8304 acc_train: 0.7302 loss_val: 0.7355 acc_val: 0.7529 time: 16.2126s\n",
            "Epoch: 0067 loss_train: 0.8225 acc_train: 0.7341 loss_val: 0.7259 acc_val: 0.7579 time: 16.4538s\n",
            "Epoch: 0068 loss_train: 0.8098 acc_train: 0.7374 loss_val: 0.7137 acc_val: 0.7558 time: 16.7067s\n",
            "Epoch: 0069 loss_train: 0.8002 acc_train: 0.7353 loss_val: 0.7093 acc_val: 0.7554 time: 16.9546s\n",
            "Epoch: 0070 loss_train: 0.8101 acc_train: 0.7353 loss_val: 0.7029 acc_val: 0.7562 time: 17.2065s\n",
            "Epoch: 0071 loss_train: 0.8031 acc_train: 0.7321 loss_val: 0.7029 acc_val: 0.7600 time: 17.4515s\n",
            "Epoch: 0072 loss_train: 0.7767 acc_train: 0.7442 loss_val: 0.7020 acc_val: 0.7600 time: 17.6947s\n",
            "Epoch: 0073 loss_train: 0.7858 acc_train: 0.7384 loss_val: 0.6894 acc_val: 0.7571 time: 17.9407s\n",
            "Epoch: 0074 loss_train: 0.7765 acc_train: 0.7427 loss_val: 0.6839 acc_val: 0.7575 time: 18.1790s\n",
            "Epoch: 0075 loss_train: 0.7684 acc_train: 0.7448 loss_val: 0.6771 acc_val: 0.7625 time: 18.4283s\n",
            "Epoch: 0076 loss_train: 0.7669 acc_train: 0.7478 loss_val: 0.6748 acc_val: 0.7629 time: 18.6728s\n",
            "Epoch: 0077 loss_train: 0.7687 acc_train: 0.7436 loss_val: 0.6713 acc_val: 0.7633 time: 18.9195s\n",
            "Epoch: 0078 loss_train: 0.7713 acc_train: 0.7403 loss_val: 0.6563 acc_val: 0.7667 time: 19.1674s\n",
            "Epoch: 0079 loss_train: 0.7432 acc_train: 0.7503 loss_val: 0.6551 acc_val: 0.7679 time: 19.4154s\n",
            "Epoch: 0080 loss_train: 0.7369 acc_train: 0.7552 loss_val: 0.6503 acc_val: 0.7733 time: 19.6562s\n",
            "Epoch: 0081 loss_train: 0.7395 acc_train: 0.7558 loss_val: 0.6511 acc_val: 0.7721 time: 19.9008s\n",
            "Epoch: 0082 loss_train: 0.7271 acc_train: 0.7481 loss_val: 0.6449 acc_val: 0.7738 time: 20.1451s\n",
            "Epoch: 0083 loss_train: 0.7464 acc_train: 0.7489 loss_val: 0.6401 acc_val: 0.7779 time: 20.4071s\n",
            "Epoch: 0084 loss_train: 0.7359 acc_train: 0.7589 loss_val: 0.6357 acc_val: 0.7804 time: 20.6510s\n",
            "Epoch: 0085 loss_train: 0.7179 acc_train: 0.7574 loss_val: 0.6336 acc_val: 0.7758 time: 20.8964s\n",
            "Epoch: 0086 loss_train: 0.7140 acc_train: 0.7609 loss_val: 0.6335 acc_val: 0.7742 time: 21.1438s\n",
            "Epoch: 0087 loss_train: 0.7155 acc_train: 0.7577 loss_val: 0.6281 acc_val: 0.7837 time: 21.3254s\n",
            "Epoch: 0088 loss_train: 0.7122 acc_train: 0.7632 loss_val: 0.6169 acc_val: 0.7942 time: 21.5691s\n",
            "Epoch: 0089 loss_train: 0.7224 acc_train: 0.7634 loss_val: 0.6086 acc_val: 0.7946 time: 21.8289s\n",
            "Epoch: 0090 loss_train: 0.7070 acc_train: 0.7697 loss_val: 0.6142 acc_val: 0.7817 time: 22.0824s\n",
            "Epoch: 0091 loss_train: 0.7065 acc_train: 0.7626 loss_val: 0.6130 acc_val: 0.7792 time: 22.3302s\n",
            "Epoch: 0092 loss_train: 0.7146 acc_train: 0.7584 loss_val: 0.6089 acc_val: 0.7900 time: 22.5783s\n",
            "Epoch: 0093 loss_train: 0.7089 acc_train: 0.7644 loss_val: 0.6073 acc_val: 0.7917 time: 22.8254s\n",
            "Epoch: 0094 loss_train: 0.6999 acc_train: 0.7709 loss_val: 0.6038 acc_val: 0.7942 time: 23.0752s\n",
            "Epoch: 0095 loss_train: 0.6908 acc_train: 0.7743 loss_val: 0.6055 acc_val: 0.7871 time: 23.3351s\n",
            "Epoch: 0096 loss_train: 0.7013 acc_train: 0.7608 loss_val: 0.6002 acc_val: 0.7921 time: 23.5835s\n",
            "Epoch: 0097 loss_train: 0.7065 acc_train: 0.7649 loss_val: 0.5968 acc_val: 0.7942 time: 23.8284s\n",
            "Epoch: 0098 loss_train: 0.6834 acc_train: 0.7709 loss_val: 0.5959 acc_val: 0.7933 time: 24.0800s\n",
            "Epoch: 0099 loss_train: 0.6937 acc_train: 0.7740 loss_val: 0.5968 acc_val: 0.7933 time: 24.3225s\n",
            "Epoch: 0100 loss_train: 0.6758 acc_train: 0.7727 loss_val: 0.5945 acc_val: 0.7921 time: 24.5554s\n",
            "Epoch: 0101 loss_train: 0.6803 acc_train: 0.7752 loss_val: 0.5847 acc_val: 0.7954 time: 24.7985s\n",
            "Epoch: 0102 loss_train: 0.6934 acc_train: 0.7681 loss_val: 0.5800 acc_val: 0.8017 time: 25.0472s\n",
            "Epoch: 0103 loss_train: 0.6743 acc_train: 0.7789 loss_val: 0.5777 acc_val: 0.8025 time: 25.2496s\n",
            "Epoch: 0104 loss_train: 0.6721 acc_train: 0.7815 loss_val: 0.5795 acc_val: 0.7963 time: 25.4727s\n",
            "Epoch: 0105 loss_train: 0.6741 acc_train: 0.7765 loss_val: 0.5793 acc_val: 0.7937 time: 25.7332s\n",
            "Epoch: 0106 loss_train: 0.6733 acc_train: 0.7760 loss_val: 0.5733 acc_val: 0.8013 time: 25.9839s\n",
            "Epoch: 0107 loss_train: 0.6572 acc_train: 0.7830 loss_val: 0.5717 acc_val: 0.8046 time: 26.2134s\n",
            "Epoch: 0108 loss_train: 0.6516 acc_train: 0.7855 loss_val: 0.5685 acc_val: 0.8046 time: 26.4584s\n",
            "Epoch: 0109 loss_train: 0.6711 acc_train: 0.7813 loss_val: 0.5683 acc_val: 0.8013 time: 26.7101s\n",
            "Epoch: 0110 loss_train: 0.6652 acc_train: 0.7804 loss_val: 0.5638 acc_val: 0.8046 time: 26.9556s\n",
            "Epoch: 0111 loss_train: 0.6482 acc_train: 0.7893 loss_val: 0.5598 acc_val: 0.8096 time: 27.2069s\n",
            "Epoch: 0112 loss_train: 0.6497 acc_train: 0.7880 loss_val: 0.5562 acc_val: 0.8121 time: 27.4533s\n",
            "Epoch: 0113 loss_train: 0.6520 acc_train: 0.7937 loss_val: 0.5554 acc_val: 0.8125 time: 27.6977s\n",
            "Epoch: 0114 loss_train: 0.6497 acc_train: 0.7848 loss_val: 0.5556 acc_val: 0.8104 time: 27.9483s\n",
            "Epoch: 0115 loss_train: 0.6510 acc_train: 0.7868 loss_val: 0.5558 acc_val: 0.8058 time: 28.1919s\n",
            "Epoch: 0116 loss_train: 0.6440 acc_train: 0.7895 loss_val: 0.5511 acc_val: 0.8154 time: 28.4352s\n",
            "Epoch: 0117 loss_train: 0.6449 acc_train: 0.7964 loss_val: 0.5487 acc_val: 0.8175 time: 28.6887s\n",
            "Epoch: 0118 loss_train: 0.6350 acc_train: 0.7900 loss_val: 0.5493 acc_val: 0.8121 time: 28.9349s\n",
            "Epoch: 0119 loss_train: 0.6330 acc_train: 0.7962 loss_val: 0.5514 acc_val: 0.8150 time: 29.1614s\n",
            "Epoch: 0120 loss_train: 0.6452 acc_train: 0.7870 loss_val: 0.5467 acc_val: 0.8137 time: 29.4062s\n",
            "Epoch: 0121 loss_train: 0.6386 acc_train: 0.7987 loss_val: 0.5527 acc_val: 0.8100 time: 29.6533s\n",
            "Epoch: 0122 loss_train: 0.6431 acc_train: 0.7912 loss_val: 0.5461 acc_val: 0.8154 time: 29.9086s\n",
            "Epoch: 0123 loss_train: 0.6354 acc_train: 0.7945 loss_val: 0.5398 acc_val: 0.8192 time: 30.1562s\n",
            "Epoch: 0124 loss_train: 0.6189 acc_train: 0.7966 loss_val: 0.5384 acc_val: 0.8204 time: 30.4073s\n",
            "Epoch: 0125 loss_train: 0.6326 acc_train: 0.7941 loss_val: 0.5328 acc_val: 0.8208 time: 30.6575s\n",
            "Epoch: 0126 loss_train: 0.6253 acc_train: 0.7988 loss_val: 0.5358 acc_val: 0.8196 time: 30.9054s\n",
            "Epoch: 0127 loss_train: 0.6240 acc_train: 0.8014 loss_val: 0.5397 acc_val: 0.8137 time: 31.1571s\n",
            "Epoch: 0128 loss_train: 0.6198 acc_train: 0.8051 loss_val: 0.5374 acc_val: 0.8192 time: 31.3989s\n",
            "Epoch: 0129 loss_train: 0.6090 acc_train: 0.7969 loss_val: 0.5270 acc_val: 0.8275 time: 31.6437s\n",
            "Epoch: 0130 loss_train: 0.6105 acc_train: 0.8103 loss_val: 0.5246 acc_val: 0.8292 time: 31.8791s\n",
            "Epoch: 0131 loss_train: 0.6280 acc_train: 0.8001 loss_val: 0.5290 acc_val: 0.8275 time: 32.1330s\n",
            "Epoch: 0132 loss_train: 0.6212 acc_train: 0.8049 loss_val: 0.5315 acc_val: 0.8208 time: 32.3854s\n",
            "Epoch: 0133 loss_train: 0.6151 acc_train: 0.7987 loss_val: 0.5228 acc_val: 0.8237 time: 32.6279s\n",
            "Epoch: 0134 loss_train: 0.6070 acc_train: 0.8084 loss_val: 0.5201 acc_val: 0.8254 time: 32.8706s\n",
            "Epoch: 0135 loss_train: 0.6175 acc_train: 0.7949 loss_val: 0.5189 acc_val: 0.8242 time: 33.1228s\n",
            "Epoch: 0136 loss_train: 0.6165 acc_train: 0.7997 loss_val: 0.5219 acc_val: 0.8250 time: 33.3347s\n",
            "Epoch: 0137 loss_train: 0.6037 acc_train: 0.8037 loss_val: 0.5194 acc_val: 0.8296 time: 33.5444s\n",
            "Epoch: 0138 loss_train: 0.6134 acc_train: 0.8086 loss_val: 0.5140 acc_val: 0.8387 time: 33.7886s\n",
            "Epoch: 0139 loss_train: 0.6074 acc_train: 0.8125 loss_val: 0.5115 acc_val: 0.8346 time: 34.0467s\n",
            "Epoch: 0140 loss_train: 0.6040 acc_train: 0.8057 loss_val: 0.5131 acc_val: 0.8275 time: 34.2993s\n",
            "Epoch: 0141 loss_train: 0.6049 acc_train: 0.8055 loss_val: 0.5146 acc_val: 0.8250 time: 34.5488s\n",
            "Epoch: 0142 loss_train: 0.5929 acc_train: 0.8074 loss_val: 0.5099 acc_val: 0.8333 time: 34.7919s\n",
            "Epoch: 0143 loss_train: 0.5908 acc_train: 0.8101 loss_val: 0.5044 acc_val: 0.8354 time: 35.0394s\n",
            "Epoch: 0144 loss_train: 0.5958 acc_train: 0.8069 loss_val: 0.5072 acc_val: 0.8350 time: 35.2941s\n",
            "Epoch: 0145 loss_train: 0.5948 acc_train: 0.8095 loss_val: 0.5056 acc_val: 0.8367 time: 35.5548s\n",
            "Epoch: 0146 loss_train: 0.6099 acc_train: 0.8044 loss_val: 0.5007 acc_val: 0.8363 time: 35.7964s\n",
            "Epoch: 0147 loss_train: 0.5843 acc_train: 0.8118 loss_val: 0.5063 acc_val: 0.8396 time: 36.0466s\n",
            "Epoch: 0148 loss_train: 0.5888 acc_train: 0.8097 loss_val: 0.5043 acc_val: 0.8317 time: 36.3042s\n",
            "Epoch: 0149 loss_train: 0.5909 acc_train: 0.8068 loss_val: 0.4965 acc_val: 0.8337 time: 36.5493s\n",
            "Epoch: 0150 loss_train: 0.5822 acc_train: 0.8156 loss_val: 0.4942 acc_val: 0.8363 time: 36.7997s\n",
            "Epoch: 0151 loss_train: 0.5824 acc_train: 0.8160 loss_val: 0.4918 acc_val: 0.8387 time: 37.0599s\n",
            "Epoch: 0152 loss_train: 0.5809 acc_train: 0.8177 loss_val: 0.4941 acc_val: 0.8350 time: 37.3054s\n",
            "Epoch: 0153 loss_train: 0.5926 acc_train: 0.8111 loss_val: 0.4955 acc_val: 0.8333 time: 37.5618s\n",
            "Epoch: 0154 loss_train: 0.5759 acc_train: 0.8113 loss_val: 0.4894 acc_val: 0.8392 time: 37.8123s\n",
            "Epoch: 0155 loss_train: 0.5907 acc_train: 0.8111 loss_val: 0.4865 acc_val: 0.8392 time: 38.0568s\n",
            "Epoch: 0156 loss_train: 0.5724 acc_train: 0.8147 loss_val: 0.4855 acc_val: 0.8375 time: 38.3086s\n",
            "Epoch: 0157 loss_train: 0.5695 acc_train: 0.8225 loss_val: 0.4928 acc_val: 0.8358 time: 38.5549s\n",
            "Epoch: 0158 loss_train: 0.5854 acc_train: 0.8064 loss_val: 0.4887 acc_val: 0.8479 time: 38.7593s\n",
            "Epoch: 0159 loss_train: 0.5741 acc_train: 0.8203 loss_val: 0.4848 acc_val: 0.8454 time: 39.0144s\n",
            "Epoch: 0160 loss_train: 0.5656 acc_train: 0.8215 loss_val: 0.4871 acc_val: 0.8379 time: 39.2607s\n",
            "Epoch: 0161 loss_train: 0.5621 acc_train: 0.8184 loss_val: 0.4863 acc_val: 0.8383 time: 39.5067s\n",
            "Epoch: 0162 loss_train: 0.5742 acc_train: 0.8133 loss_val: 0.4781 acc_val: 0.8533 time: 39.7506s\n",
            "Epoch: 0163 loss_train: 0.5745 acc_train: 0.8222 loss_val: 0.4794 acc_val: 0.8525 time: 39.9862s\n",
            "Epoch: 0164 loss_train: 0.5594 acc_train: 0.8311 loss_val: 0.4749 acc_val: 0.8442 time: 40.2276s\n",
            "Epoch: 0165 loss_train: 0.5744 acc_train: 0.8176 loss_val: 0.4800 acc_val: 0.8425 time: 40.4729s\n",
            "Epoch: 0166 loss_train: 0.5692 acc_train: 0.8184 loss_val: 0.4687 acc_val: 0.8429 time: 40.6920s\n",
            "Epoch: 0167 loss_train: 0.5646 acc_train: 0.8167 loss_val: 0.4798 acc_val: 0.8475 time: 40.8716s\n",
            "Epoch: 0168 loss_train: 0.5737 acc_train: 0.8190 loss_val: 0.4718 acc_val: 0.8529 time: 41.1147s\n",
            "Epoch: 0169 loss_train: 0.5512 acc_train: 0.8278 loss_val: 0.4743 acc_val: 0.8446 time: 41.3579s\n",
            "Epoch: 0170 loss_train: 0.5590 acc_train: 0.8258 loss_val: 0.4739 acc_val: 0.8454 time: 41.6100s\n",
            "Epoch: 0171 loss_train: 0.5653 acc_train: 0.8237 loss_val: 0.4699 acc_val: 0.8425 time: 41.8399s\n",
            "Epoch: 0172 loss_train: 0.5473 acc_train: 0.8263 loss_val: 0.4764 acc_val: 0.8500 time: 42.0889s\n",
            "Epoch: 0173 loss_train: 0.5535 acc_train: 0.8266 loss_val: 0.4720 acc_val: 0.8571 time: 42.3422s\n",
            "Epoch: 0174 loss_train: 0.5458 acc_train: 0.8278 loss_val: 0.4648 acc_val: 0.8512 time: 42.5863s\n",
            "Epoch: 0175 loss_train: 0.5518 acc_train: 0.8253 loss_val: 0.4686 acc_val: 0.8438 time: 42.8365s\n",
            "Epoch: 0176 loss_train: 0.5532 acc_train: 0.8232 loss_val: 0.4594 acc_val: 0.8521 time: 43.0837s\n",
            "Epoch: 0177 loss_train: 0.5397 acc_train: 0.8277 loss_val: 0.4631 acc_val: 0.8571 time: 43.3335s\n",
            "Epoch: 0178 loss_train: 0.5411 acc_train: 0.8313 loss_val: 0.4606 acc_val: 0.8442 time: 43.5368s\n",
            "Epoch: 0179 loss_train: 0.5477 acc_train: 0.8268 loss_val: 0.4551 acc_val: 0.8458 time: 43.7927s\n",
            "Epoch: 0180 loss_train: 0.5431 acc_train: 0.8264 loss_val: 0.4569 acc_val: 0.8467 time: 44.0396s\n",
            "Epoch: 0181 loss_train: 0.5365 acc_train: 0.8314 loss_val: 0.4541 acc_val: 0.8442 time: 44.2949s\n",
            "Epoch: 0182 loss_train: 0.5324 acc_train: 0.8286 loss_val: 0.4527 acc_val: 0.8562 time: 44.4854s\n",
            "Epoch: 0183 loss_train: 0.5246 acc_train: 0.8366 loss_val: 0.4476 acc_val: 0.8612 time: 44.7317s\n",
            "Epoch: 0184 loss_train: 0.5356 acc_train: 0.8319 loss_val: 0.4460 acc_val: 0.8462 time: 44.9871s\n",
            "Epoch: 0185 loss_train: 0.5446 acc_train: 0.8247 loss_val: 0.4444 acc_val: 0.8579 time: 45.2304s\n",
            "Epoch: 0186 loss_train: 0.5272 acc_train: 0.8348 loss_val: 0.4428 acc_val: 0.8625 time: 45.4777s\n",
            "Epoch: 0187 loss_train: 0.5276 acc_train: 0.8322 loss_val: 0.4463 acc_val: 0.8562 time: 45.7325s\n",
            "Epoch: 0188 loss_train: 0.5261 acc_train: 0.8353 loss_val: 0.4421 acc_val: 0.8650 time: 45.9753s\n",
            "Epoch: 0189 loss_train: 0.5229 acc_train: 0.8416 loss_val: 0.4420 acc_val: 0.8483 time: 46.2247s\n",
            "Epoch: 0190 loss_train: 0.5101 acc_train: 0.8405 loss_val: 0.4434 acc_val: 0.8454 time: 46.4749s\n",
            "Epoch: 0191 loss_train: 0.5267 acc_train: 0.8269 loss_val: 0.4442 acc_val: 0.8567 time: 46.7289s\n",
            "Epoch: 0192 loss_train: 0.5264 acc_train: 0.8380 loss_val: 0.4433 acc_val: 0.8588 time: 46.9789s\n",
            "Epoch: 0193 loss_train: 0.5166 acc_train: 0.8425 loss_val: 0.4540 acc_val: 0.8400 time: 47.2233s\n",
            "Epoch: 0194 loss_train: 0.5268 acc_train: 0.8298 loss_val: 0.4319 acc_val: 0.8658 time: 47.4674s\n",
            "Epoch: 0195 loss_train: 0.5154 acc_train: 0.8412 loss_val: 0.4303 acc_val: 0.8654 time: 47.6945s\n",
            "Epoch: 0196 loss_train: 0.5082 acc_train: 0.8438 loss_val: 0.4421 acc_val: 0.8462 time: 47.9484s\n",
            "Epoch: 0197 loss_train: 0.5218 acc_train: 0.8287 loss_val: 0.4264 acc_val: 0.8633 time: 48.1910s\n",
            "Epoch: 0198 loss_train: 0.4935 acc_train: 0.8420 loss_val: 0.4243 acc_val: 0.8646 time: 48.4486s\n",
            "Epoch: 0199 loss_train: 0.5022 acc_train: 0.8454 loss_val: 0.4252 acc_val: 0.8554 time: 48.6910s\n",
            "Epoch: 0200 loss_train: 0.5059 acc_train: 0.8360 loss_val: 0.4237 acc_val: 0.8579 time: 48.9348s\n",
            "Epoch: 0201 loss_train: 0.4964 acc_train: 0.8393 loss_val: 0.4240 acc_val: 0.8633 time: 49.1777s\n",
            "Epoch: 0202 loss_train: 0.4984 acc_train: 0.8433 loss_val: 0.4238 acc_val: 0.8608 time: 49.4264s\n",
            "Epoch: 0203 loss_train: 0.4984 acc_train: 0.8386 loss_val: 0.4245 acc_val: 0.8571 time: 49.6764s\n",
            "Epoch: 0204 loss_train: 0.4974 acc_train: 0.8403 loss_val: 0.4214 acc_val: 0.8646 time: 49.8938s\n",
            "Epoch: 0205 loss_train: 0.4972 acc_train: 0.8454 loss_val: 0.4201 acc_val: 0.8650 time: 50.1389s\n",
            "Epoch: 0206 loss_train: 0.4920 acc_train: 0.8429 loss_val: 0.4173 acc_val: 0.8604 time: 50.3889s\n",
            "Epoch: 0207 loss_train: 0.5006 acc_train: 0.8374 loss_val: 0.4150 acc_val: 0.8662 time: 50.6413s\n",
            "Epoch: 0208 loss_train: 0.4912 acc_train: 0.8471 loss_val: 0.4145 acc_val: 0.8662 time: 50.8840s\n",
            "Epoch: 0209 loss_train: 0.4819 acc_train: 0.8479 loss_val: 0.4154 acc_val: 0.8667 time: 51.1343s\n",
            "Epoch: 0210 loss_train: 0.4842 acc_train: 0.8525 loss_val: 0.4149 acc_val: 0.8671 time: 51.3846s\n",
            "Epoch: 0211 loss_train: 0.5039 acc_train: 0.8415 loss_val: 0.4135 acc_val: 0.8646 time: 51.6304s\n",
            "Epoch: 0212 loss_train: 0.4904 acc_train: 0.8454 loss_val: 0.4122 acc_val: 0.8688 time: 51.8774s\n",
            "Epoch: 0213 loss_train: 0.4893 acc_train: 0.8500 loss_val: 0.4123 acc_val: 0.8617 time: 52.1253s\n",
            "Epoch: 0214 loss_train: 0.4835 acc_train: 0.8452 loss_val: 0.4104 acc_val: 0.8696 time: 52.3781s\n",
            "Epoch: 0215 loss_train: 0.4736 acc_train: 0.8520 loss_val: 0.4089 acc_val: 0.8688 time: 52.6299s\n",
            "Epoch: 0216 loss_train: 0.4748 acc_train: 0.8510 loss_val: 0.4147 acc_val: 0.8633 time: 52.8712s\n",
            "Epoch: 0217 loss_train: 0.4902 acc_train: 0.8436 loss_val: 0.4174 acc_val: 0.8617 time: 53.1218s\n",
            "Epoch: 0218 loss_train: 0.4915 acc_train: 0.8440 loss_val: 0.4189 acc_val: 0.8504 time: 53.3911s\n",
            "Epoch: 0219 loss_train: 0.4989 acc_train: 0.8360 loss_val: 0.4106 acc_val: 0.8679 time: 53.6403s\n",
            "Epoch: 0220 loss_train: 0.4758 acc_train: 0.8497 loss_val: 0.4074 acc_val: 0.8679 time: 53.8936s\n",
            "Epoch: 0221 loss_train: 0.4759 acc_train: 0.8496 loss_val: 0.4063 acc_val: 0.8696 time: 54.1426s\n",
            "Epoch: 0222 loss_train: 0.4850 acc_train: 0.8462 loss_val: 0.4096 acc_val: 0.8658 time: 54.3917s\n",
            "Epoch: 0223 loss_train: 0.4818 acc_train: 0.8418 loss_val: 0.4161 acc_val: 0.8638 time: 54.6359s\n",
            "Epoch: 0224 loss_train: 0.4921 acc_train: 0.8456 loss_val: 0.4218 acc_val: 0.8562 time: 54.8666s\n",
            "Epoch: 0225 loss_train: 0.4903 acc_train: 0.8357 loss_val: 0.4127 acc_val: 0.8675 time: 55.1095s\n",
            "Epoch: 0226 loss_train: 0.4765 acc_train: 0.8559 loss_val: 0.4137 acc_val: 0.8633 time: 55.3560s\n",
            "Epoch: 0227 loss_train: 0.4804 acc_train: 0.8481 loss_val: 0.4139 acc_val: 0.8562 time: 55.6071s\n",
            "Epoch: 0228 loss_train: 0.4754 acc_train: 0.8420 loss_val: 0.4025 acc_val: 0.8662 time: 55.8028s\n",
            "Epoch: 0229 loss_train: 0.4712 acc_train: 0.8505 loss_val: 0.4020 acc_val: 0.8679 time: 56.0528s\n",
            "Epoch: 0230 loss_train: 0.4729 acc_train: 0.8542 loss_val: 0.4085 acc_val: 0.8600 time: 56.2964s\n",
            "Epoch: 0231 loss_train: 0.4715 acc_train: 0.8455 loss_val: 0.4013 acc_val: 0.8671 time: 56.5426s\n",
            "Epoch: 0232 loss_train: 0.4780 acc_train: 0.8501 loss_val: 0.4021 acc_val: 0.8658 time: 56.7914s\n",
            "Epoch: 0233 loss_train: 0.4744 acc_train: 0.8482 loss_val: 0.4065 acc_val: 0.8633 time: 57.0105s\n",
            "Epoch: 0234 loss_train: 0.4712 acc_train: 0.8429 loss_val: 0.4022 acc_val: 0.8646 time: 57.2593s\n",
            "Epoch: 0235 loss_train: 0.4653 acc_train: 0.8549 loss_val: 0.4004 acc_val: 0.8671 time: 57.5087s\n",
            "Epoch: 0236 loss_train: 0.4654 acc_train: 0.8509 loss_val: 0.4146 acc_val: 0.8592 time: 57.7523s\n",
            "Epoch: 0237 loss_train: 0.4919 acc_train: 0.8368 loss_val: 0.4030 acc_val: 0.8721 time: 58.0080s\n",
            "Epoch: 0238 loss_train: 0.4674 acc_train: 0.8573 loss_val: 0.3999 acc_val: 0.8750 time: 58.2483s\n",
            "Epoch: 0239 loss_train: 0.4680 acc_train: 0.8587 loss_val: 0.4058 acc_val: 0.8579 time: 58.4339s\n",
            "Epoch: 0240 loss_train: 0.4699 acc_train: 0.8403 loss_val: 0.4081 acc_val: 0.8621 time: 58.6682s\n",
            "Epoch: 0241 loss_train: 0.4780 acc_train: 0.8492 loss_val: 0.4174 acc_val: 0.8625 time: 58.9154s\n",
            "Epoch: 0242 loss_train: 0.4826 acc_train: 0.8426 loss_val: 0.4002 acc_val: 0.8658 time: 59.1534s\n",
            "Epoch: 0243 loss_train: 0.4716 acc_train: 0.8503 loss_val: 0.4004 acc_val: 0.8679 time: 59.3941s\n",
            "Epoch: 0244 loss_train: 0.4669 acc_train: 0.8520 loss_val: 0.3964 acc_val: 0.8688 time: 59.6425s\n",
            "Epoch: 0245 loss_train: 0.4746 acc_train: 0.8541 loss_val: 0.3968 acc_val: 0.8708 time: 59.8948s\n",
            "Epoch: 0246 loss_train: 0.4677 acc_train: 0.8535 loss_val: 0.3947 acc_val: 0.8679 time: 60.1506s\n",
            "Epoch: 0247 loss_train: 0.4605 acc_train: 0.8516 loss_val: 0.3943 acc_val: 0.8662 time: 60.3947s\n",
            "Epoch: 0248 loss_train: 0.4581 acc_train: 0.8551 loss_val: 0.4000 acc_val: 0.8646 time: 60.6062s\n",
            "Epoch: 0249 loss_train: 0.4548 acc_train: 0.8546 loss_val: 0.3982 acc_val: 0.8683 time: 60.8526s\n",
            "Epoch: 0250 loss_train: 0.4633 acc_train: 0.8552 loss_val: 0.3970 acc_val: 0.8696 time: 61.0995s\n",
            "Epoch: 0251 loss_train: 0.4591 acc_train: 0.8581 loss_val: 0.3929 acc_val: 0.8696 time: 61.3487s\n",
            "Epoch: 0252 loss_train: 0.4571 acc_train: 0.8519 loss_val: 0.3892 acc_val: 0.8746 time: 61.6147s\n",
            "Epoch: 0253 loss_train: 0.4588 acc_train: 0.8562 loss_val: 0.3893 acc_val: 0.8725 time: 61.8607s\n",
            "Epoch: 0254 loss_train: 0.4585 acc_train: 0.8549 loss_val: 0.3913 acc_val: 0.8717 time: 62.1024s\n",
            "Epoch: 0255 loss_train: 0.4393 acc_train: 0.8609 loss_val: 0.3915 acc_val: 0.8679 time: 62.3483s\n",
            "Epoch: 0256 loss_train: 0.4538 acc_train: 0.8545 loss_val: 0.3905 acc_val: 0.8692 time: 62.5852s\n",
            "Epoch: 0257 loss_train: 0.4522 acc_train: 0.8522 loss_val: 0.3919 acc_val: 0.8708 time: 62.8161s\n",
            "Epoch: 0258 loss_train: 0.4617 acc_train: 0.8525 loss_val: 0.3908 acc_val: 0.8700 time: 63.0597s\n",
            "Epoch: 0259 loss_train: 0.4512 acc_train: 0.8512 loss_val: 0.3856 acc_val: 0.8704 time: 63.3084s\n",
            "Epoch: 0260 loss_train: 0.4453 acc_train: 0.8549 loss_val: 0.3866 acc_val: 0.8725 time: 63.5620s\n",
            "Epoch: 0261 loss_train: 0.4602 acc_train: 0.8567 loss_val: 0.3915 acc_val: 0.8650 time: 63.8081s\n",
            "Epoch: 0262 loss_train: 0.4557 acc_train: 0.8520 loss_val: 0.3893 acc_val: 0.8712 time: 64.0503s\n",
            "Epoch: 0263 loss_train: 0.4509 acc_train: 0.8632 loss_val: 0.3934 acc_val: 0.8708 time: 64.2288s\n",
            "Epoch: 0264 loss_train: 0.4645 acc_train: 0.8486 loss_val: 0.3916 acc_val: 0.8683 time: 64.4599s\n",
            "Epoch: 0265 loss_train: 0.4534 acc_train: 0.8555 loss_val: 0.3892 acc_val: 0.8721 time: 64.6384s\n",
            "Epoch: 0266 loss_train: 0.4578 acc_train: 0.8553 loss_val: 0.3852 acc_val: 0.8704 time: 64.8335s\n",
            "Epoch: 0267 loss_train: 0.4397 acc_train: 0.8623 loss_val: 0.3819 acc_val: 0.8708 time: 65.0778s\n",
            "Epoch: 0268 loss_train: 0.4518 acc_train: 0.8585 loss_val: 0.3854 acc_val: 0.8717 time: 65.3183s\n",
            "Epoch: 0269 loss_train: 0.4407 acc_train: 0.8595 loss_val: 0.3889 acc_val: 0.8696 time: 65.5350s\n",
            "Epoch: 0270 loss_train: 0.4470 acc_train: 0.8632 loss_val: 0.3858 acc_val: 0.8721 time: 65.7806s\n",
            "Epoch: 0271 loss_train: 0.4528 acc_train: 0.8547 loss_val: 0.3869 acc_val: 0.8708 time: 66.0224s\n",
            "Epoch: 0272 loss_train: 0.4482 acc_train: 0.8580 loss_val: 0.3901 acc_val: 0.8683 time: 66.2659s\n",
            "Epoch: 0273 loss_train: 0.4502 acc_train: 0.8578 loss_val: 0.3867 acc_val: 0.8746 time: 66.4500s\n",
            "Epoch: 0274 loss_train: 0.4573 acc_train: 0.8526 loss_val: 0.3880 acc_val: 0.8738 time: 66.7085s\n",
            "Epoch: 0275 loss_train: 0.4395 acc_train: 0.8627 loss_val: 0.3824 acc_val: 0.8725 time: 66.9527s\n",
            "Epoch: 0276 loss_train: 0.4476 acc_train: 0.8586 loss_val: 0.3815 acc_val: 0.8754 time: 67.1738s\n",
            "Epoch: 0277 loss_train: 0.4354 acc_train: 0.8663 loss_val: 0.3818 acc_val: 0.8729 time: 67.4196s\n",
            "Epoch: 0278 loss_train: 0.4333 acc_train: 0.8635 loss_val: 0.3897 acc_val: 0.8704 time: 67.6721s\n",
            "Epoch: 0279 loss_train: 0.4566 acc_train: 0.8481 loss_val: 0.3904 acc_val: 0.8717 time: 67.8584s\n",
            "Epoch: 0280 loss_train: 0.4518 acc_train: 0.8573 loss_val: 0.3870 acc_val: 0.8712 time: 68.0557s\n",
            "Epoch: 0281 loss_train: 0.4448 acc_train: 0.8610 loss_val: 0.4014 acc_val: 0.8562 time: 68.2999s\n",
            "Epoch: 0282 loss_train: 0.4545 acc_train: 0.8498 loss_val: 0.3941 acc_val: 0.8646 time: 68.5563s\n",
            "Epoch: 0283 loss_train: 0.4488 acc_train: 0.8557 loss_val: 0.3812 acc_val: 0.8725 time: 68.8001s\n",
            "Epoch: 0284 loss_train: 0.4338 acc_train: 0.8634 loss_val: 0.4104 acc_val: 0.8583 time: 69.0402s\n",
            "Epoch: 0285 loss_train: 0.4572 acc_train: 0.8460 loss_val: 0.4057 acc_val: 0.8700 time: 69.2838s\n",
            "Epoch: 0286 loss_train: 0.4734 acc_train: 0.8543 loss_val: 0.3836 acc_val: 0.8729 time: 69.5276s\n",
            "Epoch: 0287 loss_train: 0.4343 acc_train: 0.8637 loss_val: 0.4203 acc_val: 0.8521 time: 69.7612s\n",
            "Epoch: 0288 loss_train: 0.4674 acc_train: 0.8422 loss_val: 0.3844 acc_val: 0.8700 time: 69.9980s\n",
            "Epoch: 0289 loss_train: 0.4423 acc_train: 0.8568 loss_val: 0.3975 acc_val: 0.8712 time: 70.2403s\n",
            "Epoch: 0290 loss_train: 0.4510 acc_train: 0.8592 loss_val: 0.3875 acc_val: 0.8633 time: 70.4890s\n",
            "Epoch: 0291 loss_train: 0.4425 acc_train: 0.8527 loss_val: 0.3908 acc_val: 0.8633 time: 70.7393s\n",
            "Epoch: 0292 loss_train: 0.4478 acc_train: 0.8480 loss_val: 0.3903 acc_val: 0.8746 time: 70.9932s\n",
            "Epoch: 0293 loss_train: 0.4565 acc_train: 0.8554 loss_val: 0.3758 acc_val: 0.8746 time: 71.2429s\n",
            "Epoch: 0294 loss_train: 0.4391 acc_train: 0.8656 loss_val: 0.4013 acc_val: 0.8588 time: 71.4899s\n",
            "Epoch: 0295 loss_train: 0.4475 acc_train: 0.8501 loss_val: 0.3817 acc_val: 0.8729 time: 71.7306s\n",
            "Epoch: 0296 loss_train: 0.4387 acc_train: 0.8643 loss_val: 0.3922 acc_val: 0.8712 time: 71.9887s\n",
            "Epoch: 0297 loss_train: 0.4533 acc_train: 0.8611 loss_val: 0.3806 acc_val: 0.8725 time: 72.2350s\n",
            "Epoch: 0298 loss_train: 0.4224 acc_train: 0.8671 loss_val: 0.4085 acc_val: 0.8562 time: 72.4830s\n",
            "Epoch: 0299 loss_train: 0.4604 acc_train: 0.8418 loss_val: 0.3813 acc_val: 0.8758 time: 72.7226s\n",
            "Epoch: 0300 loss_train: 0.4389 acc_train: 0.8656 loss_val: 0.3915 acc_val: 0.8717 time: 72.9645s\n",
            "Epoch: 0301 loss_train: 0.4475 acc_train: 0.8611 loss_val: 0.3877 acc_val: 0.8717 time: 73.2111s\n",
            "Epoch: 0302 loss_train: 0.4354 acc_train: 0.8567 loss_val: 0.3938 acc_val: 0.8571 time: 73.4643s\n",
            "Epoch: 0303 loss_train: 0.4520 acc_train: 0.8504 loss_val: 0.3847 acc_val: 0.8746 time: 73.7207s\n",
            "Epoch: 0304 loss_train: 0.4466 acc_train: 0.8605 loss_val: 0.3829 acc_val: 0.8704 time: 73.9671s\n",
            "Epoch: 0305 loss_train: 0.4433 acc_train: 0.8573 loss_val: 0.3907 acc_val: 0.8679 time: 74.2139s\n",
            "Epoch: 0306 loss_train: 0.4427 acc_train: 0.8546 loss_val: 0.3832 acc_val: 0.8671 time: 74.4560s\n",
            "Epoch: 0307 loss_train: 0.4378 acc_train: 0.8505 loss_val: 0.3815 acc_val: 0.8767 time: 74.7053s\n",
            "Epoch: 0308 loss_train: 0.4412 acc_train: 0.8646 loss_val: 0.3746 acc_val: 0.8767 time: 74.9468s\n",
            "Epoch: 0309 loss_train: 0.4254 acc_train: 0.8677 loss_val: 0.3873 acc_val: 0.8708 time: 75.1910s\n",
            "Epoch: 0310 loss_train: 0.4404 acc_train: 0.8607 loss_val: 0.3744 acc_val: 0.8762 time: 75.4326s\n",
            "Epoch: 0311 loss_train: 0.4224 acc_train: 0.8662 loss_val: 0.3757 acc_val: 0.8788 time: 75.6875s\n",
            "Epoch: 0312 loss_train: 0.4382 acc_train: 0.8595 loss_val: 0.3723 acc_val: 0.8729 time: 75.9324s\n",
            "Epoch: 0313 loss_train: 0.4245 acc_train: 0.8642 loss_val: 0.3801 acc_val: 0.8708 time: 76.1762s\n",
            "Epoch: 0314 loss_train: 0.4269 acc_train: 0.8629 loss_val: 0.3724 acc_val: 0.8767 time: 76.4200s\n",
            "Epoch: 0315 loss_train: 0.4280 acc_train: 0.8652 loss_val: 0.3728 acc_val: 0.8792 time: 76.6484s\n",
            "Epoch: 0316 loss_train: 0.4290 acc_train: 0.8685 loss_val: 0.3737 acc_val: 0.8733 time: 76.8730s\n",
            "Epoch: 0317 loss_train: 0.4218 acc_train: 0.8673 loss_val: 0.3761 acc_val: 0.8733 time: 77.1210s\n",
            "Epoch: 0318 loss_train: 0.4339 acc_train: 0.8565 loss_val: 0.3770 acc_val: 0.8729 time: 77.3645s\n",
            "Epoch: 0319 loss_train: 0.4385 acc_train: 0.8638 loss_val: 0.3702 acc_val: 0.8771 time: 77.6053s\n",
            "Epoch: 0320 loss_train: 0.4220 acc_train: 0.8664 loss_val: 0.3717 acc_val: 0.8742 time: 77.8108s\n",
            "Epoch: 0321 loss_train: 0.4189 acc_train: 0.8673 loss_val: 0.3700 acc_val: 0.8733 time: 78.0597s\n",
            "Epoch: 0322 loss_train: 0.4267 acc_train: 0.8616 loss_val: 0.3693 acc_val: 0.8771 time: 78.3056s\n",
            "Epoch: 0323 loss_train: 0.4151 acc_train: 0.8702 loss_val: 0.3713 acc_val: 0.8733 time: 78.5639s\n",
            "Epoch: 0324 loss_train: 0.4127 acc_train: 0.8676 loss_val: 0.3721 acc_val: 0.8725 time: 78.8136s\n",
            "Epoch: 0325 loss_train: 0.4363 acc_train: 0.8581 loss_val: 0.3709 acc_val: 0.8792 time: 79.0613s\n",
            "Epoch: 0326 loss_train: 0.4238 acc_train: 0.8684 loss_val: 0.3696 acc_val: 0.8788 time: 79.3067s\n",
            "Epoch: 0327 loss_train: 0.4214 acc_train: 0.8668 loss_val: 0.3740 acc_val: 0.8783 time: 79.5504s\n",
            "Epoch: 0328 loss_train: 0.4209 acc_train: 0.8684 loss_val: 0.3736 acc_val: 0.8738 time: 79.8021s\n",
            "Epoch: 0329 loss_train: 0.4214 acc_train: 0.8675 loss_val: 0.3684 acc_val: 0.8762 time: 80.0459s\n",
            "Epoch: 0330 loss_train: 0.4179 acc_train: 0.8696 loss_val: 0.3689 acc_val: 0.8758 time: 80.2919s\n",
            "Epoch: 0331 loss_train: 0.4203 acc_train: 0.8680 loss_val: 0.3770 acc_val: 0.8729 time: 80.5432s\n",
            "Epoch: 0332 loss_train: 0.4139 acc_train: 0.8652 loss_val: 0.3697 acc_val: 0.8758 time: 80.7944s\n",
            "Epoch: 0333 loss_train: 0.4170 acc_train: 0.8685 loss_val: 0.3685 acc_val: 0.8775 time: 81.0218s\n",
            "Epoch: 0334 loss_train: 0.4164 acc_train: 0.8711 loss_val: 0.3704 acc_val: 0.8742 time: 81.2597s\n",
            "Epoch: 0335 loss_train: 0.4268 acc_train: 0.8610 loss_val: 0.3729 acc_val: 0.8733 time: 81.4504s\n",
            "Epoch: 0336 loss_train: 0.4316 acc_train: 0.8581 loss_val: 0.3734 acc_val: 0.8750 time: 81.6968s\n",
            "Epoch: 0337 loss_train: 0.4387 acc_train: 0.8581 loss_val: 0.3669 acc_val: 0.8792 time: 81.9519s\n",
            "Epoch: 0338 loss_train: 0.4170 acc_train: 0.8710 loss_val: 0.3836 acc_val: 0.8654 time: 82.1995s\n",
            "Epoch: 0339 loss_train: 0.4279 acc_train: 0.8604 loss_val: 0.3677 acc_val: 0.8800 time: 82.4446s\n",
            "Epoch: 0340 loss_train: 0.4197 acc_train: 0.8701 loss_val: 0.3719 acc_val: 0.8754 time: 82.6893s\n",
            "Epoch: 0341 loss_train: 0.4226 acc_train: 0.8667 loss_val: 0.3710 acc_val: 0.8742 time: 82.9358s\n",
            "Epoch: 0342 loss_train: 0.4163 acc_train: 0.8682 loss_val: 0.3743 acc_val: 0.8692 time: 83.1683s\n",
            "Epoch: 0343 loss_train: 0.4243 acc_train: 0.8634 loss_val: 0.3724 acc_val: 0.8775 time: 83.4152s\n",
            "Epoch: 0344 loss_train: 0.4353 acc_train: 0.8646 loss_val: 0.3673 acc_val: 0.8829 time: 83.6680s\n",
            "Epoch: 0345 loss_train: 0.4105 acc_train: 0.8657 loss_val: 0.3824 acc_val: 0.8700 time: 83.8987s\n",
            "Epoch: 0346 loss_train: 0.4349 acc_train: 0.8571 loss_val: 0.3651 acc_val: 0.8775 time: 84.1458s\n",
            "Epoch: 0347 loss_train: 0.4232 acc_train: 0.8676 loss_val: 0.3693 acc_val: 0.8762 time: 84.3883s\n",
            "Epoch: 0348 loss_train: 0.4153 acc_train: 0.8718 loss_val: 0.3829 acc_val: 0.8688 time: 84.6384s\n",
            "Epoch: 0349 loss_train: 0.4305 acc_train: 0.8619 loss_val: 0.3666 acc_val: 0.8804 time: 84.8864s\n",
            "Epoch: 0350 loss_train: 0.4191 acc_train: 0.8669 loss_val: 0.3776 acc_val: 0.8804 time: 85.1370s\n",
            "Epoch: 0351 loss_train: 0.4321 acc_train: 0.8727 loss_val: 0.3692 acc_val: 0.8800 time: 85.3798s\n",
            "Epoch: 0352 loss_train: 0.4203 acc_train: 0.8678 loss_val: 0.3725 acc_val: 0.8750 time: 85.5694s\n",
            "Epoch: 0353 loss_train: 0.4277 acc_train: 0.8586 loss_val: 0.3728 acc_val: 0.8742 time: 85.7881s\n",
            "Epoch: 0354 loss_train: 0.4306 acc_train: 0.8565 loss_val: 0.3686 acc_val: 0.8754 time: 86.0274s\n",
            "Epoch: 0355 loss_train: 0.4241 acc_train: 0.8655 loss_val: 0.3873 acc_val: 0.8654 time: 86.2859s\n",
            "Epoch: 0356 loss_train: 0.4351 acc_train: 0.8535 loss_val: 0.3679 acc_val: 0.8779 time: 86.5330s\n",
            "Epoch: 0357 loss_train: 0.4153 acc_train: 0.8744 loss_val: 0.3684 acc_val: 0.8792 time: 86.7877s\n",
            "Epoch: 0358 loss_train: 0.4140 acc_train: 0.8712 loss_val: 0.3628 acc_val: 0.8800 time: 87.0362s\n",
            "Epoch: 0359 loss_train: 0.4110 acc_train: 0.8711 loss_val: 0.3665 acc_val: 0.8750 time: 87.2635s\n",
            "Epoch: 0360 loss_train: 0.4180 acc_train: 0.8684 loss_val: 0.3649 acc_val: 0.8792 time: 87.4546s\n",
            "Epoch: 0361 loss_train: 0.4207 acc_train: 0.8638 loss_val: 0.3609 acc_val: 0.8783 time: 87.6940s\n",
            "Epoch: 0362 loss_train: 0.4028 acc_train: 0.8723 loss_val: 0.3696 acc_val: 0.8754 time: 87.9487s\n",
            "Epoch: 0363 loss_train: 0.4205 acc_train: 0.8654 loss_val: 0.3628 acc_val: 0.8775 time: 88.1871s\n",
            "Epoch: 0364 loss_train: 0.4083 acc_train: 0.8696 loss_val: 0.3649 acc_val: 0.8800 time: 88.4342s\n",
            "Epoch: 0365 loss_train: 0.4110 acc_train: 0.8705 loss_val: 0.3624 acc_val: 0.8812 time: 88.6893s\n",
            "Epoch: 0366 loss_train: 0.4045 acc_train: 0.8741 loss_val: 0.3692 acc_val: 0.8746 time: 88.9498s\n",
            "Epoch: 0367 loss_train: 0.4102 acc_train: 0.8698 loss_val: 0.3599 acc_val: 0.8812 time: 89.2065s\n",
            "Epoch: 0368 loss_train: 0.4225 acc_train: 0.8679 loss_val: 0.3617 acc_val: 0.8792 time: 89.4517s\n",
            "Epoch: 0369 loss_train: 0.4064 acc_train: 0.8734 loss_val: 0.3631 acc_val: 0.8775 time: 89.6985s\n",
            "Epoch: 0370 loss_train: 0.4192 acc_train: 0.8647 loss_val: 0.3612 acc_val: 0.8779 time: 89.9516s\n",
            "Epoch: 0371 loss_train: 0.4066 acc_train: 0.8671 loss_val: 0.3597 acc_val: 0.8808 time: 90.2086s\n",
            "Epoch: 0372 loss_train: 0.4054 acc_train: 0.8734 loss_val: 0.3585 acc_val: 0.8804 time: 90.4604s\n",
            "Epoch: 0373 loss_train: 0.3995 acc_train: 0.8742 loss_val: 0.3619 acc_val: 0.8779 time: 90.6578s\n",
            "Epoch: 0374 loss_train: 0.4111 acc_train: 0.8686 loss_val: 0.3577 acc_val: 0.8817 time: 90.9110s\n",
            "Epoch: 0375 loss_train: 0.4071 acc_train: 0.8720 loss_val: 0.3581 acc_val: 0.8808 time: 91.1542s\n",
            "Epoch: 0376 loss_train: 0.3985 acc_train: 0.8745 loss_val: 0.3580 acc_val: 0.8825 time: 91.3983s\n",
            "Epoch: 0377 loss_train: 0.4129 acc_train: 0.8692 loss_val: 0.3584 acc_val: 0.8833 time: 91.6447s\n",
            "Epoch: 0378 loss_train: 0.4077 acc_train: 0.8738 loss_val: 0.3569 acc_val: 0.8821 time: 91.8946s\n",
            "Epoch: 0379 loss_train: 0.3987 acc_train: 0.8773 loss_val: 0.3700 acc_val: 0.8792 time: 92.1456s\n",
            "Epoch: 0380 loss_train: 0.4005 acc_train: 0.8759 loss_val: 0.3598 acc_val: 0.8779 time: 92.3897s\n",
            "Epoch: 0381 loss_train: 0.4120 acc_train: 0.8656 loss_val: 0.3672 acc_val: 0.8829 time: 92.6350s\n",
            "Epoch: 0382 loss_train: 0.4225 acc_train: 0.8718 loss_val: 0.3735 acc_val: 0.8717 time: 92.8578s\n",
            "Epoch: 0383 loss_train: 0.4116 acc_train: 0.8641 loss_val: 0.3629 acc_val: 0.8804 time: 93.1101s\n",
            "Epoch: 0384 loss_train: 0.4023 acc_train: 0.8749 loss_val: 0.3665 acc_val: 0.8783 time: 93.3546s\n",
            "Epoch: 0385 loss_train: 0.4198 acc_train: 0.8667 loss_val: 0.3646 acc_val: 0.8746 time: 93.6080s\n",
            "Epoch: 0386 loss_train: 0.3968 acc_train: 0.8679 loss_val: 0.3616 acc_val: 0.8804 time: 93.8606s\n",
            "Epoch: 0387 loss_train: 0.3987 acc_train: 0.8745 loss_val: 0.3654 acc_val: 0.8771 time: 94.1184s\n",
            "Epoch: 0388 loss_train: 0.4233 acc_train: 0.8668 loss_val: 0.3630 acc_val: 0.8821 time: 94.3607s\n",
            "Epoch: 0389 loss_train: 0.4097 acc_train: 0.8715 loss_val: 0.3640 acc_val: 0.8775 time: 94.6079s\n",
            "Epoch: 0390 loss_train: 0.4087 acc_train: 0.8719 loss_val: 0.3605 acc_val: 0.8842 time: 94.8627s\n",
            "Epoch: 0391 loss_train: 0.4084 acc_train: 0.8781 loss_val: 0.3579 acc_val: 0.8808 time: 95.0985s\n",
            "Epoch: 0392 loss_train: 0.3925 acc_train: 0.8747 loss_val: 0.3698 acc_val: 0.8812 time: 95.3419s\n",
            "Epoch: 0393 loss_train: 0.4051 acc_train: 0.8695 loss_val: 0.3554 acc_val: 0.8829 time: 95.5858s\n",
            "Epoch: 0394 loss_train: 0.4051 acc_train: 0.8725 loss_val: 0.3619 acc_val: 0.8817 time: 95.8292s\n",
            "Epoch: 0395 loss_train: 0.4086 acc_train: 0.8714 loss_val: 0.3741 acc_val: 0.8725 time: 96.0817s\n",
            "Epoch: 0396 loss_train: 0.4057 acc_train: 0.8667 loss_val: 0.3671 acc_val: 0.8788 time: 96.3263s\n",
            "Epoch: 0397 loss_train: 0.4077 acc_train: 0.8676 loss_val: 0.3631 acc_val: 0.8783 time: 96.5815s\n",
            "Epoch: 0398 loss_train: 0.4094 acc_train: 0.8696 loss_val: 0.3771 acc_val: 0.8688 time: 96.8366s\n",
            "Epoch: 0399 loss_train: 0.4198 acc_train: 0.8571 loss_val: 0.3569 acc_val: 0.8825 time: 97.0877s\n",
            "Epoch: 0400 loss_train: 0.4048 acc_train: 0.8749 loss_val: 0.3708 acc_val: 0.8788 time: 97.3322s\n",
            "Epoch: 0401 loss_train: 0.4210 acc_train: 0.8686 loss_val: 0.3642 acc_val: 0.8821 time: 97.5594s\n",
            "Epoch: 0402 loss_train: 0.4060 acc_train: 0.8713 loss_val: 0.3741 acc_val: 0.8712 time: 97.8027s\n",
            "Epoch: 0403 loss_train: 0.4178 acc_train: 0.8600 loss_val: 0.3824 acc_val: 0.8767 time: 98.0610s\n",
            "Epoch: 0404 loss_train: 0.4372 acc_train: 0.8668 loss_val: 0.3779 acc_val: 0.8729 time: 98.3145s\n",
            "Epoch: 0405 loss_train: 0.4111 acc_train: 0.8673 loss_val: 0.3861 acc_val: 0.8742 time: 98.5591s\n",
            "Epoch: 0406 loss_train: 0.4173 acc_train: 0.8657 loss_val: 0.3635 acc_val: 0.8854 time: 98.8035s\n",
            "Epoch: 0407 loss_train: 0.4052 acc_train: 0.8734 loss_val: 0.3746 acc_val: 0.8804 time: 99.0593s\n",
            "Epoch: 0408 loss_train: 0.4224 acc_train: 0.8678 loss_val: 0.3799 acc_val: 0.8721 time: 99.3011s\n",
            "Epoch: 0409 loss_train: 0.4242 acc_train: 0.8621 loss_val: 0.3729 acc_val: 0.8758 time: 99.5492s\n",
            "Epoch: 0410 loss_train: 0.4113 acc_train: 0.8703 loss_val: 0.3587 acc_val: 0.8854 time: 99.8053s\n",
            "Epoch: 0411 loss_train: 0.4043 acc_train: 0.8758 loss_val: 0.3750 acc_val: 0.8738 time: 100.0448s\n",
            "Epoch: 0412 loss_train: 0.4179 acc_train: 0.8670 loss_val: 0.3739 acc_val: 0.8683 time: 100.2874s\n",
            "Epoch: 0413 loss_train: 0.4143 acc_train: 0.8654 loss_val: 0.3797 acc_val: 0.8704 time: 100.5141s\n",
            "Epoch: 0414 loss_train: 0.4318 acc_train: 0.8560 loss_val: 0.3631 acc_val: 0.8788 time: 100.7597s\n",
            "Epoch: 0415 loss_train: 0.4015 acc_train: 0.8753 loss_val: 0.3788 acc_val: 0.8667 time: 101.0090s\n",
            "Epoch: 0416 loss_train: 0.4194 acc_train: 0.8591 loss_val: 0.3634 acc_val: 0.8754 time: 101.2499s\n",
            "Epoch: 0417 loss_train: 0.3947 acc_train: 0.8744 loss_val: 0.3729 acc_val: 0.8775 time: 101.5038s\n",
            "Epoch: 0418 loss_train: 0.4160 acc_train: 0.8714 loss_val: 0.3561 acc_val: 0.8808 time: 101.7553s\n",
            "Epoch: 0419 loss_train: 0.3893 acc_train: 0.8770 loss_val: 0.3819 acc_val: 0.8646 time: 102.0053s\n",
            "Epoch: 0420 loss_train: 0.4225 acc_train: 0.8571 loss_val: 0.3614 acc_val: 0.8796 time: 102.2554s\n",
            "Epoch: 0421 loss_train: 0.4097 acc_train: 0.8710 loss_val: 0.3632 acc_val: 0.8779 time: 102.4987s\n",
            "Epoch: 0422 loss_train: 0.4039 acc_train: 0.8742 loss_val: 0.3705 acc_val: 0.8804 time: 102.7473s\n",
            "Epoch: 0423 loss_train: 0.4084 acc_train: 0.8660 loss_val: 0.3667 acc_val: 0.8767 time: 102.9980s\n",
            "Epoch: 0424 loss_train: 0.3984 acc_train: 0.8679 loss_val: 0.3625 acc_val: 0.8842 time: 103.2452s\n",
            "Epoch: 0425 loss_train: 0.4087 acc_train: 0.8749 loss_val: 0.3552 acc_val: 0.8796 time: 103.4848s\n",
            "Epoch: 0426 loss_train: 0.3948 acc_train: 0.8744 loss_val: 0.3689 acc_val: 0.8775 time: 103.7256s\n",
            "Epoch: 0427 loss_train: 0.4213 acc_train: 0.8624 loss_val: 0.3523 acc_val: 0.8825 time: 103.9857s\n",
            "Epoch: 0428 loss_train: 0.3936 acc_train: 0.8748 loss_val: 0.3646 acc_val: 0.8858 time: 104.2457s\n",
            "Epoch: 0429 loss_train: 0.4172 acc_train: 0.8757 loss_val: 0.3761 acc_val: 0.8783 time: 104.4880s\n",
            "Epoch: 0430 loss_train: 0.4093 acc_train: 0.8705 loss_val: 0.3683 acc_val: 0.8812 time: 104.7327s\n",
            "Epoch: 0431 loss_train: 0.4031 acc_train: 0.8698 loss_val: 0.3589 acc_val: 0.8800 time: 104.9806s\n",
            "Epoch: 0432 loss_train: 0.4080 acc_train: 0.8725 loss_val: 0.3582 acc_val: 0.8817 time: 105.2395s\n",
            "Epoch: 0433 loss_train: 0.3943 acc_train: 0.8753 loss_val: 0.3796 acc_val: 0.8671 time: 105.4819s\n",
            "Epoch: 0434 loss_train: 0.4148 acc_train: 0.8582 loss_val: 0.3550 acc_val: 0.8846 time: 105.7266s\n",
            "Epoch: 0435 loss_train: 0.3890 acc_train: 0.8823 loss_val: 0.3686 acc_val: 0.8842 time: 105.9694s\n",
            "Epoch: 0436 loss_train: 0.4200 acc_train: 0.8675 loss_val: 0.3547 acc_val: 0.8854 time: 106.2212s\n",
            "Epoch: 0437 loss_train: 0.3946 acc_train: 0.8780 loss_val: 0.3744 acc_val: 0.8683 time: 106.4666s\n",
            "Epoch: 0438 loss_train: 0.4151 acc_train: 0.8600 loss_val: 0.3524 acc_val: 0.8812 time: 106.7161s\n",
            "Epoch: 0439 loss_train: 0.3990 acc_train: 0.8782 loss_val: 0.3602 acc_val: 0.8788 time: 106.9608s\n",
            "Epoch: 0440 loss_train: 0.4055 acc_train: 0.8719 loss_val: 0.3549 acc_val: 0.8817 time: 107.2100s\n",
            "Epoch: 0441 loss_train: 0.3923 acc_train: 0.8720 loss_val: 0.3600 acc_val: 0.8808 time: 107.4505s\n",
            "Epoch: 0442 loss_train: 0.3999 acc_train: 0.8733 loss_val: 0.3527 acc_val: 0.8867 time: 107.6977s\n",
            "Epoch: 0443 loss_train: 0.3999 acc_train: 0.8735 loss_val: 0.3554 acc_val: 0.8817 time: 107.9397s\n",
            "Epoch: 0444 loss_train: 0.3947 acc_train: 0.8719 loss_val: 0.3570 acc_val: 0.8812 time: 108.1901s\n",
            "Epoch: 0445 loss_train: 0.3886 acc_train: 0.8745 loss_val: 0.3504 acc_val: 0.8838 time: 108.4383s\n",
            "Epoch: 0446 loss_train: 0.3979 acc_train: 0.8747 loss_val: 0.3486 acc_val: 0.8858 time: 108.6851s\n",
            "Epoch: 0447 loss_train: 0.3889 acc_train: 0.8755 loss_val: 0.3487 acc_val: 0.8867 time: 108.9381s\n",
            "Epoch: 0448 loss_train: 0.3936 acc_train: 0.8765 loss_val: 0.3544 acc_val: 0.8858 time: 109.1941s\n",
            "Epoch: 0449 loss_train: 0.4035 acc_train: 0.8729 loss_val: 0.3505 acc_val: 0.8842 time: 109.4249s\n",
            "Epoch: 0450 loss_train: 0.3870 acc_train: 0.8758 loss_val: 0.3515 acc_val: 0.8867 time: 109.6684s\n",
            "Epoch: 0451 loss_train: 0.4000 acc_train: 0.8747 loss_val: 0.3671 acc_val: 0.8754 time: 109.9084s\n",
            "Epoch: 0452 loss_train: 0.4095 acc_train: 0.8653 loss_val: 0.3559 acc_val: 0.8846 time: 110.1702s\n",
            "Epoch: 0453 loss_train: 0.3947 acc_train: 0.8736 loss_val: 0.3633 acc_val: 0.8871 time: 110.4145s\n",
            "Epoch: 0454 loss_train: 0.4053 acc_train: 0.8793 loss_val: 0.3731 acc_val: 0.8725 time: 110.6639s\n",
            "Epoch: 0455 loss_train: 0.4235 acc_train: 0.8569 loss_val: 0.3590 acc_val: 0.8796 time: 110.9092s\n",
            "Epoch: 0456 loss_train: 0.3917 acc_train: 0.8769 loss_val: 0.3681 acc_val: 0.8754 time: 111.1628s\n",
            "Epoch: 0457 loss_train: 0.4178 acc_train: 0.8666 loss_val: 0.3519 acc_val: 0.8875 time: 111.4023s\n",
            "Epoch: 0458 loss_train: 0.3860 acc_train: 0.8803 loss_val: 0.3775 acc_val: 0.8738 time: 111.6544s\n",
            "Epoch: 0459 loss_train: 0.4109 acc_train: 0.8644 loss_val: 0.3506 acc_val: 0.8862 time: 111.9068s\n",
            "Epoch: 0460 loss_train: 0.3856 acc_train: 0.8837 loss_val: 0.3706 acc_val: 0.8750 time: 112.1609s\n",
            "Epoch: 0461 loss_train: 0.4097 acc_train: 0.8648 loss_val: 0.3538 acc_val: 0.8833 time: 112.4010s\n",
            "Epoch: 0462 loss_train: 0.3838 acc_train: 0.8765 loss_val: 0.3587 acc_val: 0.8842 time: 112.6470s\n",
            "Epoch: 0463 loss_train: 0.3948 acc_train: 0.8746 loss_val: 0.3622 acc_val: 0.8838 time: 112.8934s\n",
            "Epoch: 0464 loss_train: 0.4015 acc_train: 0.8762 loss_val: 0.3530 acc_val: 0.8833 time: 113.1422s\n",
            "Epoch: 0465 loss_train: 0.4026 acc_train: 0.8699 loss_val: 0.3665 acc_val: 0.8788 time: 113.3858s\n",
            "Epoch: 0466 loss_train: 0.4075 acc_train: 0.8685 loss_val: 0.3522 acc_val: 0.8871 time: 113.6265s\n",
            "Epoch: 0467 loss_train: 0.3909 acc_train: 0.8824 loss_val: 0.3540 acc_val: 0.8875 time: 113.8740s\n",
            "Epoch: 0468 loss_train: 0.3880 acc_train: 0.8779 loss_val: 0.3585 acc_val: 0.8825 time: 114.1239s\n",
            "Epoch: 0469 loss_train: 0.3971 acc_train: 0.8707 loss_val: 0.3583 acc_val: 0.8812 time: 114.3667s\n",
            "Epoch: 0470 loss_train: 0.3964 acc_train: 0.8751 loss_val: 0.3532 acc_val: 0.8829 time: 114.6167s\n",
            "Epoch: 0471 loss_train: 0.3880 acc_train: 0.8753 loss_val: 0.3519 acc_val: 0.8825 time: 114.8617s\n",
            "Epoch: 0472 loss_train: 0.3901 acc_train: 0.8755 loss_val: 0.3567 acc_val: 0.8808 time: 115.1149s\n",
            "Epoch: 0473 loss_train: 0.3997 acc_train: 0.8745 loss_val: 0.3520 acc_val: 0.8862 time: 115.3573s\n",
            "Epoch: 0474 loss_train: 0.3882 acc_train: 0.8755 loss_val: 0.3584 acc_val: 0.8821 time: 115.6071s\n",
            "Epoch: 0475 loss_train: 0.3964 acc_train: 0.8732 loss_val: 0.3480 acc_val: 0.8850 time: 115.8515s\n",
            "Epoch: 0476 loss_train: 0.3909 acc_train: 0.8758 loss_val: 0.3516 acc_val: 0.8812 time: 116.1030s\n",
            "Epoch: 0477 loss_train: 0.3902 acc_train: 0.8755 loss_val: 0.3484 acc_val: 0.8875 time: 116.3450s\n",
            "Epoch: 0478 loss_train: 0.3865 acc_train: 0.8776 loss_val: 0.3508 acc_val: 0.8854 time: 116.5916s\n",
            "Epoch: 0479 loss_train: 0.3839 acc_train: 0.8760 loss_val: 0.3545 acc_val: 0.8846 time: 116.8441s\n",
            "Epoch: 0480 loss_train: 0.3917 acc_train: 0.8729 loss_val: 0.3470 acc_val: 0.8854 time: 117.1036s\n",
            "Epoch: 0481 loss_train: 0.3905 acc_train: 0.8758 loss_val: 0.3488 acc_val: 0.8854 time: 117.3528s\n",
            "Epoch: 0482 loss_train: 0.3873 acc_train: 0.8764 loss_val: 0.3551 acc_val: 0.8854 time: 117.5972s\n",
            "Epoch: 0483 loss_train: 0.3901 acc_train: 0.8712 loss_val: 0.3516 acc_val: 0.8867 time: 117.8446s\n",
            "Epoch: 0484 loss_train: 0.3828 acc_train: 0.8785 loss_val: 0.3525 acc_val: 0.8900 time: 118.0930s\n",
            "Epoch: 0485 loss_train: 0.3912 acc_train: 0.8827 loss_val: 0.3522 acc_val: 0.8846 time: 118.3446s\n",
            "Epoch: 0486 loss_train: 0.3845 acc_train: 0.8771 loss_val: 0.3476 acc_val: 0.8842 time: 118.5923s\n",
            "Epoch: 0487 loss_train: 0.3874 acc_train: 0.8760 loss_val: 0.3489 acc_val: 0.8825 time: 118.8385s\n",
            "Epoch: 0488 loss_train: 0.3893 acc_train: 0.8785 loss_val: 0.3485 acc_val: 0.8808 time: 119.0979s\n",
            "Epoch: 0489 loss_train: 0.3864 acc_train: 0.8757 loss_val: 0.3530 acc_val: 0.8808 time: 119.3574s\n",
            "Epoch: 0490 loss_train: 0.3894 acc_train: 0.8747 loss_val: 0.3456 acc_val: 0.8875 time: 119.6018s\n",
            "Epoch: 0491 loss_train: 0.3812 acc_train: 0.8803 loss_val: 0.3572 acc_val: 0.8908 time: 119.8444s\n",
            "Epoch: 0492 loss_train: 0.3999 acc_train: 0.8788 loss_val: 0.3554 acc_val: 0.8858 time: 120.0916s\n",
            "Epoch: 0493 loss_train: 0.3777 acc_train: 0.8781 loss_val: 0.3566 acc_val: 0.8808 time: 120.3362s\n",
            "Epoch: 0494 loss_train: 0.3919 acc_train: 0.8718 loss_val: 0.3583 acc_val: 0.8846 time: 120.5914s\n",
            "Epoch: 0495 loss_train: 0.3946 acc_train: 0.8719 loss_val: 0.3473 acc_val: 0.8862 time: 120.8374s\n",
            "Epoch: 0496 loss_train: 0.3912 acc_train: 0.8754 loss_val: 0.3710 acc_val: 0.8750 time: 121.0800s\n",
            "Epoch: 0497 loss_train: 0.3995 acc_train: 0.8682 loss_val: 0.3497 acc_val: 0.8858 time: 121.3340s\n",
            "Epoch: 0498 loss_train: 0.3772 acc_train: 0.8820 loss_val: 0.3525 acc_val: 0.8858 time: 121.5593s\n",
            "Epoch: 0499 loss_train: 0.3981 acc_train: 0.8781 loss_val: 0.3534 acc_val: 0.8821 time: 121.7853s\n",
            "Epoch: 0500 loss_train: 0.4018 acc_train: 0.8663 loss_val: 0.3451 acc_val: 0.8879 time: 122.0336s\n",
            "Epoch: 0501 loss_train: 0.3695 acc_train: 0.8803 loss_val: 0.3472 acc_val: 0.8871 time: 122.2873s\n",
            "Epoch: 0502 loss_train: 0.3827 acc_train: 0.8833 loss_val: 0.3443 acc_val: 0.8883 time: 122.5361s\n",
            "Epoch: 0503 loss_train: 0.3798 acc_train: 0.8781 loss_val: 0.3490 acc_val: 0.8854 time: 122.7860s\n",
            "Epoch: 0504 loss_train: 0.3829 acc_train: 0.8760 loss_val: 0.3456 acc_val: 0.8888 time: 123.0341s\n",
            "Epoch: 0505 loss_train: 0.3780 acc_train: 0.8871 loss_val: 0.3452 acc_val: 0.8867 time: 123.2869s\n",
            "Epoch: 0506 loss_train: 0.3857 acc_train: 0.8798 loss_val: 0.3457 acc_val: 0.8875 time: 123.5365s\n",
            "Epoch: 0507 loss_train: 0.3886 acc_train: 0.8753 loss_val: 0.3461 acc_val: 0.8871 time: 123.7935s\n",
            "Epoch: 0508 loss_train: 0.3715 acc_train: 0.8829 loss_val: 0.3435 acc_val: 0.8875 time: 124.0105s\n",
            "Epoch: 0509 loss_train: 0.3706 acc_train: 0.8790 loss_val: 0.3421 acc_val: 0.8888 time: 124.2592s\n",
            "Epoch: 0510 loss_train: 0.3850 acc_train: 0.8813 loss_val: 0.3434 acc_val: 0.8892 time: 124.5136s\n",
            "Epoch: 0511 loss_train: 0.3818 acc_train: 0.8811 loss_val: 0.3431 acc_val: 0.8892 time: 124.7719s\n",
            "Epoch: 0512 loss_train: 0.3786 acc_train: 0.8770 loss_val: 0.3429 acc_val: 0.8867 time: 125.0186s\n",
            "Epoch: 0513 loss_train: 0.3850 acc_train: 0.8807 loss_val: 0.3443 acc_val: 0.8867 time: 125.2699s\n",
            "Epoch: 0514 loss_train: 0.3733 acc_train: 0.8804 loss_val: 0.3434 acc_val: 0.8888 time: 125.5162s\n",
            "Epoch: 0515 loss_train: 0.3683 acc_train: 0.8829 loss_val: 0.3435 acc_val: 0.8896 time: 125.7610s\n",
            "Epoch: 0516 loss_train: 0.3841 acc_train: 0.8804 loss_val: 0.3429 acc_val: 0.8892 time: 126.0070s\n",
            "Epoch: 0517 loss_train: 0.3667 acc_train: 0.8859 loss_val: 0.3408 acc_val: 0.8883 time: 126.2610s\n",
            "Epoch: 0518 loss_train: 0.3735 acc_train: 0.8845 loss_val: 0.3461 acc_val: 0.8850 time: 126.5085s\n",
            "Epoch: 0519 loss_train: 0.3883 acc_train: 0.8741 loss_val: 0.3413 acc_val: 0.8875 time: 126.7589s\n",
            "Epoch: 0520 loss_train: 0.3761 acc_train: 0.8835 loss_val: 0.3465 acc_val: 0.8892 time: 127.0115s\n",
            "Epoch: 0521 loss_train: 0.3898 acc_train: 0.8789 loss_val: 0.3455 acc_val: 0.8892 time: 127.2848s\n",
            "Epoch: 0522 loss_train: 0.3695 acc_train: 0.8838 loss_val: 0.3428 acc_val: 0.8867 time: 127.5374s\n",
            "Epoch: 0523 loss_train: 0.3781 acc_train: 0.8807 loss_val: 0.3450 acc_val: 0.8867 time: 127.7868s\n",
            "Epoch: 0524 loss_train: 0.3772 acc_train: 0.8820 loss_val: 0.3424 acc_val: 0.8879 time: 128.0400s\n",
            "Epoch: 0525 loss_train: 0.3707 acc_train: 0.8816 loss_val: 0.3452 acc_val: 0.8896 time: 128.2979s\n",
            "Epoch: 0526 loss_train: 0.3830 acc_train: 0.8818 loss_val: 0.3432 acc_val: 0.8912 time: 128.5465s\n",
            "Epoch: 0527 loss_train: 0.3691 acc_train: 0.8843 loss_val: 0.3434 acc_val: 0.8883 time: 128.7841s\n",
            "Epoch: 0528 loss_train: 0.3720 acc_train: 0.8809 loss_val: 0.3409 acc_val: 0.8871 time: 129.0309s\n",
            "Epoch: 0529 loss_train: 0.3762 acc_train: 0.8810 loss_val: 0.3441 acc_val: 0.8871 time: 129.2829s\n",
            "Epoch: 0530 loss_train: 0.3713 acc_train: 0.8811 loss_val: 0.3449 acc_val: 0.8888 time: 129.5404s\n",
            "Epoch: 0531 loss_train: 0.3759 acc_train: 0.8801 loss_val: 0.3447 acc_val: 0.8904 time: 129.7947s\n",
            "Epoch: 0532 loss_train: 0.3777 acc_train: 0.8879 loss_val: 0.3433 acc_val: 0.8900 time: 130.0407s\n",
            "Epoch: 0533 loss_train: 0.3756 acc_train: 0.8836 loss_val: 0.3431 acc_val: 0.8883 time: 130.2940s\n",
            "Epoch: 0534 loss_train: 0.3671 acc_train: 0.8840 loss_val: 0.3420 acc_val: 0.8879 time: 130.5354s\n",
            "Epoch: 0535 loss_train: 0.3752 acc_train: 0.8803 loss_val: 0.3406 acc_val: 0.8896 time: 130.7836s\n",
            "Epoch: 0536 loss_train: 0.3804 acc_train: 0.8800 loss_val: 0.3428 acc_val: 0.8871 time: 131.0293s\n",
            "Epoch: 0537 loss_train: 0.3659 acc_train: 0.8829 loss_val: 0.3441 acc_val: 0.8896 time: 131.2851s\n",
            "Epoch: 0538 loss_train: 0.3712 acc_train: 0.8867 loss_val: 0.3443 acc_val: 0.8862 time: 131.5390s\n",
            "Epoch: 0539 loss_train: 0.3726 acc_train: 0.8822 loss_val: 0.3420 acc_val: 0.8896 time: 131.7829s\n",
            "Epoch: 0540 loss_train: 0.3750 acc_train: 0.8851 loss_val: 0.3428 acc_val: 0.8871 time: 132.0325s\n",
            "Epoch: 0541 loss_train: 0.3778 acc_train: 0.8775 loss_val: 0.3418 acc_val: 0.8879 time: 132.2817s\n",
            "Epoch: 0542 loss_train: 0.3620 acc_train: 0.8885 loss_val: 0.3436 acc_val: 0.8875 time: 132.5086s\n",
            "Epoch: 0543 loss_train: 0.3771 acc_train: 0.8781 loss_val: 0.3435 acc_val: 0.8896 time: 132.7531s\n",
            "Epoch: 0544 loss_train: 0.3708 acc_train: 0.8821 loss_val: 0.3430 acc_val: 0.8867 time: 132.9913s\n",
            "Epoch: 0545 loss_train: 0.3657 acc_train: 0.8862 loss_val: 0.3513 acc_val: 0.8833 time: 133.2218s\n",
            "Epoch: 0546 loss_train: 0.3883 acc_train: 0.8752 loss_val: 0.3500 acc_val: 0.8883 time: 133.4700s\n",
            "Epoch: 0547 loss_train: 0.3814 acc_train: 0.8847 loss_val: 0.3465 acc_val: 0.8892 time: 133.6695s\n",
            "Epoch: 0548 loss_train: 0.3721 acc_train: 0.8811 loss_val: 0.3590 acc_val: 0.8842 time: 133.8964s\n",
            "Epoch: 0549 loss_train: 0.3823 acc_train: 0.8778 loss_val: 0.3448 acc_val: 0.8904 time: 134.1414s\n",
            "Epoch: 0550 loss_train: 0.3787 acc_train: 0.8822 loss_val: 0.3452 acc_val: 0.8917 time: 134.3893s\n",
            "Epoch: 0551 loss_train: 0.3768 acc_train: 0.8802 loss_val: 0.3625 acc_val: 0.8796 time: 134.6320s\n",
            "Epoch: 0552 loss_train: 0.3921 acc_train: 0.8725 loss_val: 0.3495 acc_val: 0.8833 time: 134.8861s\n",
            "Epoch: 0553 loss_train: 0.3747 acc_train: 0.8813 loss_val: 0.3423 acc_val: 0.8896 time: 135.1145s\n",
            "Epoch: 0554 loss_train: 0.3737 acc_train: 0.8800 loss_val: 0.3772 acc_val: 0.8738 time: 135.3635s\n",
            "Epoch: 0555 loss_train: 0.3920 acc_train: 0.8651 loss_val: 0.3465 acc_val: 0.8888 time: 135.6063s\n",
            "Epoch: 0556 loss_train: 0.3807 acc_train: 0.8841 loss_val: 0.3511 acc_val: 0.8875 time: 135.8555s\n",
            "Epoch: 0557 loss_train: 0.3855 acc_train: 0.8842 loss_val: 0.3555 acc_val: 0.8788 time: 136.0997s\n",
            "Epoch: 0558 loss_train: 0.3862 acc_train: 0.8742 loss_val: 0.3418 acc_val: 0.8871 time: 136.3388s\n",
            "Epoch: 0559 loss_train: 0.3640 acc_train: 0.8812 loss_val: 0.3479 acc_val: 0.8854 time: 136.5883s\n",
            "Epoch: 0560 loss_train: 0.3771 acc_train: 0.8795 loss_val: 0.3395 acc_val: 0.8900 time: 136.8471s\n",
            "Epoch: 0561 loss_train: 0.3656 acc_train: 0.8832 loss_val: 0.3577 acc_val: 0.8796 time: 137.0936s\n",
            "Epoch: 0562 loss_train: 0.3789 acc_train: 0.8773 loss_val: 0.3466 acc_val: 0.8900 time: 137.3470s\n",
            "Epoch: 0563 loss_train: 0.3740 acc_train: 0.8808 loss_val: 0.3465 acc_val: 0.8883 time: 137.5979s\n",
            "Epoch: 0564 loss_train: 0.3628 acc_train: 0.8895 loss_val: 0.3783 acc_val: 0.8725 time: 137.8344s\n",
            "Epoch: 0565 loss_train: 0.4021 acc_train: 0.8637 loss_val: 0.3442 acc_val: 0.8908 time: 138.0617s\n",
            "Epoch: 0566 loss_train: 0.3723 acc_train: 0.8853 loss_val: 0.3436 acc_val: 0.8912 time: 138.3116s\n",
            "Epoch: 0567 loss_train: 0.3813 acc_train: 0.8857 loss_val: 0.3750 acc_val: 0.8758 time: 138.5590s\n",
            "Epoch: 0568 loss_train: 0.3974 acc_train: 0.8689 loss_val: 0.3394 acc_val: 0.8904 time: 138.7964s\n",
            "Epoch: 0569 loss_train: 0.3829 acc_train: 0.8798 loss_val: 0.3423 acc_val: 0.8929 time: 139.0467s\n",
            "Epoch: 0570 loss_train: 0.3746 acc_train: 0.8843 loss_val: 0.3770 acc_val: 0.8767 time: 139.2823s\n",
            "Epoch: 0571 loss_train: 0.4031 acc_train: 0.8666 loss_val: 0.3424 acc_val: 0.8875 time: 139.5429s\n",
            "Epoch: 0572 loss_train: 0.3661 acc_train: 0.8827 loss_val: 0.3532 acc_val: 0.8875 time: 139.7902s\n",
            "Epoch: 0573 loss_train: 0.3965 acc_train: 0.8798 loss_val: 0.3528 acc_val: 0.8833 time: 140.0404s\n",
            "Epoch: 0574 loss_train: 0.3740 acc_train: 0.8781 loss_val: 0.3533 acc_val: 0.8842 time: 140.2894s\n",
            "Epoch: 0575 loss_train: 0.3787 acc_train: 0.8768 loss_val: 0.3470 acc_val: 0.8871 time: 140.5368s\n",
            "Epoch: 0576 loss_train: 0.3736 acc_train: 0.8846 loss_val: 0.3414 acc_val: 0.8912 time: 140.7854s\n",
            "Epoch: 0577 loss_train: 0.3704 acc_train: 0.8844 loss_val: 0.3545 acc_val: 0.8817 time: 141.0344s\n",
            "Epoch: 0578 loss_train: 0.3828 acc_train: 0.8718 loss_val: 0.3419 acc_val: 0.8929 time: 141.2852s\n",
            "Epoch: 0579 loss_train: 0.3756 acc_train: 0.8807 loss_val: 0.3444 acc_val: 0.8883 time: 141.5340s\n",
            "Epoch: 0580 loss_train: 0.3749 acc_train: 0.8814 loss_val: 0.3585 acc_val: 0.8862 time: 141.7956s\n",
            "Epoch: 0581 loss_train: 0.3676 acc_train: 0.8859 loss_val: 0.3473 acc_val: 0.8867 time: 142.0414s\n",
            "Epoch: 0582 loss_train: 0.3657 acc_train: 0.8822 loss_val: 0.3420 acc_val: 0.8921 time: 142.2988s\n",
            "Epoch: 0583 loss_train: 0.3752 acc_train: 0.8843 loss_val: 0.3430 acc_val: 0.8867 time: 142.5474s\n",
            "Epoch: 0584 loss_train: 0.3667 acc_train: 0.8825 loss_val: 0.3426 acc_val: 0.8883 time: 142.7936s\n",
            "Epoch: 0585 loss_train: 0.3625 acc_train: 0.8816 loss_val: 0.3404 acc_val: 0.8942 time: 143.0336s\n",
            "Epoch: 0586 loss_train: 0.3644 acc_train: 0.8854 loss_val: 0.3384 acc_val: 0.8938 time: 143.2828s\n",
            "Epoch: 0587 loss_train: 0.3669 acc_train: 0.8811 loss_val: 0.3434 acc_val: 0.8875 time: 143.5315s\n",
            "Epoch: 0588 loss_train: 0.3664 acc_train: 0.8830 loss_val: 0.3395 acc_val: 0.8925 time: 143.7844s\n",
            "Epoch: 0589 loss_train: 0.3606 acc_train: 0.8876 loss_val: 0.3360 acc_val: 0.8900 time: 144.0266s\n",
            "Epoch: 0590 loss_train: 0.3685 acc_train: 0.8825 loss_val: 0.3443 acc_val: 0.8858 time: 144.2839s\n",
            "Epoch: 0591 loss_train: 0.3670 acc_train: 0.8814 loss_val: 0.3364 acc_val: 0.8933 time: 144.5336s\n",
            "Epoch: 0592 loss_train: 0.3669 acc_train: 0.8836 loss_val: 0.3379 acc_val: 0.8921 time: 144.7945s\n",
            "Epoch: 0593 loss_train: 0.3648 acc_train: 0.8865 loss_val: 0.3426 acc_val: 0.8900 time: 145.0421s\n",
            "Epoch: 0594 loss_train: 0.3592 acc_train: 0.8826 loss_val: 0.3367 acc_val: 0.8938 time: 145.2880s\n",
            "Epoch: 0595 loss_train: 0.3612 acc_train: 0.8854 loss_val: 0.3353 acc_val: 0.8917 time: 145.5428s\n",
            "Epoch: 0596 loss_train: 0.3622 acc_train: 0.8868 loss_val: 0.3494 acc_val: 0.8854 time: 145.7895s\n",
            "Epoch: 0597 loss_train: 0.3744 acc_train: 0.8737 loss_val: 0.3419 acc_val: 0.8908 time: 146.0285s\n",
            "Epoch: 0598 loss_train: 0.3715 acc_train: 0.8869 loss_val: 0.3426 acc_val: 0.8929 time: 146.2782s\n",
            "Epoch: 0599 loss_train: 0.3597 acc_train: 0.8875 loss_val: 0.3485 acc_val: 0.8854 time: 146.5368s\n",
            "Epoch: 0600 loss_train: 0.3686 acc_train: 0.8812 loss_val: 0.3368 acc_val: 0.8912 time: 146.7918s\n",
            "Epoch: 0601 loss_train: 0.3598 acc_train: 0.8857 loss_val: 0.3395 acc_val: 0.8892 time: 147.0441s\n",
            "Epoch: 0602 loss_train: 0.3607 acc_train: 0.8896 loss_val: 0.3485 acc_val: 0.8850 time: 147.2915s\n",
            "Epoch: 0603 loss_train: 0.3582 acc_train: 0.8821 loss_val: 0.3396 acc_val: 0.8938 time: 147.5414s\n",
            "Epoch: 0604 loss_train: 0.3595 acc_train: 0.8866 loss_val: 0.3375 acc_val: 0.8962 time: 147.7821s\n",
            "Epoch: 0605 loss_train: 0.3656 acc_train: 0.8870 loss_val: 0.3478 acc_val: 0.8875 time: 147.9919s\n",
            "Epoch: 0606 loss_train: 0.3626 acc_train: 0.8834 loss_val: 0.3351 acc_val: 0.8921 time: 148.2118s\n",
            "Epoch: 0607 loss_train: 0.3573 acc_train: 0.8858 loss_val: 0.3374 acc_val: 0.8958 time: 148.4605s\n",
            "Epoch: 0608 loss_train: 0.3604 acc_train: 0.8864 loss_val: 0.3437 acc_val: 0.8896 time: 148.7159s\n",
            "Epoch: 0609 loss_train: 0.3589 acc_train: 0.8889 loss_val: 0.3451 acc_val: 0.8904 time: 148.9691s\n",
            "Epoch: 0610 loss_train: 0.3559 acc_train: 0.8815 loss_val: 0.3375 acc_val: 0.8933 time: 149.2136s\n",
            "Epoch: 0611 loss_train: 0.3641 acc_train: 0.8871 loss_val: 0.3378 acc_val: 0.8933 time: 149.4741s\n",
            "Epoch: 0612 loss_train: 0.3581 acc_train: 0.8848 loss_val: 0.3417 acc_val: 0.8900 time: 149.7339s\n",
            "Epoch: 0613 loss_train: 0.3654 acc_train: 0.8822 loss_val: 0.3347 acc_val: 0.8900 time: 149.9822s\n",
            "Epoch: 0614 loss_train: 0.3593 acc_train: 0.8884 loss_val: 0.3369 acc_val: 0.8933 time: 150.2336s\n",
            "Epoch: 0615 loss_train: 0.3611 acc_train: 0.8835 loss_val: 0.3409 acc_val: 0.8896 time: 150.4867s\n",
            "Epoch: 0616 loss_train: 0.3540 acc_train: 0.8888 loss_val: 0.3403 acc_val: 0.8879 time: 150.7338s\n",
            "Epoch: 0617 loss_train: 0.3528 acc_train: 0.8870 loss_val: 0.3365 acc_val: 0.8933 time: 150.9805s\n",
            "Epoch: 0618 loss_train: 0.3598 acc_train: 0.8874 loss_val: 0.3418 acc_val: 0.8858 time: 151.2219s\n",
            "Epoch: 0619 loss_train: 0.3636 acc_train: 0.8857 loss_val: 0.3369 acc_val: 0.8917 time: 151.4699s\n",
            "Epoch: 0620 loss_train: 0.3536 acc_train: 0.8859 loss_val: 0.3393 acc_val: 0.8888 time: 151.7280s\n",
            "Epoch: 0621 loss_train: 0.3670 acc_train: 0.8827 loss_val: 0.3424 acc_val: 0.8908 time: 151.9698s\n",
            "Epoch: 0622 loss_train: 0.3584 acc_train: 0.8864 loss_val: 0.3353 acc_val: 0.8925 time: 152.2430s\n",
            "Epoch: 0623 loss_train: 0.3552 acc_train: 0.8847 loss_val: 0.3353 acc_val: 0.8971 time: 152.4907s\n",
            "Epoch: 0624 loss_train: 0.3560 acc_train: 0.8877 loss_val: 0.3404 acc_val: 0.8892 time: 152.7393s\n",
            "Epoch: 0625 loss_train: 0.3501 acc_train: 0.8867 loss_val: 0.3494 acc_val: 0.8867 time: 152.9832s\n",
            "Epoch: 0626 loss_train: 0.3614 acc_train: 0.8838 loss_val: 0.3395 acc_val: 0.8950 time: 153.2311s\n",
            "Epoch: 0627 loss_train: 0.3547 acc_train: 0.8940 loss_val: 0.3364 acc_val: 0.8904 time: 153.4745s\n",
            "Epoch: 0628 loss_train: 0.3509 acc_train: 0.8885 loss_val: 0.3412 acc_val: 0.8917 time: 153.7226s\n",
            "Epoch: 0629 loss_train: 0.3621 acc_train: 0.8842 loss_val: 0.3415 acc_val: 0.8904 time: 153.9679s\n",
            "Epoch: 0630 loss_train: 0.3713 acc_train: 0.8833 loss_val: 0.3383 acc_val: 0.8896 time: 154.2294s\n",
            "Epoch: 0631 loss_train: 0.3552 acc_train: 0.8819 loss_val: 0.3394 acc_val: 0.8892 time: 154.4718s\n",
            "Epoch: 0632 loss_train: 0.3496 acc_train: 0.8874 loss_val: 0.3348 acc_val: 0.8933 time: 154.7227s\n",
            "Epoch: 0633 loss_train: 0.3495 acc_train: 0.8908 loss_val: 0.3345 acc_val: 0.8900 time: 154.9660s\n",
            "Epoch: 0634 loss_train: 0.3444 acc_train: 0.8915 loss_val: 0.3398 acc_val: 0.8892 time: 155.2084s\n",
            "Epoch: 0635 loss_train: 0.3508 acc_train: 0.8891 loss_val: 0.3333 acc_val: 0.8925 time: 155.4166s\n",
            "Epoch: 0636 loss_train: 0.3465 acc_train: 0.8892 loss_val: 0.3320 acc_val: 0.8946 time: 155.6659s\n",
            "Epoch: 0637 loss_train: 0.3615 acc_train: 0.8868 loss_val: 0.3354 acc_val: 0.8917 time: 155.8766s\n",
            "Epoch: 0638 loss_train: 0.3498 acc_train: 0.8885 loss_val: 0.3371 acc_val: 0.8900 time: 156.1320s\n",
            "Epoch: 0639 loss_train: 0.3517 acc_train: 0.8875 loss_val: 0.3339 acc_val: 0.8946 time: 156.3778s\n",
            "Epoch: 0640 loss_train: 0.3546 acc_train: 0.8895 loss_val: 0.3426 acc_val: 0.8875 time: 156.6360s\n",
            "Epoch: 0641 loss_train: 0.3540 acc_train: 0.8857 loss_val: 0.3347 acc_val: 0.8921 time: 156.8491s\n",
            "Epoch: 0642 loss_train: 0.3478 acc_train: 0.8895 loss_val: 0.3354 acc_val: 0.8950 time: 157.0781s\n",
            "Epoch: 0643 loss_train: 0.3537 acc_train: 0.8924 loss_val: 0.3483 acc_val: 0.8875 time: 157.3211s\n",
            "Epoch: 0644 loss_train: 0.3505 acc_train: 0.8848 loss_val: 0.3298 acc_val: 0.8958 time: 157.5672s\n",
            "Epoch: 0645 loss_train: 0.3519 acc_train: 0.8875 loss_val: 0.3378 acc_val: 0.8975 time: 157.8159s\n",
            "Epoch: 0646 loss_train: 0.3611 acc_train: 0.8877 loss_val: 0.3472 acc_val: 0.8867 time: 158.0574s\n",
            "Epoch: 0647 loss_train: 0.3494 acc_train: 0.8871 loss_val: 0.3319 acc_val: 0.8917 time: 158.3133s\n",
            "Epoch: 0648 loss_train: 0.3513 acc_train: 0.8919 loss_val: 0.3321 acc_val: 0.8954 time: 158.5620s\n",
            "Epoch: 0649 loss_train: 0.3500 acc_train: 0.8907 loss_val: 0.3339 acc_val: 0.8925 time: 158.8140s\n",
            "Epoch: 0650 loss_train: 0.3436 acc_train: 0.8880 loss_val: 0.3358 acc_val: 0.8929 time: 159.0676s\n",
            "Epoch: 0651 loss_train: 0.3462 acc_train: 0.8895 loss_val: 0.3344 acc_val: 0.8962 time: 159.3105s\n",
            "Epoch: 0652 loss_train: 0.3564 acc_train: 0.8895 loss_val: 0.3398 acc_val: 0.8900 time: 159.5581s\n",
            "Epoch: 0653 loss_train: 0.3440 acc_train: 0.8869 loss_val: 0.3337 acc_val: 0.8933 time: 159.8076s\n",
            "Epoch: 0654 loss_train: 0.3510 acc_train: 0.8881 loss_val: 0.3357 acc_val: 0.8975 time: 160.0704s\n",
            "Epoch: 0655 loss_train: 0.3562 acc_train: 0.8910 loss_val: 0.3399 acc_val: 0.8904 time: 160.3122s\n",
            "Epoch: 0656 loss_train: 0.3430 acc_train: 0.8898 loss_val: 0.3342 acc_val: 0.8933 time: 160.5345s\n",
            "Epoch: 0657 loss_train: 0.3482 acc_train: 0.8866 loss_val: 0.3397 acc_val: 0.8967 time: 160.7147s\n",
            "Epoch: 0658 loss_train: 0.3575 acc_train: 0.8904 loss_val: 0.3418 acc_val: 0.8892 time: 160.9261s\n",
            "Epoch: 0659 loss_train: 0.3498 acc_train: 0.8865 loss_val: 0.3455 acc_val: 0.8883 time: 161.1734s\n",
            "Epoch: 0660 loss_train: 0.3634 acc_train: 0.8812 loss_val: 0.3437 acc_val: 0.8971 time: 161.3765s\n",
            "Epoch: 0661 loss_train: 0.3682 acc_train: 0.8879 loss_val: 0.3578 acc_val: 0.8825 time: 161.6199s\n",
            "Epoch: 0662 loss_train: 0.3578 acc_train: 0.8846 loss_val: 0.3486 acc_val: 0.8858 time: 161.8716s\n",
            "Epoch: 0663 loss_train: 0.3640 acc_train: 0.8798 loss_val: 0.3411 acc_val: 0.8900 time: 162.1271s\n",
            "Epoch: 0664 loss_train: 0.3602 acc_train: 0.8874 loss_val: 0.3483 acc_val: 0.8838 time: 162.3831s\n",
            "Epoch: 0665 loss_train: 0.3571 acc_train: 0.8778 loss_val: 0.3399 acc_val: 0.8892 time: 162.6393s\n",
            "Epoch: 0666 loss_train: 0.3540 acc_train: 0.8856 loss_val: 0.3549 acc_val: 0.8862 time: 162.8903s\n",
            "Epoch: 0667 loss_train: 0.3733 acc_train: 0.8814 loss_val: 0.3357 acc_val: 0.8917 time: 163.1129s\n",
            "Epoch: 0668 loss_train: 0.3444 acc_train: 0.8865 loss_val: 0.3502 acc_val: 0.8812 time: 163.3576s\n",
            "Epoch: 0669 loss_train: 0.3615 acc_train: 0.8809 loss_val: 0.3445 acc_val: 0.8917 time: 163.6067s\n",
            "Epoch: 0670 loss_train: 0.3652 acc_train: 0.8880 loss_val: 0.3453 acc_val: 0.8879 time: 163.8647s\n",
            "Epoch: 0671 loss_train: 0.3495 acc_train: 0.8891 loss_val: 0.3592 acc_val: 0.8817 time: 164.1097s\n",
            "Epoch: 0672 loss_train: 0.3580 acc_train: 0.8805 loss_val: 0.3378 acc_val: 0.8962 time: 164.3485s\n",
            "Epoch: 0673 loss_train: 0.3551 acc_train: 0.8908 loss_val: 0.3310 acc_val: 0.8929 time: 164.5852s\n",
            "Epoch: 0674 loss_train: 0.3477 acc_train: 0.8880 loss_val: 0.3566 acc_val: 0.8842 time: 164.8341s\n",
            "Epoch: 0675 loss_train: 0.3601 acc_train: 0.8825 loss_val: 0.3377 acc_val: 0.8942 time: 165.0644s\n",
            "Epoch: 0676 loss_train: 0.3540 acc_train: 0.8909 loss_val: 0.3393 acc_val: 0.8938 time: 165.2560s\n",
            "Epoch: 0677 loss_train: 0.3630 acc_train: 0.8876 loss_val: 0.3737 acc_val: 0.8788 time: 165.4985s\n",
            "Epoch: 0678 loss_train: 0.3837 acc_train: 0.8733 loss_val: 0.3402 acc_val: 0.8942 time: 165.7468s\n",
            "Epoch: 0679 loss_train: 0.3536 acc_train: 0.8940 loss_val: 0.3379 acc_val: 0.8946 time: 166.0076s\n",
            "Epoch: 0680 loss_train: 0.3581 acc_train: 0.8938 loss_val: 0.3570 acc_val: 0.8833 time: 166.2528s\n",
            "Epoch: 0681 loss_train: 0.3706 acc_train: 0.8792 loss_val: 0.3305 acc_val: 0.8912 time: 166.4977s\n",
            "Epoch: 0682 loss_train: 0.3479 acc_train: 0.8891 loss_val: 0.3353 acc_val: 0.8950 time: 166.6747s\n",
            "Epoch: 0683 loss_train: 0.3593 acc_train: 0.8852 loss_val: 0.3569 acc_val: 0.8833 time: 166.8908s\n",
            "Epoch: 0684 loss_train: 0.3589 acc_train: 0.8804 loss_val: 0.3418 acc_val: 0.8917 time: 167.1385s\n",
            "Epoch: 0685 loss_train: 0.3555 acc_train: 0.8886 loss_val: 0.3365 acc_val: 0.8946 time: 167.3846s\n",
            "Epoch: 0686 loss_train: 0.3579 acc_train: 0.8875 loss_val: 0.3488 acc_val: 0.8825 time: 167.6298s\n",
            "Epoch: 0687 loss_train: 0.3602 acc_train: 0.8796 loss_val: 0.3342 acc_val: 0.8908 time: 167.8998s\n",
            "Epoch: 0688 loss_train: 0.3424 acc_train: 0.8891 loss_val: 0.3360 acc_val: 0.8912 time: 168.1466s\n",
            "Epoch: 0689 loss_train: 0.3601 acc_train: 0.8897 loss_val: 0.3436 acc_val: 0.8912 time: 168.3912s\n",
            "Epoch: 0690 loss_train: 0.3466 acc_train: 0.8849 loss_val: 0.3302 acc_val: 0.8946 time: 168.6075s\n",
            "Epoch: 0691 loss_train: 0.3418 acc_train: 0.8891 loss_val: 0.3303 acc_val: 0.8946 time: 168.8576s\n",
            "Epoch: 0692 loss_train: 0.3408 acc_train: 0.8901 loss_val: 0.3332 acc_val: 0.8921 time: 169.0905s\n",
            "Epoch: 0693 loss_train: 0.3409 acc_train: 0.8892 loss_val: 0.3315 acc_val: 0.8925 time: 169.3288s\n",
            "Epoch: 0694 loss_train: 0.3366 acc_train: 0.8922 loss_val: 0.3305 acc_val: 0.8954 time: 169.5782s\n",
            "Epoch: 0695 loss_train: 0.3455 acc_train: 0.8897 loss_val: 0.3288 acc_val: 0.8979 time: 169.8368s\n",
            "Epoch: 0696 loss_train: 0.3402 acc_train: 0.8927 loss_val: 0.3328 acc_val: 0.8929 time: 170.0876s\n",
            "Epoch: 0697 loss_train: 0.3457 acc_train: 0.8888 loss_val: 0.3329 acc_val: 0.8946 time: 170.3371s\n",
            "Epoch: 0698 loss_train: 0.3462 acc_train: 0.8875 loss_val: 0.3313 acc_val: 0.8958 time: 170.5926s\n",
            "Epoch: 0699 loss_train: 0.3541 acc_train: 0.8842 loss_val: 0.3322 acc_val: 0.8946 time: 170.8408s\n",
            "Epoch: 0700 loss_train: 0.3421 acc_train: 0.8903 loss_val: 0.3331 acc_val: 0.8921 time: 171.0858s\n",
            "Epoch: 0701 loss_train: 0.3464 acc_train: 0.8878 loss_val: 0.3297 acc_val: 0.8967 time: 171.3370s\n",
            "Epoch: 0702 loss_train: 0.3405 acc_train: 0.8962 loss_val: 0.3329 acc_val: 0.8958 time: 171.5858s\n",
            "Epoch: 0703 loss_train: 0.3468 acc_train: 0.8923 loss_val: 0.3339 acc_val: 0.8946 time: 171.8342s\n",
            "Epoch: 0704 loss_train: 0.3422 acc_train: 0.8878 loss_val: 0.3273 acc_val: 0.8967 time: 172.0811s\n",
            "Epoch: 0705 loss_train: 0.3370 acc_train: 0.8964 loss_val: 0.3337 acc_val: 0.8921 time: 172.3250s\n",
            "Epoch: 0706 loss_train: 0.3400 acc_train: 0.8887 loss_val: 0.3298 acc_val: 0.8950 time: 172.5708s\n",
            "Epoch: 0707 loss_train: 0.3441 acc_train: 0.8891 loss_val: 0.3285 acc_val: 0.8983 time: 172.8164s\n",
            "Epoch: 0708 loss_train: 0.3417 acc_train: 0.8907 loss_val: 0.3362 acc_val: 0.8912 time: 173.0907s\n",
            "Epoch: 0709 loss_train: 0.3407 acc_train: 0.8891 loss_val: 0.3313 acc_val: 0.8925 time: 173.3356s\n",
            "Epoch: 0710 loss_train: 0.3413 acc_train: 0.8919 loss_val: 0.3340 acc_val: 0.8938 time: 173.5758s\n",
            "Epoch: 0711 loss_train: 0.3498 acc_train: 0.8927 loss_val: 0.3462 acc_val: 0.8883 time: 173.8271s\n",
            "Epoch: 0712 loss_train: 0.3553 acc_train: 0.8842 loss_val: 0.3299 acc_val: 0.8962 time: 174.0704s\n",
            "Epoch: 0713 loss_train: 0.3401 acc_train: 0.8954 loss_val: 0.3274 acc_val: 0.8933 time: 174.3147s\n",
            "Epoch: 0714 loss_train: 0.3385 acc_train: 0.8929 loss_val: 0.3460 acc_val: 0.8888 time: 174.5591s\n",
            "Epoch: 0715 loss_train: 0.3476 acc_train: 0.8873 loss_val: 0.3311 acc_val: 0.8962 time: 174.8065s\n",
            "Epoch: 0716 loss_train: 0.3508 acc_train: 0.8927 loss_val: 0.3329 acc_val: 0.8933 time: 175.0594s\n",
            "Epoch: 0717 loss_train: 0.3485 acc_train: 0.8896 loss_val: 0.3288 acc_val: 0.8933 time: 175.3047s\n",
            "Epoch: 0718 loss_train: 0.3387 acc_train: 0.8896 loss_val: 0.3367 acc_val: 0.8904 time: 175.5376s\n",
            "Epoch: 0719 loss_train: 0.3498 acc_train: 0.8881 loss_val: 0.3286 acc_val: 0.8967 time: 175.7861s\n",
            "Epoch: 0720 loss_train: 0.3407 acc_train: 0.8921 loss_val: 0.3367 acc_val: 0.8904 time: 176.0413s\n",
            "Epoch: 0721 loss_train: 0.3415 acc_train: 0.8924 loss_val: 0.3294 acc_val: 0.8950 time: 176.2942s\n",
            "Epoch: 0722 loss_train: 0.3399 acc_train: 0.8911 loss_val: 0.3378 acc_val: 0.8925 time: 176.5433s\n",
            "Epoch: 0723 loss_train: 0.3378 acc_train: 0.8943 loss_val: 0.3325 acc_val: 0.8979 time: 176.7899s\n",
            "Epoch: 0724 loss_train: 0.3522 acc_train: 0.8899 loss_val: 0.3299 acc_val: 0.8979 time: 177.0441s\n",
            "Epoch: 0725 loss_train: 0.3404 acc_train: 0.8920 loss_val: 0.3499 acc_val: 0.8883 time: 177.2877s\n",
            "Epoch: 0726 loss_train: 0.3486 acc_train: 0.8895 loss_val: 0.3270 acc_val: 0.8958 time: 177.5362s\n",
            "Epoch: 0727 loss_train: 0.3428 acc_train: 0.8912 loss_val: 0.3505 acc_val: 0.8967 time: 177.7798s\n",
            "Epoch: 0728 loss_train: 0.3593 acc_train: 0.8874 loss_val: 0.3378 acc_val: 0.8929 time: 178.0301s\n",
            "Epoch: 0729 loss_train: 0.3383 acc_train: 0.8910 loss_val: 0.3422 acc_val: 0.8900 time: 178.2829s\n",
            "Epoch: 0730 loss_train: 0.3466 acc_train: 0.8892 loss_val: 0.3354 acc_val: 0.8979 time: 178.5192s\n",
            "Epoch: 0731 loss_train: 0.3560 acc_train: 0.8922 loss_val: 0.3543 acc_val: 0.8846 time: 178.7777s\n",
            "Epoch: 0732 loss_train: 0.3697 acc_train: 0.8789 loss_val: 0.3426 acc_val: 0.8888 time: 179.0003s\n",
            "Epoch: 0733 loss_train: 0.3527 acc_train: 0.8880 loss_val: 0.3344 acc_val: 0.8954 time: 179.1779s\n",
            "Epoch: 0734 loss_train: 0.3557 acc_train: 0.8845 loss_val: 0.3342 acc_val: 0.8921 time: 179.3546s\n",
            "Epoch: 0735 loss_train: 0.3530 acc_train: 0.8858 loss_val: 0.3672 acc_val: 0.8800 time: 179.6041s\n",
            "Epoch: 0736 loss_train: 0.3694 acc_train: 0.8763 loss_val: 0.3581 acc_val: 0.8821 time: 179.8493s\n",
            "Epoch: 0737 loss_train: 0.3755 acc_train: 0.8822 loss_val: 0.3346 acc_val: 0.8938 time: 180.0741s\n",
            "Epoch: 0738 loss_train: 0.3462 acc_train: 0.8944 loss_val: 0.3652 acc_val: 0.8779 time: 180.2495s\n",
            "Epoch: 0739 loss_train: 0.3720 acc_train: 0.8722 loss_val: 0.3380 acc_val: 0.8958 time: 180.4412s\n",
            "Epoch: 0740 loss_train: 0.3521 acc_train: 0.8927 loss_val: 0.3467 acc_val: 0.8892 time: 180.6931s\n",
            "Epoch: 0741 loss_train: 0.3729 acc_train: 0.8812 loss_val: 0.3780 acc_val: 0.8754 time: 180.9479s\n",
            "Epoch: 0742 loss_train: 0.3824 acc_train: 0.8737 loss_val: 0.3429 acc_val: 0.8925 time: 181.1924s\n",
            "Epoch: 0743 loss_train: 0.3669 acc_train: 0.8868 loss_val: 0.3574 acc_val: 0.8858 time: 181.4380s\n",
            "Epoch: 0744 loss_train: 0.3673 acc_train: 0.8779 loss_val: 0.3694 acc_val: 0.8846 time: 181.6894s\n",
            "Epoch: 0745 loss_train: 0.3629 acc_train: 0.8809 loss_val: 0.3372 acc_val: 0.8958 time: 181.9449s\n",
            "Epoch: 0746 loss_train: 0.3520 acc_train: 0.8921 loss_val: 0.3577 acc_val: 0.8912 time: 182.1906s\n",
            "Epoch: 0747 loss_train: 0.3707 acc_train: 0.8875 loss_val: 0.3872 acc_val: 0.8771 time: 182.4364s\n",
            "Epoch: 0748 loss_train: 0.3829 acc_train: 0.8718 loss_val: 0.3703 acc_val: 0.8825 time: 182.6839s\n",
            "Epoch: 0749 loss_train: 0.3864 acc_train: 0.8731 loss_val: 0.3345 acc_val: 0.8958 time: 182.9443s\n",
            "Epoch: 0750 loss_train: 0.3529 acc_train: 0.8876 loss_val: 0.3671 acc_val: 0.8817 time: 183.1881s\n",
            "Epoch: 0751 loss_train: 0.3876 acc_train: 0.8718 loss_val: 0.3707 acc_val: 0.8792 time: 183.4368s\n",
            "Epoch: 0752 loss_train: 0.3705 acc_train: 0.8768 loss_val: 0.3763 acc_val: 0.8792 time: 183.6948s\n",
            "Epoch: 0753 loss_train: 0.3905 acc_train: 0.8755 loss_val: 0.3496 acc_val: 0.8946 time: 183.9485s\n",
            "Epoch: 0754 loss_train: 0.3551 acc_train: 0.8889 loss_val: 0.3853 acc_val: 0.8675 time: 184.1935s\n",
            "Epoch: 0755 loss_train: 0.3899 acc_train: 0.8667 loss_val: 0.3440 acc_val: 0.8933 time: 184.4387s\n",
            "Epoch: 0756 loss_train: 0.3599 acc_train: 0.8929 loss_val: 0.3442 acc_val: 0.8925 time: 184.6903s\n",
            "Epoch: 0757 loss_train: 0.3707 acc_train: 0.8851 loss_val: 0.3504 acc_val: 0.8921 time: 184.9334s\n",
            "Epoch: 0758 loss_train: 0.3781 acc_train: 0.8789 loss_val: 0.3428 acc_val: 0.8938 time: 185.1823s\n",
            "Epoch: 0759 loss_train: 0.3857 acc_train: 0.8812 loss_val: 0.3385 acc_val: 0.8950 time: 185.4249s\n",
            "Epoch: 0760 loss_train: 0.3579 acc_train: 0.8888 loss_val: 0.3509 acc_val: 0.8908 time: 185.6667s\n",
            "Epoch: 0761 loss_train: 0.3636 acc_train: 0.8890 loss_val: 0.3367 acc_val: 0.8971 time: 185.9126s\n",
            "Epoch: 0762 loss_train: 0.3552 acc_train: 0.8922 loss_val: 0.3365 acc_val: 0.8904 time: 186.1432s\n",
            "Epoch: 0763 loss_train: 0.3446 acc_train: 0.8926 loss_val: 0.3567 acc_val: 0.8846 time: 186.3865s\n",
            "Epoch: 0764 loss_train: 0.3769 acc_train: 0.8768 loss_val: 0.3393 acc_val: 0.8921 time: 186.6328s\n",
            "Epoch: 0765 loss_train: 0.3496 acc_train: 0.8877 loss_val: 0.3385 acc_val: 0.8983 time: 186.8806s\n",
            "Epoch: 0766 loss_train: 0.3480 acc_train: 0.8935 loss_val: 0.3472 acc_val: 0.8908 time: 187.0980s\n",
            "Epoch: 0767 loss_train: 0.3589 acc_train: 0.8835 loss_val: 0.3340 acc_val: 0.8954 time: 187.3433s\n",
            "Epoch: 0768 loss_train: 0.3415 acc_train: 0.8946 loss_val: 0.3354 acc_val: 0.8942 time: 187.5849s\n",
            "Epoch: 0769 loss_train: 0.3531 acc_train: 0.8860 loss_val: 0.3430 acc_val: 0.8896 time: 187.8328s\n",
            "Epoch: 0770 loss_train: 0.3613 acc_train: 0.8835 loss_val: 0.3356 acc_val: 0.8946 time: 188.0802s\n",
            "Epoch: 0771 loss_train: 0.3438 acc_train: 0.8913 loss_val: 0.3406 acc_val: 0.8971 time: 188.3331s\n",
            "Epoch: 0772 loss_train: 0.3549 acc_train: 0.8952 loss_val: 0.3369 acc_val: 0.8942 time: 188.5767s\n",
            "Epoch: 0773 loss_train: 0.3408 acc_train: 0.8898 loss_val: 0.3467 acc_val: 0.8917 time: 188.8261s\n",
            "Epoch: 0774 loss_train: 0.3567 acc_train: 0.8860 loss_val: 0.3387 acc_val: 0.8925 time: 189.0735s\n",
            "Epoch: 0775 loss_train: 0.3442 acc_train: 0.8941 loss_val: 0.3318 acc_val: 0.8979 time: 189.3208s\n",
            "Epoch: 0776 loss_train: 0.3434 acc_train: 0.8888 loss_val: 0.3374 acc_val: 0.8929 time: 189.5492s\n",
            "Epoch: 0777 loss_train: 0.3467 acc_train: 0.8884 loss_val: 0.3249 acc_val: 0.8958 time: 189.7964s\n",
            "Epoch: 0778 loss_train: 0.3387 acc_train: 0.8957 loss_val: 0.3315 acc_val: 0.8921 time: 190.0421s\n",
            "Epoch: 0779 loss_train: 0.3463 acc_train: 0.8887 loss_val: 0.3425 acc_val: 0.8912 time: 190.2894s\n",
            "Epoch: 0780 loss_train: 0.3471 acc_train: 0.8893 loss_val: 0.3307 acc_val: 0.8958 time: 190.5304s\n",
            "Epoch: 0781 loss_train: 0.3456 acc_train: 0.8957 loss_val: 0.3301 acc_val: 0.8950 time: 190.7774s\n",
            "Epoch: 0782 loss_train: 0.3377 acc_train: 0.8968 loss_val: 0.3287 acc_val: 0.8950 time: 191.0272s\n",
            "Epoch: 0783 loss_train: 0.3417 acc_train: 0.8913 loss_val: 0.3258 acc_val: 0.8962 time: 191.2760s\n",
            "Epoch: 0784 loss_train: 0.3259 acc_train: 0.8970 loss_val: 0.3305 acc_val: 0.8921 time: 191.5205s\n",
            "Epoch: 0785 loss_train: 0.3399 acc_train: 0.8919 loss_val: 0.3280 acc_val: 0.8979 time: 191.7754s\n",
            "Epoch: 0786 loss_train: 0.3407 acc_train: 0.8925 loss_val: 0.3350 acc_val: 0.8946 time: 192.0098s\n",
            "Epoch: 0787 loss_train: 0.3409 acc_train: 0.8898 loss_val: 0.3281 acc_val: 0.8971 time: 192.2594s\n",
            "Epoch: 0788 loss_train: 0.3290 acc_train: 0.8951 loss_val: 0.3246 acc_val: 0.8983 time: 192.5010s\n",
            "Epoch: 0789 loss_train: 0.3346 acc_train: 0.8956 loss_val: 0.3251 acc_val: 0.8942 time: 192.7452s\n",
            "Epoch: 0790 loss_train: 0.3331 acc_train: 0.8941 loss_val: 0.3254 acc_val: 0.8946 time: 192.9864s\n",
            "Epoch: 0791 loss_train: 0.3218 acc_train: 0.8948 loss_val: 0.3259 acc_val: 0.8979 time: 193.2436s\n",
            "Epoch: 0792 loss_train: 0.3315 acc_train: 0.8942 loss_val: 0.3264 acc_val: 0.8996 time: 193.4864s\n",
            "Epoch: 0793 loss_train: 0.3298 acc_train: 0.8965 loss_val: 0.3310 acc_val: 0.8962 time: 193.7345s\n",
            "Epoch: 0794 loss_train: 0.3308 acc_train: 0.8942 loss_val: 0.3252 acc_val: 0.8962 time: 193.9817s\n",
            "Epoch: 0795 loss_train: 0.3298 acc_train: 0.8931 loss_val: 0.3252 acc_val: 0.8954 time: 194.2341s\n",
            "Epoch: 0796 loss_train: 0.3232 acc_train: 0.8944 loss_val: 0.3261 acc_val: 0.8954 time: 194.4886s\n",
            "Epoch: 0797 loss_train: 0.3339 acc_train: 0.8943 loss_val: 0.3267 acc_val: 0.8954 time: 194.7349s\n",
            "Epoch: 0798 loss_train: 0.3345 acc_train: 0.8932 loss_val: 0.3261 acc_val: 0.8958 time: 194.9873s\n",
            "Epoch: 0799 loss_train: 0.3205 acc_train: 0.8979 loss_val: 0.3236 acc_val: 0.8996 time: 195.2378s\n",
            "Epoch: 0800 loss_train: 0.3394 acc_train: 0.8931 loss_val: 0.3279 acc_val: 0.8971 time: 195.4833s\n",
            "Epoch: 0801 loss_train: 0.3375 acc_train: 0.8909 loss_val: 0.3245 acc_val: 0.8975 time: 195.7262s\n",
            "Epoch: 0802 loss_train: 0.3304 acc_train: 0.8931 loss_val: 0.3247 acc_val: 0.8988 time: 195.9717s\n",
            "Epoch: 0803 loss_train: 0.3303 acc_train: 0.8991 loss_val: 0.3323 acc_val: 0.8958 time: 196.2253s\n",
            "Epoch: 0804 loss_train: 0.3312 acc_train: 0.8945 loss_val: 0.3258 acc_val: 0.8971 time: 196.4695s\n",
            "Epoch: 0805 loss_train: 0.3344 acc_train: 0.8936 loss_val: 0.3257 acc_val: 0.8983 time: 196.7177s\n",
            "Epoch: 0806 loss_train: 0.3351 acc_train: 0.8936 loss_val: 0.3268 acc_val: 0.8958 time: 196.9619s\n",
            "Epoch: 0807 loss_train: 0.3202 acc_train: 0.8978 loss_val: 0.3278 acc_val: 0.8971 time: 197.2119s\n",
            "Epoch: 0808 loss_train: 0.3301 acc_train: 0.8965 loss_val: 0.3464 acc_val: 0.8900 time: 197.4605s\n",
            "Epoch: 0809 loss_train: 0.3528 acc_train: 0.8880 loss_val: 0.3351 acc_val: 0.8983 time: 197.7111s\n",
            "Epoch: 0810 loss_train: 0.3538 acc_train: 0.8911 loss_val: 0.3288 acc_val: 0.8967 time: 197.9665s\n",
            "Epoch: 0811 loss_train: 0.3372 acc_train: 0.8895 loss_val: 0.3351 acc_val: 0.8929 time: 198.2170s\n",
            "Epoch: 0812 loss_train: 0.3344 acc_train: 0.8912 loss_val: 0.3289 acc_val: 0.8954 time: 198.4719s\n",
            "Epoch: 0813 loss_train: 0.3472 acc_train: 0.8934 loss_val: 0.3430 acc_val: 0.8896 time: 198.7131s\n",
            "Epoch: 0814 loss_train: 0.3364 acc_train: 0.8911 loss_val: 0.3477 acc_val: 0.8875 time: 198.9618s\n",
            "Epoch: 0815 loss_train: 0.3408 acc_train: 0.8918 loss_val: 0.3352 acc_val: 0.8967 time: 199.2114s\n",
            "Epoch: 0816 loss_train: 0.3455 acc_train: 0.8929 loss_val: 0.3323 acc_val: 0.8954 time: 199.4551s\n",
            "Epoch: 0817 loss_train: 0.3420 acc_train: 0.8898 loss_val: 0.3335 acc_val: 0.8958 time: 199.6993s\n",
            "Epoch: 0818 loss_train: 0.3418 acc_train: 0.8893 loss_val: 0.3366 acc_val: 0.8971 time: 199.9445s\n",
            "Epoch: 0819 loss_train: 0.3500 acc_train: 0.8916 loss_val: 0.3266 acc_val: 0.8962 time: 200.1962s\n",
            "Epoch: 0820 loss_train: 0.3308 acc_train: 0.8958 loss_val: 0.3316 acc_val: 0.8962 time: 200.4472s\n",
            "Epoch: 0821 loss_train: 0.3376 acc_train: 0.8925 loss_val: 0.3236 acc_val: 0.8983 time: 200.6953s\n",
            "Epoch: 0822 loss_train: 0.3314 acc_train: 0.8971 loss_val: 0.3361 acc_val: 0.8929 time: 200.9409s\n",
            "Epoch: 0823 loss_train: 0.3432 acc_train: 0.8895 loss_val: 0.3327 acc_val: 0.8946 time: 201.1894s\n",
            "Epoch: 0824 loss_train: 0.3297 acc_train: 0.8943 loss_val: 0.3288 acc_val: 0.9000 time: 201.4335s\n",
            "Epoch: 0825 loss_train: 0.3379 acc_train: 0.8977 loss_val: 0.3289 acc_val: 0.9004 time: 201.6785s\n",
            "Epoch: 0826 loss_train: 0.3300 acc_train: 0.8969 loss_val: 0.3308 acc_val: 0.8958 time: 201.9322s\n",
            "Epoch: 0827 loss_train: 0.3318 acc_train: 0.8946 loss_val: 0.3305 acc_val: 0.8975 time: 202.1858s\n",
            "Epoch: 0828 loss_train: 0.3368 acc_train: 0.8933 loss_val: 0.3328 acc_val: 0.8958 time: 202.4146s\n",
            "Epoch: 0829 loss_train: 0.3471 acc_train: 0.8907 loss_val: 0.3403 acc_val: 0.8933 time: 202.6597s\n",
            "Epoch: 0830 loss_train: 0.3427 acc_train: 0.8921 loss_val: 0.3275 acc_val: 0.8962 time: 202.9069s\n",
            "Epoch: 0831 loss_train: 0.3319 acc_train: 0.8940 loss_val: 0.3315 acc_val: 0.8962 time: 203.1435s\n",
            "Epoch: 0832 loss_train: 0.3354 acc_train: 0.8945 loss_val: 0.3381 acc_val: 0.8917 time: 203.3977s\n",
            "Epoch: 0833 loss_train: 0.3440 acc_train: 0.8871 loss_val: 0.3312 acc_val: 0.8950 time: 203.6528s\n",
            "Epoch: 0834 loss_train: 0.3307 acc_train: 0.8937 loss_val: 0.3255 acc_val: 0.8983 time: 203.8983s\n",
            "Epoch: 0835 loss_train: 0.3323 acc_train: 0.8964 loss_val: 0.3255 acc_val: 0.8979 time: 204.1383s\n",
            "Epoch: 0836 loss_train: 0.3374 acc_train: 0.8919 loss_val: 0.3261 acc_val: 0.8975 time: 204.3893s\n",
            "Epoch: 0837 loss_train: 0.3232 acc_train: 0.8945 loss_val: 0.3227 acc_val: 0.8983 time: 204.6472s\n",
            "Epoch: 0838 loss_train: 0.3241 acc_train: 0.8955 loss_val: 0.3254 acc_val: 0.8971 time: 204.8935s\n",
            "Epoch: 0839 loss_train: 0.3310 acc_train: 0.8953 loss_val: 0.3239 acc_val: 0.8992 time: 205.1105s\n",
            "Epoch: 0840 loss_train: 0.3280 acc_train: 0.8930 loss_val: 0.3259 acc_val: 0.8967 time: 205.3562s\n",
            "Epoch: 0841 loss_train: 0.3272 acc_train: 0.8945 loss_val: 0.3240 acc_val: 0.8979 time: 205.6049s\n",
            "Epoch: 0842 loss_train: 0.3239 acc_train: 0.8974 loss_val: 0.3249 acc_val: 0.8975 time: 205.8500s\n",
            "Epoch: 0843 loss_train: 0.3236 acc_train: 0.8976 loss_val: 0.3264 acc_val: 0.8992 time: 206.1149s\n",
            "Epoch: 0844 loss_train: 0.3401 acc_train: 0.8943 loss_val: 0.3209 acc_val: 0.9004 time: 206.3587s\n",
            "Epoch: 0845 loss_train: 0.3294 acc_train: 0.8970 loss_val: 0.3216 acc_val: 0.9008 time: 206.6058s\n",
            "Epoch: 0846 loss_train: 0.3320 acc_train: 0.8931 loss_val: 0.3302 acc_val: 0.8967 time: 206.7855s\n",
            "Epoch: 0847 loss_train: 0.3369 acc_train: 0.8897 loss_val: 0.3241 acc_val: 0.8975 time: 207.0282s\n",
            "Epoch: 0848 loss_train: 0.3281 acc_train: 0.9018 loss_val: 0.3243 acc_val: 0.8975 time: 207.2788s\n",
            "Epoch: 0849 loss_train: 0.3254 acc_train: 0.8975 loss_val: 0.3273 acc_val: 0.8975 time: 207.5244s\n",
            "Epoch: 0850 loss_train: 0.3346 acc_train: 0.8914 loss_val: 0.3236 acc_val: 0.8996 time: 207.7831s\n",
            "Epoch: 0851 loss_train: 0.3292 acc_train: 0.8975 loss_val: 0.3289 acc_val: 0.8967 time: 208.0321s\n",
            "Epoch: 0852 loss_train: 0.3274 acc_train: 0.8956 loss_val: 0.3226 acc_val: 0.8983 time: 208.2805s\n",
            "Epoch: 0853 loss_train: 0.3347 acc_train: 0.8923 loss_val: 0.3208 acc_val: 0.8996 time: 208.5335s\n",
            "Epoch: 0854 loss_train: 0.3211 acc_train: 0.8989 loss_val: 0.3296 acc_val: 0.8950 time: 208.7784s\n",
            "Epoch: 0855 loss_train: 0.3324 acc_train: 0.8920 loss_val: 0.3282 acc_val: 0.8983 time: 209.0334s\n",
            "Epoch: 0856 loss_train: 0.3341 acc_train: 0.8970 loss_val: 0.3312 acc_val: 0.8962 time: 209.2921s\n",
            "Epoch: 0857 loss_train: 0.3222 acc_train: 0.8976 loss_val: 0.3269 acc_val: 0.8979 time: 209.5483s\n",
            "Epoch: 0858 loss_train: 0.3220 acc_train: 0.8992 loss_val: 0.3228 acc_val: 0.8992 time: 209.7950s\n",
            "Epoch: 0859 loss_train: 0.3311 acc_train: 0.8967 loss_val: 0.3501 acc_val: 0.8879 time: 210.0401s\n",
            "Epoch: 0860 loss_train: 0.3420 acc_train: 0.8868 loss_val: 0.3318 acc_val: 0.8958 time: 210.2296s\n",
            "Epoch: 0861 loss_train: 0.3430 acc_train: 0.8967 loss_val: 0.3448 acc_val: 0.8921 time: 210.4766s\n",
            "Epoch: 0862 loss_train: 0.3412 acc_train: 0.8881 loss_val: 0.3217 acc_val: 0.9021 time: 210.7065s\n",
            "Epoch: 0863 loss_train: 0.3326 acc_train: 0.8966 loss_val: 0.3341 acc_val: 0.8983 time: 210.9513s\n",
            "Epoch: 0864 loss_train: 0.3460 acc_train: 0.8953 loss_val: 0.3709 acc_val: 0.8804 time: 211.1974s\n",
            "Epoch: 0865 loss_train: 0.3770 acc_train: 0.8704 loss_val: 0.3574 acc_val: 0.8925 time: 211.4452s\n",
            "Epoch: 0866 loss_train: 0.3610 acc_train: 0.8892 loss_val: 0.3308 acc_val: 0.9004 time: 211.6973s\n",
            "Epoch: 0867 loss_train: 0.3330 acc_train: 0.8970 loss_val: 0.3541 acc_val: 0.8850 time: 211.9437s\n",
            "Epoch: 0868 loss_train: 0.3605 acc_train: 0.8785 loss_val: 0.3249 acc_val: 0.9000 time: 212.1739s\n",
            "Epoch: 0869 loss_train: 0.3377 acc_train: 0.8953 loss_val: 0.3372 acc_val: 0.8938 time: 212.4323s\n",
            "Epoch: 0870 loss_train: 0.3457 acc_train: 0.8896 loss_val: 0.3529 acc_val: 0.8888 time: 212.6863s\n",
            "Epoch: 0871 loss_train: 0.3593 acc_train: 0.8852 loss_val: 0.3385 acc_val: 0.8996 time: 212.9323s\n",
            "Epoch: 0872 loss_train: 0.3478 acc_train: 0.8941 loss_val: 0.3440 acc_val: 0.8933 time: 213.1787s\n",
            "Epoch: 0873 loss_train: 0.3455 acc_train: 0.8896 loss_val: 0.3394 acc_val: 0.8892 time: 213.4290s\n",
            "Epoch: 0874 loss_train: 0.3444 acc_train: 0.8893 loss_val: 0.3435 acc_val: 0.8908 time: 213.6784s\n",
            "Epoch: 0875 loss_train: 0.3581 acc_train: 0.8866 loss_val: 0.3300 acc_val: 0.8967 time: 213.9254s\n",
            "Epoch: 0876 loss_train: 0.3338 acc_train: 0.8962 loss_val: 0.3268 acc_val: 0.8942 time: 214.1678s\n",
            "Epoch: 0877 loss_train: 0.3325 acc_train: 0.9008 loss_val: 0.3279 acc_val: 0.8954 time: 214.4196s\n",
            "Epoch: 0878 loss_train: 0.3280 acc_train: 0.8934 loss_val: 0.3290 acc_val: 0.8967 time: 214.6691s\n",
            "Epoch: 0879 loss_train: 0.3444 acc_train: 0.8896 loss_val: 0.3408 acc_val: 0.8946 time: 214.9118s\n",
            "Epoch: 0880 loss_train: 0.3475 acc_train: 0.8892 loss_val: 0.3272 acc_val: 0.8992 time: 215.1572s\n",
            "Epoch: 0881 loss_train: 0.3315 acc_train: 0.8989 loss_val: 0.3350 acc_val: 0.8921 time: 215.4043s\n",
            "Epoch: 0882 loss_train: 0.3332 acc_train: 0.8936 loss_val: 0.3352 acc_val: 0.8925 time: 215.6305s\n",
            "Epoch: 0883 loss_train: 0.3336 acc_train: 0.8908 loss_val: 0.3295 acc_val: 0.8950 time: 215.8698s\n",
            "Epoch: 0884 loss_train: 0.3346 acc_train: 0.8942 loss_val: 0.3307 acc_val: 0.8946 time: 216.1191s\n",
            "Epoch: 0885 loss_train: 0.3311 acc_train: 0.8905 loss_val: 0.3372 acc_val: 0.8921 time: 216.3682s\n",
            "Epoch: 0886 loss_train: 0.3324 acc_train: 0.8914 loss_val: 0.3295 acc_val: 0.9000 time: 216.6167s\n",
            "Epoch: 0887 loss_train: 0.3384 acc_train: 0.8984 loss_val: 0.3307 acc_val: 0.8950 time: 216.8573s\n",
            "Epoch: 0888 loss_train: 0.3221 acc_train: 0.8958 loss_val: 0.3398 acc_val: 0.8917 time: 217.0741s\n",
            "Epoch: 0889 loss_train: 0.3329 acc_train: 0.8918 loss_val: 0.3283 acc_val: 0.8938 time: 217.3184s\n",
            "Epoch: 0890 loss_train: 0.3314 acc_train: 0.8925 loss_val: 0.3262 acc_val: 0.8983 time: 217.5814s\n",
            "Epoch: 0891 loss_train: 0.3224 acc_train: 0.8998 loss_val: 0.3357 acc_val: 0.8954 time: 217.8310s\n",
            "Epoch: 0892 loss_train: 0.3303 acc_train: 0.8924 loss_val: 0.3275 acc_val: 0.8954 time: 218.0775s\n",
            "Epoch: 0893 loss_train: 0.3288 acc_train: 0.8970 loss_val: 0.3230 acc_val: 0.8967 time: 218.3090s\n",
            "Epoch: 0894 loss_train: 0.3217 acc_train: 0.8985 loss_val: 0.3448 acc_val: 0.8867 time: 218.5444s\n",
            "Epoch: 0895 loss_train: 0.3450 acc_train: 0.8831 loss_val: 0.3228 acc_val: 0.9012 time: 218.7466s\n",
            "Epoch: 0896 loss_train: 0.3255 acc_train: 0.9013 loss_val: 0.3313 acc_val: 0.8958 time: 218.9886s\n",
            "Epoch: 0897 loss_train: 0.3305 acc_train: 0.8943 loss_val: 0.3313 acc_val: 0.8958 time: 219.2476s\n",
            "Epoch: 0898 loss_train: 0.3313 acc_train: 0.8923 loss_val: 0.3255 acc_val: 0.8983 time: 219.5012s\n",
            "Epoch: 0899 loss_train: 0.3228 acc_train: 0.8960 loss_val: 0.3259 acc_val: 0.9017 time: 219.7559s\n",
            "Epoch: 0900 loss_train: 0.3251 acc_train: 0.8992 loss_val: 0.3305 acc_val: 0.8992 time: 220.0047s\n",
            "Epoch: 0901 loss_train: 0.3285 acc_train: 0.8952 loss_val: 0.3262 acc_val: 0.9008 time: 220.2473s\n",
            "Epoch: 0902 loss_train: 0.3327 acc_train: 0.8974 loss_val: 0.3240 acc_val: 0.8992 time: 220.5050s\n",
            "Epoch: 0903 loss_train: 0.3246 acc_train: 0.8927 loss_val: 0.3252 acc_val: 0.9021 time: 220.7160s\n",
            "Epoch: 0904 loss_train: 0.3246 acc_train: 0.8965 loss_val: 0.3249 acc_val: 0.8996 time: 220.9754s\n",
            "Epoch: 0905 loss_train: 0.3231 acc_train: 0.8968 loss_val: 0.3256 acc_val: 0.9021 time: 221.2277s\n",
            "Epoch: 0906 loss_train: 0.3288 acc_train: 0.8973 loss_val: 0.3226 acc_val: 0.8983 time: 221.4666s\n",
            "Epoch: 0907 loss_train: 0.3172 acc_train: 0.9011 loss_val: 0.3294 acc_val: 0.8996 time: 221.7191s\n",
            "Epoch: 0908 loss_train: 0.3255 acc_train: 0.8940 loss_val: 0.3213 acc_val: 0.9025 time: 221.9682s\n",
            "Epoch: 0909 loss_train: 0.3220 acc_train: 0.8979 loss_val: 0.3265 acc_val: 0.8988 time: 222.1859s\n",
            "Epoch: 0910 loss_train: 0.3344 acc_train: 0.8954 loss_val: 0.3409 acc_val: 0.8946 time: 222.4355s\n",
            "Epoch: 0911 loss_train: 0.3436 acc_train: 0.8866 loss_val: 0.3265 acc_val: 0.8950 time: 222.6795s\n",
            "Epoch: 0912 loss_train: 0.3369 acc_train: 0.8932 loss_val: 0.3242 acc_val: 0.8992 time: 222.9139s\n",
            "Epoch: 0913 loss_train: 0.3257 acc_train: 0.8964 loss_val: 0.3394 acc_val: 0.8908 time: 223.1655s\n",
            "Epoch: 0914 loss_train: 0.3317 acc_train: 0.8904 loss_val: 0.3314 acc_val: 0.8975 time: 223.4094s\n",
            "Epoch: 0915 loss_train: 0.3307 acc_train: 0.9029 loss_val: 0.3279 acc_val: 0.8983 time: 223.6687s\n",
            "Epoch: 0916 loss_train: 0.3235 acc_train: 0.8973 loss_val: 0.3355 acc_val: 0.8946 time: 223.9154s\n",
            "Epoch: 0917 loss_train: 0.3340 acc_train: 0.8924 loss_val: 0.3392 acc_val: 0.8979 time: 224.1543s\n",
            "Epoch: 0918 loss_train: 0.3442 acc_train: 0.8936 loss_val: 0.3560 acc_val: 0.8875 time: 224.3987s\n",
            "Epoch: 0919 loss_train: 0.3576 acc_train: 0.8841 loss_val: 0.3222 acc_val: 0.8983 time: 224.6518s\n",
            "Epoch: 0920 loss_train: 0.3208 acc_train: 0.8971 loss_val: 0.3406 acc_val: 0.8942 time: 224.8971s\n",
            "Epoch: 0921 loss_train: 0.3407 acc_train: 0.8951 loss_val: 0.3473 acc_val: 0.8904 time: 225.1520s\n",
            "Epoch: 0922 loss_train: 0.3339 acc_train: 0.8882 loss_val: 0.3403 acc_val: 0.8908 time: 225.4007s\n",
            "Epoch: 0923 loss_train: 0.3410 acc_train: 0.8920 loss_val: 0.3269 acc_val: 0.8967 time: 225.6570s\n",
            "Epoch: 0924 loss_train: 0.3258 acc_train: 0.8998 loss_val: 0.3355 acc_val: 0.8892 time: 225.9257s\n",
            "Epoch: 0925 loss_train: 0.3406 acc_train: 0.8900 loss_val: 0.3247 acc_val: 0.8950 time: 226.1841s\n",
            "Epoch: 0926 loss_train: 0.3247 acc_train: 0.8979 loss_val: 0.3294 acc_val: 0.8942 time: 226.4276s\n",
            "Epoch: 0927 loss_train: 0.3293 acc_train: 0.8944 loss_val: 0.3288 acc_val: 0.8988 time: 226.6817s\n",
            "Epoch: 0928 loss_train: 0.3225 acc_train: 0.8973 loss_val: 0.3282 acc_val: 0.8962 time: 226.9283s\n",
            "Epoch: 0929 loss_train: 0.3302 acc_train: 0.8921 loss_val: 0.3311 acc_val: 0.9004 time: 227.1788s\n",
            "Epoch: 0930 loss_train: 0.3397 acc_train: 0.8960 loss_val: 0.3443 acc_val: 0.8900 time: 227.4218s\n",
            "Epoch: 0931 loss_train: 0.3376 acc_train: 0.8899 loss_val: 0.3353 acc_val: 0.8971 time: 227.6713s\n",
            "Epoch: 0932 loss_train: 0.3305 acc_train: 0.8947 loss_val: 0.3233 acc_val: 0.8996 time: 227.9149s\n",
            "Epoch: 0933 loss_train: 0.3231 acc_train: 0.8996 loss_val: 0.3253 acc_val: 0.9017 time: 228.1764s\n",
            "Epoch: 0934 loss_train: 0.3265 acc_train: 0.8976 loss_val: 0.3240 acc_val: 0.9004 time: 228.4199s\n",
            "Epoch: 0935 loss_train: 0.3247 acc_train: 0.8973 loss_val: 0.3242 acc_val: 0.8958 time: 228.6685s\n",
            "Epoch: 0936 loss_train: 0.3324 acc_train: 0.8920 loss_val: 0.3238 acc_val: 0.8983 time: 228.9143s\n",
            "Epoch: 0937 loss_train: 0.3264 acc_train: 0.8973 loss_val: 0.3288 acc_val: 0.8958 time: 229.1593s\n",
            "Epoch: 0938 loss_train: 0.3191 acc_train: 0.8990 loss_val: 0.3214 acc_val: 0.9008 time: 229.3977s\n",
            "Epoch: 0939 loss_train: 0.3177 acc_train: 0.9019 loss_val: 0.3238 acc_val: 0.8988 time: 229.6502s\n",
            "Epoch: 0940 loss_train: 0.3299 acc_train: 0.8933 loss_val: 0.3333 acc_val: 0.8967 time: 229.9038s\n",
            "Epoch: 0941 loss_train: 0.3325 acc_train: 0.8931 loss_val: 0.3252 acc_val: 0.8979 time: 230.1479s\n",
            "Epoch: 0942 loss_train: 0.3321 acc_train: 0.8988 loss_val: 0.3262 acc_val: 0.8983 time: 230.3842s\n",
            "Epoch: 0943 loss_train: 0.3237 acc_train: 0.8958 loss_val: 0.3269 acc_val: 0.8971 time: 230.6199s\n",
            "Epoch: 0944 loss_train: 0.3290 acc_train: 0.8896 loss_val: 0.3310 acc_val: 0.8971 time: 230.8569s\n",
            "Epoch: 0945 loss_train: 0.3362 acc_train: 0.8987 loss_val: 0.3435 acc_val: 0.8929 time: 231.1064s\n",
            "Epoch: 0946 loss_train: 0.3388 acc_train: 0.8913 loss_val: 0.3273 acc_val: 0.8979 time: 231.3562s\n",
            "Epoch: 0947 loss_train: 0.3247 acc_train: 0.8941 loss_val: 0.3433 acc_val: 0.8950 time: 231.6043s\n",
            "Epoch: 0948 loss_train: 0.3591 acc_train: 0.8888 loss_val: 0.3899 acc_val: 0.8758 time: 231.8501s\n",
            "Epoch: 0949 loss_train: 0.3704 acc_train: 0.8770 loss_val: 0.3526 acc_val: 0.8938 time: 232.0953s\n",
            "Epoch: 0950 loss_train: 0.3515 acc_train: 0.8958 loss_val: 0.3390 acc_val: 0.9012 time: 232.3394s\n",
            "Epoch: 0951 loss_train: 0.3516 acc_train: 0.8969 loss_val: 0.3914 acc_val: 0.8700 time: 232.5931s\n",
            "Epoch: 0952 loss_train: 0.3824 acc_train: 0.8703 loss_val: 0.3469 acc_val: 0.8900 time: 232.8259s\n",
            "Epoch: 0953 loss_train: 0.3713 acc_train: 0.8818 loss_val: 0.3511 acc_val: 0.8942 time: 233.0814s\n",
            "Epoch: 0954 loss_train: 0.3557 acc_train: 0.8863 loss_val: 0.3447 acc_val: 0.8942 time: 233.3248s\n",
            "Epoch: 0955 loss_train: 0.3745 acc_train: 0.8849 loss_val: 0.3456 acc_val: 0.9000 time: 233.5736s\n",
            "Epoch: 0956 loss_train: 0.3601 acc_train: 0.8918 loss_val: 0.3481 acc_val: 0.8958 time: 233.8345s\n",
            "Epoch: 0957 loss_train: 0.3569 acc_train: 0.8897 loss_val: 0.3443 acc_val: 0.8954 time: 234.0954s\n",
            "Epoch: 0958 loss_train: 0.3500 acc_train: 0.8932 loss_val: 0.3330 acc_val: 0.8938 time: 234.3435s\n",
            "Epoch: 0959 loss_train: 0.3464 acc_train: 0.8908 loss_val: 0.3224 acc_val: 0.9008 time: 234.5853s\n",
            "Epoch: 0960 loss_train: 0.3273 acc_train: 0.8977 loss_val: 0.3408 acc_val: 0.8938 time: 234.7798s\n",
            "Epoch: 0961 loss_train: 0.3424 acc_train: 0.8913 loss_val: 0.3331 acc_val: 0.8988 time: 235.0223s\n",
            "Epoch: 0962 loss_train: 0.3339 acc_train: 0.8945 loss_val: 0.3366 acc_val: 0.8988 time: 235.2395s\n",
            "Epoch: 0963 loss_train: 0.3428 acc_train: 0.8952 loss_val: 0.3425 acc_val: 0.8921 time: 235.4839s\n",
            "Epoch: 0964 loss_train: 0.3372 acc_train: 0.8910 loss_val: 0.3483 acc_val: 0.8912 time: 235.7337s\n",
            "Epoch: 0965 loss_train: 0.3500 acc_train: 0.8878 loss_val: 0.3421 acc_val: 0.8958 time: 235.9839s\n",
            "Epoch: 0966 loss_train: 0.3607 acc_train: 0.8911 loss_val: 0.3344 acc_val: 0.8933 time: 236.2328s\n",
            "Epoch: 0967 loss_train: 0.3308 acc_train: 0.8940 loss_val: 0.3416 acc_val: 0.8925 time: 236.4834s\n",
            "Epoch: 0968 loss_train: 0.3309 acc_train: 0.8930 loss_val: 0.3353 acc_val: 0.8971 time: 236.7340s\n",
            "Epoch: 0969 loss_train: 0.3364 acc_train: 0.8941 loss_val: 0.3306 acc_val: 0.9004 time: 236.9791s\n",
            "Epoch: 0970 loss_train: 0.3332 acc_train: 0.8952 loss_val: 0.3890 acc_val: 0.8767 time: 237.2320s\n",
            "Epoch: 0971 loss_train: 0.3821 acc_train: 0.8713 loss_val: 0.3520 acc_val: 0.8862 time: 237.4816s\n",
            "Epoch: 0972 loss_train: 0.3597 acc_train: 0.8857 loss_val: 0.3359 acc_val: 0.8992 time: 237.7360s\n",
            "Epoch: 0973 loss_train: 0.3364 acc_train: 0.8956 loss_val: 0.3637 acc_val: 0.8792 time: 237.9803s\n",
            "Epoch: 0974 loss_train: 0.3551 acc_train: 0.8763 loss_val: 0.3307 acc_val: 0.8946 time: 238.2327s\n",
            "Epoch: 0975 loss_train: 0.3321 acc_train: 0.8913 loss_val: 0.3453 acc_val: 0.8958 time: 238.4753s\n",
            "Epoch: 0976 loss_train: 0.3421 acc_train: 0.8958 loss_val: 0.3578 acc_val: 0.8929 time: 238.7383s\n",
            "Epoch: 0977 loss_train: 0.3425 acc_train: 0.8910 loss_val: 0.3324 acc_val: 0.8992 time: 238.9849s\n",
            "Epoch: 0978 loss_train: 0.3321 acc_train: 0.8930 loss_val: 0.3495 acc_val: 0.8912 time: 239.2298s\n",
            "Epoch: 0979 loss_train: 0.3693 acc_train: 0.8879 loss_val: 0.3842 acc_val: 0.8788 time: 239.4836s\n",
            "Epoch: 0980 loss_train: 0.3711 acc_train: 0.8776 loss_val: 0.3554 acc_val: 0.8846 time: 239.6898s\n",
            "Epoch: 0981 loss_train: 0.3497 acc_train: 0.8873 loss_val: 0.3779 acc_val: 0.8946 time: 239.9545s\n",
            "Epoch: 0982 loss_train: 0.3887 acc_train: 0.8919 loss_val: 0.3897 acc_val: 0.8725 time: 240.2074s\n",
            "Epoch: 0983 loss_train: 0.3865 acc_train: 0.8640 loss_val: 0.3372 acc_val: 0.8921 time: 240.4498s\n",
            "Epoch: 0984 loss_train: 0.3309 acc_train: 0.8968 loss_val: 0.3547 acc_val: 0.8900 time: 240.6969s\n",
            "Epoch: 0985 loss_train: 0.3776 acc_train: 0.8831 loss_val: 0.4415 acc_val: 0.8617 time: 240.9445s\n",
            "Epoch: 0986 loss_train: 0.4453 acc_train: 0.8544 loss_val: 0.3598 acc_val: 0.8925 time: 241.1891s\n",
            "Epoch: 0987 loss_train: 0.3712 acc_train: 0.8944 loss_val: 0.3629 acc_val: 0.8908 time: 241.4405s\n",
            "Epoch: 0988 loss_train: 0.3773 acc_train: 0.8842 loss_val: 0.4119 acc_val: 0.8650 time: 241.6944s\n",
            "Epoch: 0989 loss_train: 0.3768 acc_train: 0.8747 loss_val: 0.3498 acc_val: 0.8862 time: 241.8839s\n",
            "Epoch: 0990 loss_train: 0.3453 acc_train: 0.8830 loss_val: 0.3752 acc_val: 0.8871 time: 242.1355s\n",
            "Epoch: 0991 loss_train: 0.4022 acc_train: 0.8763 loss_val: 0.3482 acc_val: 0.8975 time: 242.3769s\n",
            "Epoch: 0992 loss_train: 0.3544 acc_train: 0.8913 loss_val: 0.3497 acc_val: 0.8858 time: 242.6280s\n",
            "Epoch: 0993 loss_train: 0.3424 acc_train: 0.8829 loss_val: 0.3488 acc_val: 0.8829 time: 242.8783s\n",
            "Epoch: 0994 loss_train: 0.3459 acc_train: 0.8818 loss_val: 0.3481 acc_val: 0.8946 time: 243.1283s\n",
            "Epoch: 0995 loss_train: 0.3629 acc_train: 0.8887 loss_val: 0.3486 acc_val: 0.8938 time: 243.3773s\n",
            "Epoch: 0996 loss_train: 0.3457 acc_train: 0.8931 loss_val: 0.3554 acc_val: 0.8900 time: 243.6111s\n",
            "Epoch: 0997 loss_train: 0.3532 acc_train: 0.8938 loss_val: 0.3392 acc_val: 0.8950 time: 243.8627s\n",
            "Epoch: 0998 loss_train: 0.3467 acc_train: 0.8898 loss_val: 0.3340 acc_val: 0.8983 time: 244.1080s\n",
            "Epoch: 0999 loss_train: 0.3395 acc_train: 0.8963 loss_val: 0.3346 acc_val: 0.8942 time: 244.3583s\n",
            "Epoch: 1000 loss_train: 0.3379 acc_train: 0.8926 loss_val: 0.3378 acc_val: 0.8933 time: 244.5979s\n",
            "Epoch: 1001 loss_train: 0.3212 acc_train: 0.8957 loss_val: 0.3336 acc_val: 0.8979 time: 244.8492s\n",
            "Epoch: 1002 loss_train: 0.3373 acc_train: 0.8899 loss_val: 0.3306 acc_val: 0.8992 time: 245.0727s\n",
            "Epoch: 1003 loss_train: 0.3448 acc_train: 0.8916 loss_val: 0.3334 acc_val: 0.8971 time: 245.2974s\n",
            "Epoch: 1004 loss_train: 0.3333 acc_train: 0.8976 loss_val: 0.3323 acc_val: 0.8950 time: 245.5449s\n",
            "Epoch: 1005 loss_train: 0.3334 acc_train: 0.8896 loss_val: 0.3250 acc_val: 0.8979 time: 245.7931s\n",
            "Epoch: 1006 loss_train: 0.3248 acc_train: 0.8957 loss_val: 0.3301 acc_val: 0.8933 time: 246.0382s\n",
            "Epoch: 1007 loss_train: 0.3277 acc_train: 0.8927 loss_val: 0.3330 acc_val: 0.8975 time: 246.2850s\n",
            "Epoch: 1008 loss_train: 0.3218 acc_train: 0.8941 loss_val: 0.3289 acc_val: 0.8983 time: 246.5272s\n",
            "Epoch: 1009 loss_train: 0.3264 acc_train: 0.8979 loss_val: 0.3326 acc_val: 0.8967 time: 246.7783s\n",
            "Epoch: 1010 loss_train: 0.3421 acc_train: 0.9001 loss_val: 0.3236 acc_val: 0.8992 time: 247.0389s\n",
            "Epoch: 1011 loss_train: 0.3154 acc_train: 0.8992 loss_val: 0.3362 acc_val: 0.8954 time: 247.2863s\n",
            "Epoch: 1012 loss_train: 0.3305 acc_train: 0.8926 loss_val: 0.3235 acc_val: 0.8967 time: 247.5331s\n",
            "Epoch: 1013 loss_train: 0.3216 acc_train: 0.8978 loss_val: 0.3219 acc_val: 0.9000 time: 247.7869s\n",
            "Epoch: 1014 loss_train: 0.3254 acc_train: 0.8957 loss_val: 0.3347 acc_val: 0.8962 time: 247.9959s\n",
            "Epoch: 1015 loss_train: 0.3288 acc_train: 0.8987 loss_val: 0.3270 acc_val: 0.8988 time: 248.2268s\n",
            "Epoch: 1016 loss_train: 0.3184 acc_train: 0.8988 loss_val: 0.3252 acc_val: 0.8988 time: 248.4689s\n",
            "Epoch: 1017 loss_train: 0.3237 acc_train: 0.9003 loss_val: 0.3318 acc_val: 0.8979 time: 248.7134s\n",
            "Epoch: 1018 loss_train: 0.3267 acc_train: 0.8953 loss_val: 0.3265 acc_val: 0.9004 time: 248.9726s\n",
            "Epoch: 1019 loss_train: 0.3214 acc_train: 0.8947 loss_val: 0.3242 acc_val: 0.8992 time: 249.2164s\n",
            "Epoch: 1020 loss_train: 0.3250 acc_train: 0.9019 loss_val: 0.3284 acc_val: 0.8992 time: 249.4656s\n",
            "Epoch: 1021 loss_train: 0.3266 acc_train: 0.8964 loss_val: 0.3253 acc_val: 0.9000 time: 249.7095s\n",
            "Epoch: 1022 loss_train: 0.3166 acc_train: 0.8981 loss_val: 0.3198 acc_val: 0.9021 time: 249.9629s\n",
            "Epoch: 1023 loss_train: 0.3195 acc_train: 0.8974 loss_val: 0.3209 acc_val: 0.8988 time: 250.2077s\n",
            "Epoch: 1024 loss_train: 0.3233 acc_train: 0.8967 loss_val: 0.3238 acc_val: 0.9021 time: 250.4571s\n",
            "Epoch: 1025 loss_train: 0.3170 acc_train: 0.8955 loss_val: 0.3244 acc_val: 0.9025 time: 250.7104s\n",
            "Epoch: 1026 loss_train: 0.3223 acc_train: 0.8988 loss_val: 0.3243 acc_val: 0.8983 time: 250.9520s\n",
            "Epoch: 1027 loss_train: 0.3236 acc_train: 0.9012 loss_val: 0.3237 acc_val: 0.8983 time: 251.1949s\n",
            "Epoch: 1028 loss_train: 0.3220 acc_train: 0.8964 loss_val: 0.3299 acc_val: 0.8971 time: 251.4124s\n",
            "Epoch: 1029 loss_train: 0.3236 acc_train: 0.8932 loss_val: 0.3212 acc_val: 0.8983 time: 251.6403s\n",
            "Epoch: 1030 loss_train: 0.3221 acc_train: 0.8977 loss_val: 0.3213 acc_val: 0.9008 time: 251.8912s\n",
            "Epoch: 1031 loss_train: 0.3247 acc_train: 0.8975 loss_val: 0.3280 acc_val: 0.9004 time: 252.1347s\n",
            "Epoch: 1032 loss_train: 0.3222 acc_train: 0.8927 loss_val: 0.3217 acc_val: 0.9004 time: 252.3815s\n",
            "Epoch: 1033 loss_train: 0.3238 acc_train: 0.8957 loss_val: 0.3207 acc_val: 0.8992 time: 252.6235s\n",
            "Epoch: 1034 loss_train: 0.3236 acc_train: 0.8980 loss_val: 0.3244 acc_val: 0.8979 time: 252.8717s\n",
            "Epoch: 1035 loss_train: 0.3271 acc_train: 0.8957 loss_val: 0.3254 acc_val: 0.8979 time: 253.1135s\n",
            "Epoch: 1036 loss_train: 0.3212 acc_train: 0.8960 loss_val: 0.3220 acc_val: 0.8983 time: 253.3587s\n",
            "Epoch: 1037 loss_train: 0.3171 acc_train: 0.9038 loss_val: 0.3196 acc_val: 0.9021 time: 253.6047s\n",
            "Epoch: 1038 loss_train: 0.3150 acc_train: 0.9004 loss_val: 0.3271 acc_val: 0.8983 time: 253.8577s\n",
            "Epoch: 1039 loss_train: 0.3150 acc_train: 0.8958 loss_val: 0.3207 acc_val: 0.8979 time: 254.1019s\n",
            "Epoch: 1040 loss_train: 0.3226 acc_train: 0.8951 loss_val: 0.3208 acc_val: 0.8992 time: 254.3498s\n",
            "Epoch: 1041 loss_train: 0.3157 acc_train: 0.8977 loss_val: 0.3247 acc_val: 0.8996 time: 254.6000s\n",
            "Epoch: 1042 loss_train: 0.3158 acc_train: 0.9002 loss_val: 0.3211 acc_val: 0.9004 time: 254.8527s\n",
            "Epoch: 1043 loss_train: 0.3122 acc_train: 0.9038 loss_val: 0.3219 acc_val: 0.9000 time: 255.1033s\n",
            "Epoch: 1044 loss_train: 0.3187 acc_train: 0.8975 loss_val: 0.3236 acc_val: 0.8992 time: 255.3479s\n",
            "Epoch: 1045 loss_train: 0.3097 acc_train: 0.9012 loss_val: 0.3182 acc_val: 0.9021 time: 255.5927s\n",
            "Epoch: 1046 loss_train: 0.3098 acc_train: 0.9026 loss_val: 0.3239 acc_val: 0.9017 time: 255.8436s\n",
            "Epoch: 1047 loss_train: 0.3072 acc_train: 0.9042 loss_val: 0.3214 acc_val: 0.9025 time: 256.0974s\n",
            "Epoch: 1048 loss_train: 0.3098 acc_train: 0.9003 loss_val: 0.3215 acc_val: 0.9017 time: 256.3429s\n",
            "Epoch: 1049 loss_train: 0.3287 acc_train: 0.8979 loss_val: 0.3229 acc_val: 0.9021 time: 256.5866s\n",
            "Epoch: 1050 loss_train: 0.3193 acc_train: 0.8960 loss_val: 0.3228 acc_val: 0.8992 time: 256.8164s\n",
            "Epoch: 1051 loss_train: 0.3189 acc_train: 0.8944 loss_val: 0.3205 acc_val: 0.8992 time: 257.0640s\n",
            "Epoch: 1052 loss_train: 0.3170 acc_train: 0.9010 loss_val: 0.3214 acc_val: 0.9000 time: 257.3185s\n",
            "Epoch: 1053 loss_train: 0.3156 acc_train: 0.9040 loss_val: 0.3268 acc_val: 0.8992 time: 257.5741s\n",
            "Epoch: 1054 loss_train: 0.3212 acc_train: 0.8926 loss_val: 0.3180 acc_val: 0.9017 time: 257.8199s\n",
            "Epoch: 1055 loss_train: 0.3152 acc_train: 0.8987 loss_val: 0.3184 acc_val: 0.9054 time: 258.0558s\n",
            "Epoch: 1056 loss_train: 0.3137 acc_train: 0.8993 loss_val: 0.3227 acc_val: 0.9025 time: 258.2337s\n",
            "Epoch: 1057 loss_train: 0.3154 acc_train: 0.8971 loss_val: 0.3181 acc_val: 0.9008 time: 258.4339s\n",
            "Epoch: 1058 loss_train: 0.3165 acc_train: 0.9005 loss_val: 0.3180 acc_val: 0.8983 time: 258.6437s\n",
            "Epoch: 1059 loss_train: 0.3124 acc_train: 0.9012 loss_val: 0.3272 acc_val: 0.8988 time: 258.9127s\n",
            "Epoch: 1060 loss_train: 0.3181 acc_train: 0.8973 loss_val: 0.3242 acc_val: 0.9000 time: 259.1672s\n",
            "Epoch: 1061 loss_train: 0.3137 acc_train: 0.8990 loss_val: 0.3267 acc_val: 0.8967 time: 259.4127s\n",
            "Epoch: 1062 loss_train: 0.3274 acc_train: 0.9021 loss_val: 0.3264 acc_val: 0.9004 time: 259.6609s\n",
            "Epoch: 1063 loss_train: 0.3060 acc_train: 0.8995 loss_val: 0.3270 acc_val: 0.9000 time: 259.9114s\n",
            "Epoch: 1064 loss_train: 0.3176 acc_train: 0.8980 loss_val: 0.3189 acc_val: 0.9004 time: 260.1623s\n",
            "Epoch: 1065 loss_train: 0.3168 acc_train: 0.9002 loss_val: 0.3190 acc_val: 0.9033 time: 260.4101s\n",
            "Epoch: 1066 loss_train: 0.3155 acc_train: 0.8998 loss_val: 0.3324 acc_val: 0.8971 time: 260.6616s\n",
            "Epoch: 1067 loss_train: 0.3197 acc_train: 0.8927 loss_val: 0.3267 acc_val: 0.8975 time: 260.9112s\n",
            "Epoch: 1068 loss_train: 0.3213 acc_train: 0.9024 loss_val: 0.3204 acc_val: 0.9012 time: 261.1555s\n",
            "Epoch: 1069 loss_train: 0.3148 acc_train: 0.9025 loss_val: 0.3330 acc_val: 0.8954 time: 261.3995s\n",
            "Epoch: 1070 loss_train: 0.3361 acc_train: 0.8892 loss_val: 0.3191 acc_val: 0.9025 time: 261.6460s\n",
            "Epoch: 1071 loss_train: 0.3205 acc_train: 0.9004 loss_val: 0.3330 acc_val: 0.8988 time: 261.9008s\n",
            "Epoch: 1072 loss_train: 0.3288 acc_train: 0.8932 loss_val: 0.3307 acc_val: 0.8983 time: 262.1454s\n",
            "Epoch: 1073 loss_train: 0.3230 acc_train: 0.8949 loss_val: 0.3206 acc_val: 0.9017 time: 262.3862s\n",
            "Epoch: 1074 loss_train: 0.3183 acc_train: 0.9012 loss_val: 0.3236 acc_val: 0.8992 time: 262.6334s\n",
            "Epoch: 1075 loss_train: 0.3178 acc_train: 0.8963 loss_val: 0.3188 acc_val: 0.9046 time: 262.8764s\n",
            "Epoch: 1076 loss_train: 0.3148 acc_train: 0.9018 loss_val: 0.3213 acc_val: 0.8992 time: 263.1289s\n",
            "Epoch: 1077 loss_train: 0.3131 acc_train: 0.9030 loss_val: 0.3197 acc_val: 0.9008 time: 263.3828s\n",
            "Epoch: 1078 loss_train: 0.3129 acc_train: 0.9000 loss_val: 0.3204 acc_val: 0.9021 time: 263.5992s\n",
            "Epoch: 1079 loss_train: 0.3092 acc_train: 0.9000 loss_val: 0.3193 acc_val: 0.9012 time: 263.7728s\n",
            "Epoch: 1080 loss_train: 0.3140 acc_train: 0.8974 loss_val: 0.3183 acc_val: 0.9021 time: 264.0166s\n",
            "Epoch: 1081 loss_train: 0.3109 acc_train: 0.9023 loss_val: 0.3232 acc_val: 0.9008 time: 264.2568s\n",
            "Epoch: 1082 loss_train: 0.3178 acc_train: 0.9002 loss_val: 0.3211 acc_val: 0.9021 time: 264.4976s\n",
            "Epoch: 1083 loss_train: 0.3062 acc_train: 0.9021 loss_val: 0.3169 acc_val: 0.9029 time: 264.7502s\n",
            "Epoch: 1084 loss_train: 0.3100 acc_train: 0.9037 loss_val: 0.3187 acc_val: 0.9025 time: 264.9992s\n",
            "Epoch: 1085 loss_train: 0.3106 acc_train: 0.9003 loss_val: 0.3192 acc_val: 0.9017 time: 265.2439s\n",
            "Epoch: 1086 loss_train: 0.3085 acc_train: 0.9002 loss_val: 0.3194 acc_val: 0.9046 time: 265.4874s\n",
            "Epoch: 1087 loss_train: 0.3154 acc_train: 0.9018 loss_val: 0.3241 acc_val: 0.9017 time: 265.7085s\n",
            "Epoch: 1088 loss_train: 0.3183 acc_train: 0.9005 loss_val: 0.3171 acc_val: 0.9017 time: 265.9643s\n",
            "Epoch: 1089 loss_train: 0.3120 acc_train: 0.8997 loss_val: 0.3181 acc_val: 0.9012 time: 266.2097s\n",
            "Epoch: 1090 loss_train: 0.3157 acc_train: 0.8979 loss_val: 0.3224 acc_val: 0.9025 time: 266.4681s\n",
            "Epoch: 1091 loss_train: 0.3125 acc_train: 0.8968 loss_val: 0.3203 acc_val: 0.9029 time: 266.7154s\n",
            "Epoch: 1092 loss_train: 0.3078 acc_train: 0.9059 loss_val: 0.3189 acc_val: 0.9046 time: 266.9763s\n",
            "Epoch: 1093 loss_train: 0.3141 acc_train: 0.9011 loss_val: 0.3268 acc_val: 0.8979 time: 267.2254s\n",
            "Epoch: 1094 loss_train: 0.3168 acc_train: 0.8946 loss_val: 0.3194 acc_val: 0.8992 time: 267.4724s\n",
            "Epoch: 1095 loss_train: 0.3167 acc_train: 0.9005 loss_val: 0.3216 acc_val: 0.9004 time: 267.7126s\n",
            "Epoch: 1096 loss_train: 0.3162 acc_train: 0.8984 loss_val: 0.3255 acc_val: 0.8996 time: 267.9579s\n",
            "Epoch: 1097 loss_train: 0.3151 acc_train: 0.8986 loss_val: 0.3224 acc_val: 0.9008 time: 268.2151s\n",
            "Epoch: 1098 loss_train: 0.3123 acc_train: 0.9036 loss_val: 0.3237 acc_val: 0.8996 time: 268.4623s\n",
            "Epoch: 1099 loss_train: 0.3160 acc_train: 0.8976 loss_val: 0.3397 acc_val: 0.8925 time: 268.7083s\n",
            "Epoch: 1100 loss_train: 0.3246 acc_train: 0.8937 loss_val: 0.3234 acc_val: 0.9029 time: 268.9648s\n",
            "Epoch: 1101 loss_train: 0.3206 acc_train: 0.8986 loss_val: 0.3194 acc_val: 0.8992 time: 269.2152s\n",
            "Epoch: 1102 loss_train: 0.3132 acc_train: 0.9029 loss_val: 0.3488 acc_val: 0.8904 time: 269.4649s\n",
            "Epoch: 1103 loss_train: 0.3286 acc_train: 0.8896 loss_val: 0.3402 acc_val: 0.8942 time: 269.7185s\n",
            "Epoch: 1104 loss_train: 0.3452 acc_train: 0.8905 loss_val: 0.3349 acc_val: 0.8929 time: 269.9644s\n",
            "Epoch: 1105 loss_train: 0.3219 acc_train: 0.8937 loss_val: 0.3235 acc_val: 0.8992 time: 270.1899s\n",
            "Epoch: 1106 loss_train: 0.3186 acc_train: 0.8949 loss_val: 0.3297 acc_val: 0.9000 time: 270.4393s\n",
            "Epoch: 1107 loss_train: 0.3219 acc_train: 0.9020 loss_val: 0.3290 acc_val: 0.8979 time: 270.6887s\n",
            "Epoch: 1108 loss_train: 0.3236 acc_train: 0.8957 loss_val: 0.3355 acc_val: 0.8950 time: 270.9327s\n",
            "Epoch: 1109 loss_train: 0.3209 acc_train: 0.8944 loss_val: 0.3259 acc_val: 0.9025 time: 271.1852s\n",
            "Epoch: 1110 loss_train: 0.3334 acc_train: 0.8980 loss_val: 0.3312 acc_val: 0.8975 time: 271.4253s\n",
            "Epoch: 1111 loss_train: 0.3193 acc_train: 0.8982 loss_val: 0.3291 acc_val: 0.8988 time: 271.6770s\n",
            "Epoch: 1112 loss_train: 0.3207 acc_train: 0.8948 loss_val: 0.3348 acc_val: 0.9012 time: 271.9217s\n",
            "Epoch: 1113 loss_train: 0.3418 acc_train: 0.8982 loss_val: 0.3314 acc_val: 0.8975 time: 272.1705s\n",
            "Epoch: 1114 loss_train: 0.3142 acc_train: 0.8970 loss_val: 0.3389 acc_val: 0.8933 time: 272.4229s\n",
            "Epoch: 1115 loss_train: 0.3247 acc_train: 0.8953 loss_val: 0.3311 acc_val: 0.9008 time: 272.6662s\n",
            "Epoch: 1116 loss_train: 0.3315 acc_train: 0.8992 loss_val: 0.3222 acc_val: 0.9021 time: 272.9126s\n",
            "Epoch: 1117 loss_train: 0.3148 acc_train: 0.9003 loss_val: 0.3470 acc_val: 0.8912 time: 273.1673s\n",
            "Epoch: 1118 loss_train: 0.3364 acc_train: 0.8891 loss_val: 0.3330 acc_val: 0.8988 time: 273.4347s\n",
            "Epoch: 1119 loss_train: 0.3283 acc_train: 0.8963 loss_val: 0.3265 acc_val: 0.8979 time: 273.6796s\n",
            "Epoch: 1120 loss_train: 0.3136 acc_train: 0.8970 loss_val: 0.3292 acc_val: 0.8971 time: 273.9223s\n",
            "Epoch: 1121 loss_train: 0.3196 acc_train: 0.8963 loss_val: 0.3225 acc_val: 0.8983 time: 274.1585s\n",
            "Epoch: 1122 loss_train: 0.3197 acc_train: 0.9007 loss_val: 0.3193 acc_val: 0.9033 time: 274.3989s\n",
            "Epoch: 1123 loss_train: 0.3099 acc_train: 0.9011 loss_val: 0.3245 acc_val: 0.8992 time: 274.6533s\n",
            "Epoch: 1124 loss_train: 0.3129 acc_train: 0.8970 loss_val: 0.3205 acc_val: 0.9021 time: 274.8948s\n",
            "Epoch: 1125 loss_train: 0.3067 acc_train: 0.9067 loss_val: 0.3200 acc_val: 0.9025 time: 275.1474s\n",
            "Epoch: 1126 loss_train: 0.3120 acc_train: 0.8963 loss_val: 0.3195 acc_val: 0.9021 time: 275.3565s\n",
            "Epoch: 1127 loss_train: 0.3115 acc_train: 0.9022 loss_val: 0.3182 acc_val: 0.9017 time: 275.6012s\n",
            "Epoch: 1128 loss_train: 0.3078 acc_train: 0.9015 loss_val: 0.3262 acc_val: 0.9008 time: 275.8445s\n",
            "Epoch: 1129 loss_train: 0.3197 acc_train: 0.8966 loss_val: 0.3178 acc_val: 0.9025 time: 276.0987s\n",
            "Epoch: 1130 loss_train: 0.3145 acc_train: 0.9010 loss_val: 0.3203 acc_val: 0.9012 time: 276.3454s\n",
            "Epoch: 1131 loss_train: 0.3176 acc_train: 0.8966 loss_val: 0.3196 acc_val: 0.9042 time: 276.5928s\n",
            "Epoch: 1132 loss_train: 0.3056 acc_train: 0.9010 loss_val: 0.3194 acc_val: 0.8996 time: 276.8318s\n",
            "Epoch: 1133 loss_train: 0.3091 acc_train: 0.9038 loss_val: 0.3185 acc_val: 0.9021 time: 277.0907s\n",
            "Epoch: 1134 loss_train: 0.3110 acc_train: 0.9025 loss_val: 0.3218 acc_val: 0.9017 time: 277.3312s\n",
            "Epoch: 1135 loss_train: 0.3061 acc_train: 0.9019 loss_val: 0.3199 acc_val: 0.9042 time: 277.5781s\n",
            "Epoch: 1136 loss_train: 0.3127 acc_train: 0.9024 loss_val: 0.3245 acc_val: 0.9017 time: 277.8229s\n",
            "Epoch: 1137 loss_train: 0.3236 acc_train: 0.9002 loss_val: 0.3330 acc_val: 0.8967 time: 278.0174s\n",
            "Epoch: 1138 loss_train: 0.3314 acc_train: 0.8908 loss_val: 0.3191 acc_val: 0.9008 time: 278.2548s\n",
            "Epoch: 1139 loss_train: 0.3050 acc_train: 0.9084 loss_val: 0.3228 acc_val: 0.9000 time: 278.5025s\n",
            "Epoch: 1140 loss_train: 0.3184 acc_train: 0.9004 loss_val: 0.3200 acc_val: 0.9033 time: 278.7689s\n",
            "Epoch: 1141 loss_train: 0.3239 acc_train: 0.8938 loss_val: 0.3184 acc_val: 0.9021 time: 279.0174s\n",
            "Epoch: 1142 loss_train: 0.3098 acc_train: 0.9016 loss_val: 0.3251 acc_val: 0.9029 time: 279.2733s\n",
            "Epoch: 1143 loss_train: 0.3111 acc_train: 0.9008 loss_val: 0.3227 acc_val: 0.9038 time: 279.5177s\n",
            "Epoch: 1144 loss_train: 0.3209 acc_train: 0.8975 loss_val: 0.3171 acc_val: 0.9038 time: 279.7662s\n",
            "Epoch: 1145 loss_train: 0.3081 acc_train: 0.9020 loss_val: 0.3278 acc_val: 0.8996 time: 280.0090s\n",
            "Epoch: 1146 loss_train: 0.3176 acc_train: 0.8946 loss_val: 0.3238 acc_val: 0.8983 time: 280.2426s\n",
            "Epoch: 1147 loss_train: 0.3173 acc_train: 0.9007 loss_val: 0.3237 acc_val: 0.9017 time: 280.4904s\n",
            "Epoch: 1148 loss_train: 0.3144 acc_train: 0.8986 loss_val: 0.3216 acc_val: 0.8996 time: 280.7328s\n",
            "Epoch: 1149 loss_train: 0.3124 acc_train: 0.9022 loss_val: 0.3197 acc_val: 0.9033 time: 280.9789s\n",
            "Epoch: 1150 loss_train: 0.3069 acc_train: 0.8995 loss_val: 0.3215 acc_val: 0.9004 time: 281.2280s\n",
            "Epoch: 1151 loss_train: 0.3136 acc_train: 0.8999 loss_val: 0.3183 acc_val: 0.9054 time: 281.4796s\n",
            "Epoch: 1152 loss_train: 0.3070 acc_train: 0.9026 loss_val: 0.3189 acc_val: 0.9025 time: 281.7253s\n",
            "Epoch: 1153 loss_train: 0.3049 acc_train: 0.9021 loss_val: 0.3189 acc_val: 0.9025 time: 281.9819s\n",
            "Epoch: 1154 loss_train: 0.3119 acc_train: 0.8979 loss_val: 0.3244 acc_val: 0.9000 time: 282.2398s\n",
            "Epoch: 1155 loss_train: 0.3102 acc_train: 0.9011 loss_val: 0.3262 acc_val: 0.8996 time: 282.4836s\n",
            "Epoch: 1156 loss_train: 0.3105 acc_train: 0.9005 loss_val: 0.3224 acc_val: 0.9025 time: 282.7262s\n",
            "Epoch: 1157 loss_train: 0.3126 acc_train: 0.9004 loss_val: 0.3213 acc_val: 0.9046 time: 282.9747s\n",
            "Epoch: 1158 loss_train: 0.3124 acc_train: 0.9035 loss_val: 0.3217 acc_val: 0.9033 time: 283.2249s\n",
            "Epoch: 1159 loss_train: 0.3123 acc_train: 0.8966 loss_val: 0.3215 acc_val: 0.9004 time: 283.4745s\n",
            "Epoch: 1160 loss_train: 0.3149 acc_train: 0.9000 loss_val: 0.3164 acc_val: 0.9038 time: 283.7211s\n",
            "Epoch: 1161 loss_train: 0.3051 acc_train: 0.9041 loss_val: 0.3227 acc_val: 0.9000 time: 283.9659s\n",
            "Epoch: 1162 loss_train: 0.3096 acc_train: 0.8999 loss_val: 0.3179 acc_val: 0.9033 time: 284.2191s\n",
            "Epoch: 1163 loss_train: 0.2971 acc_train: 0.9052 loss_val: 0.3207 acc_val: 0.8996 time: 284.4723s\n",
            "Epoch: 1164 loss_train: 0.3128 acc_train: 0.9016 loss_val: 0.3260 acc_val: 0.9004 time: 284.6963s\n",
            "Epoch: 1165 loss_train: 0.3122 acc_train: 0.8973 loss_val: 0.3195 acc_val: 0.9042 time: 284.8886s\n",
            "Epoch: 1166 loss_train: 0.3122 acc_train: 0.8997 loss_val: 0.3195 acc_val: 0.9000 time: 285.1403s\n",
            "Epoch: 1167 loss_train: 0.3098 acc_train: 0.9055 loss_val: 0.3240 acc_val: 0.8996 time: 285.3703s\n",
            "Epoch: 1168 loss_train: 0.3184 acc_train: 0.8962 loss_val: 0.3176 acc_val: 0.9042 time: 285.6123s\n",
            "Epoch: 1169 loss_train: 0.3027 acc_train: 0.9067 loss_val: 0.3177 acc_val: 0.9054 time: 285.8600s\n",
            "Epoch: 1170 loss_train: 0.3184 acc_train: 0.9011 loss_val: 0.3290 acc_val: 0.8988 time: 286.1060s\n",
            "Epoch: 1171 loss_train: 0.3124 acc_train: 0.8971 loss_val: 0.3208 acc_val: 0.8992 time: 286.3546s\n",
            "Epoch: 1172 loss_train: 0.3082 acc_train: 0.9022 loss_val: 0.3172 acc_val: 0.9004 time: 286.6001s\n",
            "Epoch: 1173 loss_train: 0.3115 acc_train: 0.9003 loss_val: 0.3255 acc_val: 0.8996 time: 286.8512s\n",
            "Epoch: 1174 loss_train: 0.3156 acc_train: 0.8981 loss_val: 0.3208 acc_val: 0.9029 time: 287.0973s\n",
            "Epoch: 1175 loss_train: 0.3110 acc_train: 0.9053 loss_val: 0.3246 acc_val: 0.9008 time: 287.3447s\n",
            "Epoch: 1176 loss_train: 0.3108 acc_train: 0.9018 loss_val: 0.3249 acc_val: 0.9012 time: 287.5908s\n",
            "Epoch: 1177 loss_train: 0.3082 acc_train: 0.9004 loss_val: 0.3191 acc_val: 0.9021 time: 287.8351s\n",
            "Epoch: 1178 loss_train: 0.3031 acc_train: 0.9015 loss_val: 0.3177 acc_val: 0.9025 time: 288.0721s\n",
            "Epoch: 1179 loss_train: 0.3062 acc_train: 0.9003 loss_val: 0.3284 acc_val: 0.8958 time: 288.2869s\n",
            "Epoch: 1180 loss_train: 0.3060 acc_train: 0.9020 loss_val: 0.3188 acc_val: 0.9025 time: 288.5300s\n",
            "Epoch: 1181 loss_train: 0.3025 acc_train: 0.9029 loss_val: 0.3201 acc_val: 0.9008 time: 288.7543s\n",
            "Epoch: 1182 loss_train: 0.3135 acc_train: 0.8986 loss_val: 0.3246 acc_val: 0.9012 time: 288.9802s\n",
            "Epoch: 1183 loss_train: 0.3201 acc_train: 0.8975 loss_val: 0.3217 acc_val: 0.8988 time: 289.2531s\n",
            "Epoch: 1184 loss_train: 0.3138 acc_train: 0.9015 loss_val: 0.3298 acc_val: 0.9004 time: 289.4961s\n",
            "Epoch: 1185 loss_train: 0.3220 acc_train: 0.8953 loss_val: 0.3209 acc_val: 0.9017 time: 289.6967s\n",
            "Epoch: 1186 loss_train: 0.3075 acc_train: 0.9004 loss_val: 0.3250 acc_val: 0.8988 time: 289.9368s\n",
            "Epoch: 1187 loss_train: 0.3102 acc_train: 0.9040 loss_val: 0.3312 acc_val: 0.8979 time: 290.1841s\n",
            "Epoch: 1188 loss_train: 0.3145 acc_train: 0.8980 loss_val: 0.3234 acc_val: 0.9012 time: 290.4423s\n",
            "Epoch: 1189 loss_train: 0.3195 acc_train: 0.8976 loss_val: 0.3210 acc_val: 0.8983 time: 290.6915s\n",
            "Epoch: 1190 loss_train: 0.3094 acc_train: 0.9040 loss_val: 0.3238 acc_val: 0.9000 time: 290.9155s\n",
            "Epoch: 1191 loss_train: 0.3189 acc_train: 0.8976 loss_val: 0.3275 acc_val: 0.9017 time: 291.1631s\n",
            "Epoch: 1192 loss_train: 0.3251 acc_train: 0.8971 loss_val: 0.3267 acc_val: 0.8996 time: 291.4117s\n",
            "Epoch: 1193 loss_train: 0.3178 acc_train: 0.8986 loss_val: 0.3429 acc_val: 0.8942 time: 291.6145s\n",
            "Epoch: 1194 loss_train: 0.3285 acc_train: 0.8914 loss_val: 0.3211 acc_val: 0.9038 time: 291.8555s\n",
            "Epoch: 1195 loss_train: 0.3156 acc_train: 0.8991 loss_val: 0.3206 acc_val: 0.9046 time: 292.1015s\n",
            "Epoch: 1196 loss_train: 0.3065 acc_train: 0.9065 loss_val: 0.3396 acc_val: 0.8950 time: 292.3531s\n",
            "Epoch: 1197 loss_train: 0.3158 acc_train: 0.8968 loss_val: 0.3189 acc_val: 0.9008 time: 292.6135s\n",
            "Epoch: 1198 loss_train: 0.3035 acc_train: 0.9024 loss_val: 0.3225 acc_val: 0.9021 time: 292.8636s\n",
            "Epoch: 1199 loss_train: 0.3104 acc_train: 0.9032 loss_val: 0.3323 acc_val: 0.8954 time: 293.1064s\n",
            "Epoch: 1200 loss_train: 0.3229 acc_train: 0.8931 loss_val: 0.3278 acc_val: 0.8992 time: 293.3579s\n",
            "Epoch: 1201 loss_train: 0.3223 acc_train: 0.9005 loss_val: 0.3318 acc_val: 0.8975 time: 293.6019s\n",
            "Epoch: 1202 loss_train: 0.3149 acc_train: 0.8973 loss_val: 0.3247 acc_val: 0.9008 time: 293.8441s\n",
            "Epoch: 1203 loss_train: 0.3100 acc_train: 0.8981 loss_val: 0.3273 acc_val: 0.9058 time: 294.0914s\n",
            "Epoch: 1204 loss_train: 0.3297 acc_train: 0.8975 loss_val: 0.3933 acc_val: 0.8800 time: 294.3462s\n",
            "Epoch: 1205 loss_train: 0.3558 acc_train: 0.8840 loss_val: 0.3331 acc_val: 0.8979 time: 294.5920s\n",
            "Epoch: 1206 loss_train: 0.3392 acc_train: 0.8933 loss_val: 0.3407 acc_val: 0.9029 time: 294.8385s\n",
            "Epoch: 1207 loss_train: 0.3460 acc_train: 0.8954 loss_val: 0.3565 acc_val: 0.8871 time: 295.0947s\n",
            "Epoch: 1208 loss_train: 0.3399 acc_train: 0.8853 loss_val: 0.3482 acc_val: 0.8967 time: 295.3528s\n",
            "Epoch: 1209 loss_train: 0.3443 acc_train: 0.8934 loss_val: 0.3332 acc_val: 0.9038 time: 295.6099s\n",
            "Epoch: 1210 loss_train: 0.3387 acc_train: 0.8978 loss_val: 0.4233 acc_val: 0.8633 time: 295.8527s\n",
            "Epoch: 1211 loss_train: 0.4077 acc_train: 0.8654 loss_val: 0.3802 acc_val: 0.8892 time: 296.0988s\n",
            "Epoch: 1212 loss_train: 0.3938 acc_train: 0.8832 loss_val: 0.3453 acc_val: 0.8933 time: 296.3517s\n",
            "Epoch: 1213 loss_train: 0.3399 acc_train: 0.8932 loss_val: 0.3639 acc_val: 0.8817 time: 296.6005s\n",
            "Epoch: 1214 loss_train: 0.3668 acc_train: 0.8732 loss_val: 0.3495 acc_val: 0.8921 time: 296.8411s\n",
            "Epoch: 1215 loss_train: 0.3580 acc_train: 0.8878 loss_val: 0.3691 acc_val: 0.8862 time: 297.0983s\n",
            "Epoch: 1216 loss_train: 0.3637 acc_train: 0.8869 loss_val: 0.3472 acc_val: 0.8896 time: 297.3428s\n",
            "Epoch: 1217 loss_train: 0.3310 acc_train: 0.8926 loss_val: 0.3460 acc_val: 0.8988 time: 297.5944s\n",
            "Epoch: 1218 loss_train: 0.3514 acc_train: 0.8967 loss_val: 0.3780 acc_val: 0.8821 time: 297.8173s\n",
            "Epoch: 1219 loss_train: 0.3656 acc_train: 0.8832 loss_val: 0.3400 acc_val: 0.8967 time: 298.0610s\n",
            "Epoch: 1220 loss_train: 0.3163 acc_train: 0.8971 loss_val: 0.3463 acc_val: 0.8954 time: 298.3024s\n",
            "Epoch: 1221 loss_train: 0.3560 acc_train: 0.8912 loss_val: 0.3289 acc_val: 0.8979 time: 298.5208s\n",
            "Epoch: 1222 loss_train: 0.3152 acc_train: 0.9000 loss_val: 0.3470 acc_val: 0.8917 time: 298.7607s\n",
            "Epoch: 1223 loss_train: 0.3292 acc_train: 0.8963 loss_val: 0.3299 acc_val: 0.8962 time: 299.0121s\n",
            "Epoch: 1224 loss_train: 0.3152 acc_train: 0.9016 loss_val: 0.3355 acc_val: 0.8954 time: 299.2590s\n",
            "Epoch: 1225 loss_train: 0.3354 acc_train: 0.8919 loss_val: 0.3456 acc_val: 0.8925 time: 299.5115s\n",
            "Epoch: 1226 loss_train: 0.3341 acc_train: 0.8885 loss_val: 0.3361 acc_val: 0.8958 time: 299.7612s\n",
            "Epoch: 1227 loss_train: 0.3321 acc_train: 0.8914 loss_val: 0.3272 acc_val: 0.8988 time: 300.0115s\n",
            "Epoch: 1228 loss_train: 0.3174 acc_train: 0.9022 loss_val: 0.3345 acc_val: 0.8925 time: 300.2595s\n",
            "Epoch: 1229 loss_train: 0.3243 acc_train: 0.8929 loss_val: 0.3314 acc_val: 0.8958 time: 300.5238s\n",
            "Epoch: 1230 loss_train: 0.3088 acc_train: 0.8991 loss_val: 0.3250 acc_val: 0.8979 time: 300.7788s\n",
            "Epoch: 1231 loss_train: 0.3218 acc_train: 0.8953 loss_val: 0.3248 acc_val: 0.9038 time: 301.0200s\n",
            "Epoch: 1232 loss_train: 0.3024 acc_train: 0.9052 loss_val: 0.3377 acc_val: 0.8975 time: 301.2717s\n",
            "Epoch: 1233 loss_train: 0.3267 acc_train: 0.8966 loss_val: 0.3231 acc_val: 0.9017 time: 301.5208s\n",
            "Epoch: 1234 loss_train: 0.3060 acc_train: 0.9013 loss_val: 0.3275 acc_val: 0.8958 time: 301.7755s\n",
            "Epoch: 1235 loss_train: 0.3165 acc_train: 0.9000 loss_val: 0.3246 acc_val: 0.9004 time: 302.0278s\n",
            "Epoch: 1236 loss_train: 0.3142 acc_train: 0.8976 loss_val: 0.3226 acc_val: 0.9029 time: 302.2742s\n",
            "Epoch: 1237 loss_train: 0.3105 acc_train: 0.9016 loss_val: 0.3252 acc_val: 0.9025 time: 302.5264s\n",
            "Epoch: 1238 loss_train: 0.3222 acc_train: 0.9016 loss_val: 0.3214 acc_val: 0.9046 time: 302.7735s\n",
            "Epoch: 1239 loss_train: 0.3087 acc_train: 0.9011 loss_val: 0.3211 acc_val: 0.9004 time: 303.0185s\n",
            "Epoch: 1240 loss_train: 0.3003 acc_train: 0.9019 loss_val: 0.3257 acc_val: 0.8992 time: 303.2042s\n",
            "Epoch: 1241 loss_train: 0.3177 acc_train: 0.8937 loss_val: 0.3213 acc_val: 0.9025 time: 303.4074s\n",
            "Epoch: 1242 loss_train: 0.3081 acc_train: 0.9046 loss_val: 0.3283 acc_val: 0.9012 time: 303.6541s\n",
            "Epoch: 1243 loss_train: 0.3154 acc_train: 0.8987 loss_val: 0.3265 acc_val: 0.9004 time: 303.9092s\n",
            "Epoch: 1244 loss_train: 0.3038 acc_train: 0.9070 loss_val: 0.3232 acc_val: 0.9033 time: 304.1579s\n",
            "Epoch: 1245 loss_train: 0.3142 acc_train: 0.9032 loss_val: 0.3283 acc_val: 0.9000 time: 304.4025s\n",
            "Epoch: 1246 loss_train: 0.3140 acc_train: 0.8969 loss_val: 0.3209 acc_val: 0.9021 time: 304.6494s\n",
            "Epoch: 1247 loss_train: 0.3065 acc_train: 0.9033 loss_val: 0.3234 acc_val: 0.9004 time: 304.8949s\n",
            "Epoch: 1248 loss_train: 0.3071 acc_train: 0.9014 loss_val: 0.3224 acc_val: 0.9012 time: 305.1495s\n",
            "Epoch: 1249 loss_train: 0.3157 acc_train: 0.8966 loss_val: 0.3228 acc_val: 0.9025 time: 305.3911s\n",
            "Epoch: 1250 loss_train: 0.3089 acc_train: 0.9009 loss_val: 0.3235 acc_val: 0.9025 time: 305.6447s\n",
            "Epoch: 1251 loss_train: 0.3123 acc_train: 0.8984 loss_val: 0.3218 acc_val: 0.9029 time: 305.8923s\n",
            "Epoch: 1252 loss_train: 0.3118 acc_train: 0.9027 loss_val: 0.3195 acc_val: 0.9033 time: 306.1395s\n",
            "Epoch: 1253 loss_train: 0.3099 acc_train: 0.9040 loss_val: 0.3255 acc_val: 0.9033 time: 306.3800s\n",
            "Epoch: 1254 loss_train: 0.3111 acc_train: 0.8984 loss_val: 0.3214 acc_val: 0.9025 time: 306.6334s\n",
            "Epoch: 1255 loss_train: 0.3115 acc_train: 0.9057 loss_val: 0.3212 acc_val: 0.9021 time: 306.8745s\n",
            "Epoch: 1256 loss_train: 0.3030 acc_train: 0.9013 loss_val: 0.3341 acc_val: 0.8992 time: 307.1201s\n",
            "Epoch: 1257 loss_train: 0.3223 acc_train: 0.8948 loss_val: 0.3316 acc_val: 0.9021 time: 307.3658s\n",
            "Epoch: 1258 loss_train: 0.3231 acc_train: 0.8981 loss_val: 0.3259 acc_val: 0.9012 time: 307.6200s\n",
            "Epoch: 1259 loss_train: 0.3083 acc_train: 0.8978 loss_val: 0.3356 acc_val: 0.8967 time: 307.8182s\n",
            "Epoch: 1260 loss_train: 0.3207 acc_train: 0.8953 loss_val: 0.3324 acc_val: 0.9008 time: 308.0508s\n",
            "Epoch: 1261 loss_train: 0.3226 acc_train: 0.9026 loss_val: 0.3187 acc_val: 0.9025 time: 308.2251s\n",
            "Epoch: 1262 loss_train: 0.3078 acc_train: 0.9042 loss_val: 0.3491 acc_val: 0.8896 time: 308.4750s\n",
            "Epoch: 1263 loss_train: 0.3230 acc_train: 0.8916 loss_val: 0.3217 acc_val: 0.9000 time: 308.7062s\n",
            "Epoch: 1264 loss_train: 0.3130 acc_train: 0.9054 loss_val: 0.3210 acc_val: 0.9012 time: 308.9548s\n",
            "Epoch: 1265 loss_train: 0.3057 acc_train: 0.9041 loss_val: 0.3345 acc_val: 0.8975 time: 309.1790s\n",
            "Epoch: 1266 loss_train: 0.3222 acc_train: 0.8911 loss_val: 0.3232 acc_val: 0.9038 time: 309.4273s\n",
            "Epoch: 1267 loss_train: 0.3184 acc_train: 0.9042 loss_val: 0.3311 acc_val: 0.8975 time: 309.6747s\n",
            "Epoch: 1268 loss_train: 0.3211 acc_train: 0.8964 loss_val: 0.3279 acc_val: 0.9004 time: 309.9315s\n",
            "Epoch: 1269 loss_train: 0.3060 acc_train: 0.9030 loss_val: 0.3214 acc_val: 0.9008 time: 310.1524s\n",
            "Epoch: 1270 loss_train: 0.3052 acc_train: 0.9034 loss_val: 0.3202 acc_val: 0.9021 time: 310.4029s\n",
            "Epoch: 1271 loss_train: 0.3033 acc_train: 0.9018 loss_val: 0.3200 acc_val: 0.9029 time: 310.6549s\n",
            "Epoch: 1272 loss_train: 0.3089 acc_train: 0.8999 loss_val: 0.3198 acc_val: 0.9021 time: 310.8944s\n",
            "Epoch: 1273 loss_train: 0.3084 acc_train: 0.9020 loss_val: 0.3188 acc_val: 0.9021 time: 311.1429s\n",
            "Epoch: 1274 loss_train: 0.3078 acc_train: 0.9026 loss_val: 0.3188 acc_val: 0.9004 time: 311.3787s\n",
            "Epoch: 1275 loss_train: 0.3047 acc_train: 0.9022 loss_val: 0.3198 acc_val: 0.8996 time: 311.6312s\n",
            "Epoch: 1276 loss_train: 0.3029 acc_train: 0.9038 loss_val: 0.3209 acc_val: 0.9046 time: 311.8747s\n",
            "Epoch: 1277 loss_train: 0.3059 acc_train: 0.9022 loss_val: 0.3206 acc_val: 0.9038 time: 312.1318s\n",
            "Epoch: 1278 loss_train: 0.3050 acc_train: 0.9025 loss_val: 0.3175 acc_val: 0.9025 time: 312.3770s\n",
            "Epoch: 1279 loss_train: 0.3141 acc_train: 0.8991 loss_val: 0.3184 acc_val: 0.9054 time: 312.6286s\n",
            "Epoch: 1280 loss_train: 0.3013 acc_train: 0.9056 loss_val: 0.3172 acc_val: 0.9050 time: 312.8050s\n",
            "Epoch: 1281 loss_train: 0.3022 acc_train: 0.9035 loss_val: 0.3185 acc_val: 0.9038 time: 313.0585s\n",
            "Epoch: 1282 loss_train: 0.3040 acc_train: 0.9001 loss_val: 0.3166 acc_val: 0.9050 time: 313.3003s\n",
            "Epoch: 1283 loss_train: 0.3028 acc_train: 0.9038 loss_val: 0.3226 acc_val: 0.9038 time: 313.5487s\n",
            "Epoch: 1284 loss_train: 0.3075 acc_train: 0.9013 loss_val: 0.3187 acc_val: 0.9029 time: 313.7984s\n",
            "Epoch: 1285 loss_train: 0.3011 acc_train: 0.9041 loss_val: 0.3170 acc_val: 0.9046 time: 314.0579s\n",
            "Epoch: 1286 loss_train: 0.2972 acc_train: 0.9055 loss_val: 0.3212 acc_val: 0.9029 time: 314.3073s\n",
            "Epoch: 1287 loss_train: 0.3066 acc_train: 0.9008 loss_val: 0.3188 acc_val: 0.9050 time: 314.5574s\n",
            "Epoch: 1288 loss_train: 0.3005 acc_train: 0.9057 loss_val: 0.3173 acc_val: 0.9038 time: 314.7838s\n",
            "Epoch: 1289 loss_train: 0.3047 acc_train: 0.9056 loss_val: 0.3236 acc_val: 0.9021 time: 315.0009s\n",
            "Epoch: 1290 loss_train: 0.3027 acc_train: 0.9027 loss_val: 0.3172 acc_val: 0.9054 time: 315.2482s\n",
            "Epoch: 1291 loss_train: 0.3027 acc_train: 0.9026 loss_val: 0.3226 acc_val: 0.8996 time: 315.5033s\n",
            "Epoch: 1292 loss_train: 0.3112 acc_train: 0.8989 loss_val: 0.3193 acc_val: 0.9017 time: 315.7537s\n",
            "Epoch: 1293 loss_train: 0.2990 acc_train: 0.9059 loss_val: 0.3255 acc_val: 0.9017 time: 315.9657s\n",
            "Epoch: 1294 loss_train: 0.3080 acc_train: 0.8997 loss_val: 0.3172 acc_val: 0.9033 time: 316.2133s\n",
            "Epoch: 1295 loss_train: 0.3010 acc_train: 0.9048 loss_val: 0.3181 acc_val: 0.9033 time: 316.4569s\n",
            "Epoch: 1296 loss_train: 0.3120 acc_train: 0.9020 loss_val: 0.3214 acc_val: 0.9025 time: 316.6752s\n",
            "Epoch: 1297 loss_train: 0.3043 acc_train: 0.9034 loss_val: 0.3169 acc_val: 0.9050 time: 316.8868s\n",
            "Epoch: 1298 loss_train: 0.3013 acc_train: 0.9042 loss_val: 0.3294 acc_val: 0.9000 time: 317.1403s\n",
            "Epoch: 1299 loss_train: 0.3144 acc_train: 0.8962 loss_val: 0.3201 acc_val: 0.9021 time: 317.3843s\n",
            "Epoch: 1300 loss_train: 0.3005 acc_train: 0.9071 loss_val: 0.3239 acc_val: 0.9017 time: 317.6309s\n",
            "Epoch: 1301 loss_train: 0.3060 acc_train: 0.8990 loss_val: 0.3194 acc_val: 0.9050 time: 317.8748s\n",
            "Epoch: 1302 loss_train: 0.2989 acc_train: 0.9045 loss_val: 0.3171 acc_val: 0.9025 time: 318.0565s\n",
            "Epoch: 1303 loss_train: 0.3033 acc_train: 0.9052 loss_val: 0.3205 acc_val: 0.9025 time: 318.3027s\n",
            "Epoch: 1304 loss_train: 0.3033 acc_train: 0.9009 loss_val: 0.3171 acc_val: 0.9029 time: 318.5368s\n",
            "Epoch: 1305 loss_train: 0.3009 acc_train: 0.9044 loss_val: 0.3177 acc_val: 0.9033 time: 318.7616s\n",
            "Epoch: 1306 loss_train: 0.2910 acc_train: 0.9082 loss_val: 0.3197 acc_val: 0.9042 time: 318.9938s\n",
            "Epoch: 1307 loss_train: 0.2984 acc_train: 0.9045 loss_val: 0.3188 acc_val: 0.9038 time: 319.2322s\n",
            "Epoch: 1308 loss_train: 0.3114 acc_train: 0.9034 loss_val: 0.3265 acc_val: 0.9008 time: 319.4066s\n",
            "Epoch: 1309 loss_train: 0.2997 acc_train: 0.9021 loss_val: 0.3188 acc_val: 0.9017 time: 319.6573s\n",
            "Epoch: 1310 loss_train: 0.3022 acc_train: 0.9047 loss_val: 0.3176 acc_val: 0.9038 time: 319.8877s\n",
            "Epoch: 1311 loss_train: 0.3010 acc_train: 0.9079 loss_val: 0.3214 acc_val: 0.9029 time: 320.1353s\n",
            "Epoch: 1312 loss_train: 0.3054 acc_train: 0.9004 loss_val: 0.3198 acc_val: 0.9042 time: 320.3775s\n",
            "Epoch: 1313 loss_train: 0.3082 acc_train: 0.9025 loss_val: 0.3228 acc_val: 0.9025 time: 320.6299s\n",
            "Epoch: 1314 loss_train: 0.3004 acc_train: 0.9026 loss_val: 0.3228 acc_val: 0.9004 time: 320.8787s\n",
            "Epoch: 1315 loss_train: 0.3009 acc_train: 0.9045 loss_val: 0.3225 acc_val: 0.9058 time: 321.1313s\n",
            "Epoch: 1316 loss_train: 0.3108 acc_train: 0.9042 loss_val: 0.3395 acc_val: 0.8958 time: 321.3770s\n",
            "Epoch: 1317 loss_train: 0.3113 acc_train: 0.8975 loss_val: 0.3230 acc_val: 0.9000 time: 321.6261s\n",
            "Epoch: 1318 loss_train: 0.3055 acc_train: 0.8988 loss_val: 0.3181 acc_val: 0.9033 time: 321.8793s\n",
            "Epoch: 1319 loss_train: 0.3006 acc_train: 0.9057 loss_val: 0.3469 acc_val: 0.8908 time: 322.0776s\n",
            "Epoch: 1320 loss_train: 0.3251 acc_train: 0.8885 loss_val: 0.3342 acc_val: 0.9033 time: 322.3235s\n",
            "Epoch: 1321 loss_train: 0.3228 acc_train: 0.9053 loss_val: 0.3439 acc_val: 0.8946 time: 322.5682s\n",
            "Epoch: 1322 loss_train: 0.3176 acc_train: 0.8952 loss_val: 0.3372 acc_val: 0.8979 time: 322.8219s\n",
            "Epoch: 1323 loss_train: 0.3229 acc_train: 0.8925 loss_val: 0.3729 acc_val: 0.8958 time: 323.0599s\n",
            "Epoch: 1324 loss_train: 0.3831 acc_train: 0.8875 loss_val: 0.5034 acc_val: 0.8483 time: 323.3125s\n",
            "Epoch: 1325 loss_train: 0.4801 acc_train: 0.8435 loss_val: 0.4172 acc_val: 0.8854 time: 323.5612s\n",
            "Epoch: 1326 loss_train: 0.4298 acc_train: 0.8788 loss_val: 0.3508 acc_val: 0.8954 time: 323.8104s\n",
            "Epoch: 1327 loss_train: 0.3515 acc_train: 0.8865 loss_val: 0.3482 acc_val: 0.8921 time: 324.0565s\n",
            "Epoch: 1328 loss_train: 0.3323 acc_train: 0.8896 loss_val: 0.3492 acc_val: 0.8896 time: 324.3025s\n",
            "Epoch: 1329 loss_train: 0.3436 acc_train: 0.8867 loss_val: 0.3435 acc_val: 0.8958 time: 324.5524s\n",
            "Epoch: 1330 loss_train: 0.3399 acc_train: 0.8910 loss_val: 0.3598 acc_val: 0.8862 time: 324.8078s\n",
            "Epoch: 1331 loss_train: 0.3479 acc_train: 0.8848 loss_val: 0.3472 acc_val: 0.8992 time: 325.0540s\n",
            "Epoch: 1332 loss_train: 0.3430 acc_train: 0.8980 loss_val: 0.3421 acc_val: 0.8962 time: 325.3049s\n",
            "Epoch: 1333 loss_train: 0.3209 acc_train: 0.8987 loss_val: 0.3553 acc_val: 0.8908 time: 325.5496s\n",
            "Epoch: 1334 loss_train: 0.3359 acc_train: 0.8873 loss_val: 0.3325 acc_val: 0.8979 time: 325.8019s\n",
            "Epoch: 1335 loss_train: 0.3241 acc_train: 0.8955 loss_val: 0.3315 acc_val: 0.8996 time: 326.0230s\n",
            "Epoch: 1336 loss_train: 0.3332 acc_train: 0.8970 loss_val: 0.3439 acc_val: 0.8983 time: 326.2588s\n",
            "Epoch: 1337 loss_train: 0.3245 acc_train: 0.9009 loss_val: 0.3402 acc_val: 0.8983 time: 326.5054s\n",
            "Epoch: 1338 loss_train: 0.3186 acc_train: 0.8998 loss_val: 0.3324 acc_val: 0.9025 time: 326.7550s\n",
            "Epoch: 1339 loss_train: 0.3220 acc_train: 0.8990 loss_val: 0.3291 acc_val: 0.9004 time: 327.0013s\n",
            "Epoch: 1340 loss_train: 0.3168 acc_train: 0.8964 loss_val: 0.3365 acc_val: 0.8975 time: 327.2505s\n",
            "Epoch: 1341 loss_train: 0.3116 acc_train: 0.9029 loss_val: 0.3202 acc_val: 0.9025 time: 327.5071s\n",
            "Epoch: 1342 loss_train: 0.3056 acc_train: 0.9024 loss_val: 0.3239 acc_val: 0.9008 time: 327.7097s\n",
            "Epoch: 1343 loss_train: 0.3138 acc_train: 0.9038 loss_val: 0.3295 acc_val: 0.8992 time: 327.9497s\n",
            "Epoch: 1344 loss_train: 0.3077 acc_train: 0.9013 loss_val: 0.3315 acc_val: 0.8992 time: 328.1961s\n",
            "Epoch: 1345 loss_train: 0.3019 acc_train: 0.9051 loss_val: 0.3217 acc_val: 0.8996 time: 328.4406s\n",
            "Epoch: 1346 loss_train: 0.3064 acc_train: 0.9043 loss_val: 0.3214 acc_val: 0.9025 time: 328.6890s\n",
            "Epoch: 1347 loss_train: 0.3135 acc_train: 0.8974 loss_val: 0.3378 acc_val: 0.8967 time: 328.9401s\n",
            "Epoch: 1348 loss_train: 0.3033 acc_train: 0.8989 loss_val: 0.3258 acc_val: 0.8992 time: 329.1605s\n",
            "Epoch: 1349 loss_train: 0.3077 acc_train: 0.9014 loss_val: 0.3216 acc_val: 0.9025 time: 329.4039s\n",
            "Epoch: 1350 loss_train: 0.3047 acc_train: 0.9038 loss_val: 0.3316 acc_val: 0.8975 time: 329.6378s\n",
            "Epoch: 1351 loss_train: 0.3123 acc_train: 0.8978 loss_val: 0.3175 acc_val: 0.9025 time: 329.8889s\n",
            "Epoch: 1352 loss_train: 0.3009 acc_train: 0.9021 loss_val: 0.3253 acc_val: 0.9000 time: 330.1353s\n",
            "Epoch: 1353 loss_train: 0.3156 acc_train: 0.9015 loss_val: 0.3322 acc_val: 0.9008 time: 330.3653s\n",
            "Epoch: 1354 loss_train: 0.3146 acc_train: 0.8974 loss_val: 0.3174 acc_val: 0.9042 time: 330.6152s\n",
            "Epoch: 1355 loss_train: 0.3016 acc_train: 0.9054 loss_val: 0.3177 acc_val: 0.9025 time: 330.8302s\n",
            "Epoch: 1356 loss_train: 0.3031 acc_train: 0.9022 loss_val: 0.3266 acc_val: 0.9004 time: 331.0304s\n",
            "Epoch: 1357 loss_train: 0.3054 acc_train: 0.9001 loss_val: 0.3194 acc_val: 0.9029 time: 331.2729s\n",
            "Epoch: 1358 loss_train: 0.2989 acc_train: 0.9055 loss_val: 0.3203 acc_val: 0.9012 time: 331.5154s\n",
            "Epoch: 1359 loss_train: 0.3176 acc_train: 0.9036 loss_val: 0.3302 acc_val: 0.8996 time: 331.7881s\n",
            "Epoch: 1360 loss_train: 0.3066 acc_train: 0.8985 loss_val: 0.3175 acc_val: 0.9033 time: 332.0046s\n",
            "Epoch: 1361 loss_train: 0.3036 acc_train: 0.9008 loss_val: 0.3260 acc_val: 0.9058 time: 332.2549s\n",
            "Epoch: 1362 loss_train: 0.3180 acc_train: 0.8993 loss_val: 0.3322 acc_val: 0.8979 time: 332.5089s\n",
            "Epoch: 1363 loss_train: 0.3125 acc_train: 0.8992 loss_val: 0.3207 acc_val: 0.9029 time: 332.7524s\n",
            "Epoch: 1364 loss_train: 0.3072 acc_train: 0.9041 loss_val: 0.3310 acc_val: 0.9058 time: 333.0058s\n",
            "Epoch: 1365 loss_train: 0.3225 acc_train: 0.9049 loss_val: 0.3680 acc_val: 0.8862 time: 333.2218s\n",
            "Epoch: 1366 loss_train: 0.3402 acc_train: 0.8874 loss_val: 0.3293 acc_val: 0.8996 time: 333.4773s\n",
            "Epoch: 1367 loss_train: 0.3256 acc_train: 0.8964 loss_val: 0.3317 acc_val: 0.9004 time: 333.7249s\n",
            "Epoch: 1368 loss_train: 0.3228 acc_train: 0.9019 loss_val: 0.3526 acc_val: 0.8904 time: 333.9752s\n",
            "Epoch: 1369 loss_train: 0.3388 acc_train: 0.8890 loss_val: 0.3239 acc_val: 0.8988 time: 334.2201s\n",
            "Epoch: 1370 loss_train: 0.3126 acc_train: 0.8975 loss_val: 0.3284 acc_val: 0.8967 time: 334.4634s\n",
            "Epoch: 1371 loss_train: 0.3188 acc_train: 0.8974 loss_val: 0.3461 acc_val: 0.8904 time: 334.7090s\n",
            "Epoch: 1372 loss_train: 0.3219 acc_train: 0.8922 loss_val: 0.3254 acc_val: 0.9000 time: 334.9588s\n",
            "Epoch: 1373 loss_train: 0.3082 acc_train: 0.9035 loss_val: 0.3320 acc_val: 0.8996 time: 335.2149s\n",
            "Epoch: 1374 loss_train: 0.3206 acc_train: 0.9015 loss_val: 0.3229 acc_val: 0.9008 time: 335.4612s\n",
            "Epoch: 1375 loss_train: 0.3002 acc_train: 0.9020 loss_val: 0.3269 acc_val: 0.9017 time: 335.7090s\n",
            "Epoch: 1376 loss_train: 0.3223 acc_train: 0.8941 loss_val: 0.3318 acc_val: 0.9033 time: 335.9628s\n",
            "Epoch: 1377 loss_train: 0.3172 acc_train: 0.9005 loss_val: 0.3359 acc_val: 0.8950 time: 336.2236s\n",
            "Epoch: 1378 loss_train: 0.3126 acc_train: 0.8959 loss_val: 0.3323 acc_val: 0.8958 time: 336.4625s\n",
            "Epoch: 1379 loss_train: 0.3054 acc_train: 0.9004 loss_val: 0.3261 acc_val: 0.9042 time: 336.7066s\n",
            "Epoch: 1380 loss_train: 0.3099 acc_train: 0.9049 loss_val: 0.3208 acc_val: 0.9008 time: 336.9311s\n",
            "Epoch: 1381 loss_train: 0.3032 acc_train: 0.9047 loss_val: 0.3397 acc_val: 0.8962 time: 337.1857s\n",
            "Epoch: 1382 loss_train: 0.3198 acc_train: 0.8931 loss_val: 0.3212 acc_val: 0.9021 time: 337.4284s\n",
            "Epoch: 1383 loss_train: 0.3063 acc_train: 0.9058 loss_val: 0.3224 acc_val: 0.9000 time: 337.6784s\n",
            "Epoch: 1384 loss_train: 0.3114 acc_train: 0.9013 loss_val: 0.3325 acc_val: 0.8988 time: 337.9244s\n",
            "Epoch: 1385 loss_train: 0.3101 acc_train: 0.8977 loss_val: 0.3220 acc_val: 0.9025 time: 338.1751s\n",
            "Epoch: 1386 loss_train: 0.3028 acc_train: 0.9009 loss_val: 0.3249 acc_val: 0.8988 time: 338.3508s\n",
            "Epoch: 1387 loss_train: 0.3180 acc_train: 0.8969 loss_val: 0.3255 acc_val: 0.9017 time: 338.5837s\n",
            "Epoch: 1388 loss_train: 0.2958 acc_train: 0.9026 loss_val: 0.3277 acc_val: 0.9008 time: 338.8257s\n",
            "Epoch: 1389 loss_train: 0.3099 acc_train: 0.9003 loss_val: 0.3231 acc_val: 0.9025 time: 339.0781s\n",
            "Epoch: 1390 loss_train: 0.3049 acc_train: 0.9086 loss_val: 0.3253 acc_val: 0.9029 time: 339.3258s\n",
            "Epoch: 1391 loss_train: 0.3008 acc_train: 0.9027 loss_val: 0.3206 acc_val: 0.9038 time: 339.5740s\n",
            "Epoch: 1392 loss_train: 0.3058 acc_train: 0.8998 loss_val: 0.3175 acc_val: 0.9046 time: 339.8125s\n",
            "Epoch: 1393 loss_train: 0.3015 acc_train: 0.9046 loss_val: 0.3221 acc_val: 0.9025 time: 340.0645s\n",
            "Epoch: 1394 loss_train: 0.3039 acc_train: 0.9038 loss_val: 0.3207 acc_val: 0.9021 time: 340.3052s\n",
            "Epoch: 1395 loss_train: 0.3036 acc_train: 0.9048 loss_val: 0.3218 acc_val: 0.9025 time: 340.5612s\n",
            "Epoch: 1396 loss_train: 0.2926 acc_train: 0.9096 loss_val: 0.3215 acc_val: 0.9008 time: 340.8057s\n",
            "Epoch: 1397 loss_train: 0.3084 acc_train: 0.9007 loss_val: 0.3182 acc_val: 0.9033 time: 341.0629s\n",
            "Epoch: 1398 loss_train: 0.3047 acc_train: 0.9024 loss_val: 0.3240 acc_val: 0.9000 time: 341.2867s\n",
            "Epoch: 1399 loss_train: 0.3053 acc_train: 0.9012 loss_val: 0.3176 acc_val: 0.9033 time: 341.5322s\n",
            "Epoch: 1400 loss_train: 0.2963 acc_train: 0.9044 loss_val: 0.3209 acc_val: 0.9004 time: 341.7454s\n",
            "Epoch: 1401 loss_train: 0.3167 acc_train: 0.9009 loss_val: 0.3199 acc_val: 0.9042 time: 341.9981s\n",
            "Epoch: 1402 loss_train: 0.3030 acc_train: 0.9027 loss_val: 0.3184 acc_val: 0.9025 time: 342.2624s\n",
            "Epoch: 1403 loss_train: 0.2962 acc_train: 0.9060 loss_val: 0.3204 acc_val: 0.9025 time: 342.5134s\n",
            "Epoch: 1404 loss_train: 0.3142 acc_train: 0.8991 loss_val: 0.3195 acc_val: 0.9054 time: 342.7566s\n",
            "Epoch: 1405 loss_train: 0.2990 acc_train: 0.9037 loss_val: 0.3231 acc_val: 0.9008 time: 343.0145s\n",
            "Epoch: 1406 loss_train: 0.3054 acc_train: 0.9037 loss_val: 0.3241 acc_val: 0.9004 time: 343.2661s\n",
            "Epoch: 1407 loss_train: 0.3047 acc_train: 0.9013 loss_val: 0.3161 acc_val: 0.9025 time: 343.5090s\n",
            "Epoch: 1408 loss_train: 0.3023 acc_train: 0.9046 loss_val: 0.3189 acc_val: 0.9017 time: 343.7516s\n",
            "Epoch: 1409 loss_train: 0.2999 acc_train: 0.9000 loss_val: 0.3181 acc_val: 0.9054 time: 344.0008s\n",
            "Epoch: 1410 loss_train: 0.3006 acc_train: 0.8999 loss_val: 0.3177 acc_val: 0.9033 time: 344.2465s\n",
            "Epoch: 1411 loss_train: 0.2991 acc_train: 0.9052 loss_val: 0.3170 acc_val: 0.9050 time: 344.5006s\n",
            "Epoch: 1412 loss_train: 0.2978 acc_train: 0.9058 loss_val: 0.3193 acc_val: 0.9042 time: 344.7487s\n",
            "Epoch: 1413 loss_train: 0.3003 acc_train: 0.9004 loss_val: 0.3164 acc_val: 0.9042 time: 345.0102s\n",
            "Epoch: 1414 loss_train: 0.3051 acc_train: 0.9012 loss_val: 0.3154 acc_val: 0.9042 time: 345.2577s\n",
            "Epoch: 1415 loss_train: 0.2966 acc_train: 0.9055 loss_val: 0.3241 acc_val: 0.9021 time: 345.5081s\n",
            "Epoch: 1416 loss_train: 0.3047 acc_train: 0.9019 loss_val: 0.3166 acc_val: 0.9021 time: 345.7570s\n",
            "Epoch: 1417 loss_train: 0.3012 acc_train: 0.9058 loss_val: 0.3179 acc_val: 0.9033 time: 346.0133s\n",
            "Epoch: 1418 loss_train: 0.2922 acc_train: 0.9054 loss_val: 0.3194 acc_val: 0.9050 time: 346.2616s\n",
            "Epoch: 1419 loss_train: 0.3013 acc_train: 0.9014 loss_val: 0.3231 acc_val: 0.9038 time: 346.5105s\n",
            "Epoch: 1420 loss_train: 0.3134 acc_train: 0.9041 loss_val: 0.3542 acc_val: 0.8892 time: 346.7463s\n",
            "Epoch: 1421 loss_train: 0.3147 acc_train: 0.8959 loss_val: 0.3222 acc_val: 0.9038 time: 346.9667s\n",
            "Epoch: 1422 loss_train: 0.3064 acc_train: 0.9012 loss_val: 0.3272 acc_val: 0.9083 time: 347.2136s\n",
            "Epoch: 1423 loss_train: 0.3216 acc_train: 0.9009 loss_val: 0.3751 acc_val: 0.8829 time: 347.4578s\n",
            "Epoch: 1424 loss_train: 0.3478 acc_train: 0.8823 loss_val: 0.3402 acc_val: 0.9008 time: 347.6875s\n",
            "Epoch: 1425 loss_train: 0.3293 acc_train: 0.8960 loss_val: 0.3302 acc_val: 0.9000 time: 347.8929s\n",
            "Epoch: 1426 loss_train: 0.3109 acc_train: 0.9021 loss_val: 0.3647 acc_val: 0.8854 time: 348.1550s\n",
            "Epoch: 1427 loss_train: 0.3418 acc_train: 0.8856 loss_val: 0.3321 acc_val: 0.9033 time: 348.3981s\n",
            "Epoch: 1428 loss_train: 0.3247 acc_train: 0.8992 loss_val: 0.3234 acc_val: 0.9038 time: 348.6433s\n",
            "Epoch: 1429 loss_train: 0.3129 acc_train: 0.8984 loss_val: 0.3524 acc_val: 0.8912 time: 348.8854s\n",
            "Epoch: 1430 loss_train: 0.3309 acc_train: 0.8929 loss_val: 0.3235 acc_val: 0.9017 time: 349.1404s\n",
            "Epoch: 1431 loss_train: 0.3087 acc_train: 0.9057 loss_val: 0.3273 acc_val: 0.9025 time: 349.3900s\n",
            "Epoch: 1432 loss_train: 0.3075 acc_train: 0.9043 loss_val: 0.3374 acc_val: 0.8954 time: 349.6353s\n",
            "Epoch: 1433 loss_train: 0.3119 acc_train: 0.8975 loss_val: 0.3317 acc_val: 0.9008 time: 349.8820s\n",
            "Epoch: 1434 loss_train: 0.3220 acc_train: 0.8976 loss_val: 0.3268 acc_val: 0.9029 time: 350.1287s\n",
            "Epoch: 1435 loss_train: 0.3022 acc_train: 0.9001 loss_val: 0.3458 acc_val: 0.8954 time: 350.3748s\n",
            "Epoch: 1436 loss_train: 0.3199 acc_train: 0.8999 loss_val: 0.3237 acc_val: 0.9042 time: 350.6299s\n",
            "Epoch: 1437 loss_train: 0.3162 acc_train: 0.9016 loss_val: 0.3311 acc_val: 0.8983 time: 350.8763s\n",
            "Epoch: 1438 loss_train: 0.3112 acc_train: 0.9019 loss_val: 0.3360 acc_val: 0.8954 time: 351.0931s\n",
            "Epoch: 1439 loss_train: 0.3064 acc_train: 0.8992 loss_val: 0.3287 acc_val: 0.9000 time: 351.3247s\n",
            "Epoch: 1440 loss_train: 0.3148 acc_train: 0.9023 loss_val: 0.3263 acc_val: 0.9042 time: 351.5709s\n",
            "Epoch: 1441 loss_train: 0.3080 acc_train: 0.9045 loss_val: 0.3577 acc_val: 0.8904 time: 351.8209s\n",
            "Epoch: 1442 loss_train: 0.3438 acc_train: 0.8831 loss_val: 0.3324 acc_val: 0.8954 time: 352.0797s\n",
            "Epoch: 1443 loss_train: 0.3294 acc_train: 0.8897 loss_val: 0.3206 acc_val: 0.9025 time: 352.3218s\n",
            "Epoch: 1444 loss_train: 0.3035 acc_train: 0.9057 loss_val: 0.3459 acc_val: 0.8900 time: 352.5505s\n",
            "Epoch: 1445 loss_train: 0.3301 acc_train: 0.8876 loss_val: 0.3229 acc_val: 0.9008 time: 352.8077s\n",
            "Epoch: 1446 loss_train: 0.3015 acc_train: 0.9044 loss_val: 0.3263 acc_val: 0.9008 time: 353.0618s\n",
            "Epoch: 1447 loss_train: 0.3104 acc_train: 0.9003 loss_val: 0.3318 acc_val: 0.9004 time: 353.3085s\n",
            "Epoch: 1448 loss_train: 0.3136 acc_train: 0.8978 loss_val: 0.3255 acc_val: 0.9033 time: 353.5603s\n",
            "Epoch: 1449 loss_train: 0.2995 acc_train: 0.9015 loss_val: 0.3207 acc_val: 0.9054 time: 353.8074s\n",
            "Epoch: 1450 loss_train: 0.2977 acc_train: 0.9086 loss_val: 0.3318 acc_val: 0.9004 time: 354.0637s\n",
            "Epoch: 1451 loss_train: 0.3063 acc_train: 0.9026 loss_val: 0.3291 acc_val: 0.8983 time: 354.3098s\n",
            "Epoch: 1452 loss_train: 0.3016 acc_train: 0.9029 loss_val: 0.3222 acc_val: 0.9038 time: 354.5555s\n",
            "Epoch: 1453 loss_train: 0.3121 acc_train: 0.9031 loss_val: 0.3185 acc_val: 0.9038 time: 354.7624s\n",
            "Epoch: 1454 loss_train: 0.2966 acc_train: 0.9019 loss_val: 0.3206 acc_val: 0.9042 time: 354.9495s\n",
            "Epoch: 1455 loss_train: 0.3021 acc_train: 0.9041 loss_val: 0.3189 acc_val: 0.9033 time: 355.2015s\n",
            "Epoch: 1456 loss_train: 0.2967 acc_train: 0.9103 loss_val: 0.3169 acc_val: 0.9029 time: 355.4060s\n",
            "Epoch: 1457 loss_train: 0.3048 acc_train: 0.9053 loss_val: 0.3259 acc_val: 0.9012 time: 355.6538s\n",
            "Epoch: 1458 loss_train: 0.2976 acc_train: 0.9024 loss_val: 0.3165 acc_val: 0.9046 time: 355.8988s\n",
            "Epoch: 1459 loss_train: 0.2943 acc_train: 0.9049 loss_val: 0.3202 acc_val: 0.9058 time: 356.1481s\n",
            "Epoch: 1460 loss_train: 0.3076 acc_train: 0.9046 loss_val: 0.3443 acc_val: 0.8958 time: 356.3874s\n",
            "Epoch: 1461 loss_train: 0.3143 acc_train: 0.8951 loss_val: 0.3271 acc_val: 0.9038 time: 356.6353s\n",
            "Epoch: 1462 loss_train: 0.3124 acc_train: 0.9049 loss_val: 0.3208 acc_val: 0.9008 time: 356.8847s\n",
            "Epoch: 1463 loss_train: 0.3120 acc_train: 0.9026 loss_val: 0.3366 acc_val: 0.8996 time: 357.1418s\n",
            "Epoch: 1464 loss_train: 0.3153 acc_train: 0.8976 loss_val: 0.3160 acc_val: 0.9038 time: 357.3896s\n",
            "Epoch: 1465 loss_train: 0.2974 acc_train: 0.9035 loss_val: 0.3259 acc_val: 0.9000 time: 357.6117s\n",
            "Epoch: 1466 loss_train: 0.3094 acc_train: 0.9030 loss_val: 0.3322 acc_val: 0.9008 time: 357.8511s\n",
            "Epoch: 1467 loss_train: 0.3072 acc_train: 0.9001 loss_val: 0.3167 acc_val: 0.9050 time: 358.1164s\n",
            "Epoch: 1468 loss_train: 0.2936 acc_train: 0.9074 loss_val: 0.3197 acc_val: 0.9038 time: 358.3602s\n",
            "Epoch: 1469 loss_train: 0.3043 acc_train: 0.9003 loss_val: 0.3251 acc_val: 0.9029 time: 358.6041s\n",
            "Epoch: 1470 loss_train: 0.3022 acc_train: 0.8998 loss_val: 0.3178 acc_val: 0.9021 time: 358.8495s\n",
            "Epoch: 1471 loss_train: 0.3030 acc_train: 0.9052 loss_val: 0.3212 acc_val: 0.9025 time: 359.1084s\n",
            "Epoch: 1472 loss_train: 0.3013 acc_train: 0.9018 loss_val: 0.3192 acc_val: 0.9038 time: 359.3509s\n",
            "Epoch: 1473 loss_train: 0.3046 acc_train: 0.9005 loss_val: 0.3190 acc_val: 0.9029 time: 359.5489s\n",
            "Epoch: 1474 loss_train: 0.2961 acc_train: 0.9029 loss_val: 0.3170 acc_val: 0.9038 time: 359.7929s\n",
            "Epoch: 1475 loss_train: 0.2967 acc_train: 0.9070 loss_val: 0.3186 acc_val: 0.9042 time: 360.0368s\n",
            "Epoch: 1476 loss_train: 0.2994 acc_train: 0.9042 loss_val: 0.3191 acc_val: 0.9042 time: 360.2872s\n",
            "Epoch: 1477 loss_train: 0.2922 acc_train: 0.9067 loss_val: 0.3222 acc_val: 0.9042 time: 360.5337s\n",
            "Epoch: 1478 loss_train: 0.2961 acc_train: 0.9052 loss_val: 0.3168 acc_val: 0.9046 time: 360.7864s\n",
            "Epoch: 1479 loss_train: 0.3015 acc_train: 0.9023 loss_val: 0.3217 acc_val: 0.9042 time: 361.0453s\n",
            "Epoch: 1480 loss_train: 0.3033 acc_train: 0.9029 loss_val: 0.3177 acc_val: 0.9050 time: 361.2582s\n",
            "Epoch: 1481 loss_train: 0.2945 acc_train: 0.9044 loss_val: 0.3164 acc_val: 0.9012 time: 361.4956s\n",
            "Epoch: 1482 loss_train: 0.3028 acc_train: 0.9036 loss_val: 0.3279 acc_val: 0.9012 time: 361.7461s\n",
            "Epoch: 1483 loss_train: 0.3056 acc_train: 0.8973 loss_val: 0.3167 acc_val: 0.9050 time: 362.0134s\n",
            "Epoch: 1484 loss_train: 0.3088 acc_train: 0.9024 loss_val: 0.3178 acc_val: 0.9008 time: 362.2650s\n",
            "Epoch: 1485 loss_train: 0.2947 acc_train: 0.9036 loss_val: 0.3334 acc_val: 0.9004 time: 362.5087s\n",
            "Epoch: 1486 loss_train: 0.3014 acc_train: 0.9007 loss_val: 0.3199 acc_val: 0.9033 time: 362.7566s\n",
            "Epoch: 1487 loss_train: 0.2996 acc_train: 0.9060 loss_val: 0.3182 acc_val: 0.9042 time: 363.0043s\n",
            "Epoch: 1488 loss_train: 0.2909 acc_train: 0.9070 loss_val: 0.3227 acc_val: 0.9008 time: 363.2608s\n",
            "Epoch: 1489 loss_train: 0.2965 acc_train: 0.9013 loss_val: 0.3224 acc_val: 0.9012 time: 363.5069s\n",
            "Epoch: 1490 loss_train: 0.3031 acc_train: 0.9044 loss_val: 0.3226 acc_val: 0.9017 time: 363.7758s\n",
            "Epoch: 1491 loss_train: 0.3065 acc_train: 0.9011 loss_val: 0.3166 acc_val: 0.9038 time: 364.0231s\n",
            "Epoch: 1492 loss_train: 0.3024 acc_train: 0.9038 loss_val: 0.3179 acc_val: 0.9038 time: 364.2779s\n",
            "Epoch: 1493 loss_train: 0.2953 acc_train: 0.9048 loss_val: 0.3177 acc_val: 0.9050 time: 364.5239s\n",
            "Epoch: 1494 loss_train: 0.2941 acc_train: 0.9058 loss_val: 0.3169 acc_val: 0.9046 time: 364.7714s\n",
            "Epoch: 1495 loss_train: 0.2916 acc_train: 0.9073 loss_val: 0.3161 acc_val: 0.9033 time: 365.0158s\n",
            "Epoch: 1496 loss_train: 0.2951 acc_train: 0.9068 loss_val: 0.3254 acc_val: 0.9012 time: 365.2649s\n",
            "Epoch: 1497 loss_train: 0.3051 acc_train: 0.9011 loss_val: 0.3158 acc_val: 0.9058 time: 365.5087s\n",
            "Epoch: 1498 loss_train: 0.2907 acc_train: 0.9059 loss_val: 0.3199 acc_val: 0.9046 time: 365.7590s\n",
            "Epoch: 1499 loss_train: 0.3027 acc_train: 0.9056 loss_val: 0.3208 acc_val: 0.9029 time: 366.0101s\n",
            "Epoch: 1500 loss_train: 0.3003 acc_train: 0.9023 loss_val: 0.3268 acc_val: 0.9025 time: 366.2415s\n",
            "Epoch: 1501 loss_train: 0.3000 acc_train: 0.9049 loss_val: 0.3186 acc_val: 0.9025 time: 366.4878s\n",
            "Epoch: 1502 loss_train: 0.2922 acc_train: 0.9066 loss_val: 0.3274 acc_val: 0.8983 time: 366.7336s\n",
            "Epoch: 1503 loss_train: 0.3067 acc_train: 0.8991 loss_val: 0.3178 acc_val: 0.9004 time: 366.9843s\n",
            "Epoch: 1504 loss_train: 0.3032 acc_train: 0.9044 loss_val: 0.3230 acc_val: 0.9046 time: 367.1937s\n",
            "Epoch: 1505 loss_train: 0.2936 acc_train: 0.9055 loss_val: 0.3214 acc_val: 0.9042 time: 367.4481s\n",
            "Epoch: 1506 loss_train: 0.3002 acc_train: 0.9019 loss_val: 0.3209 acc_val: 0.9058 time: 367.6969s\n",
            "Epoch: 1507 loss_train: 0.2999 acc_train: 0.9059 loss_val: 0.3322 acc_val: 0.8988 time: 367.9486s\n",
            "Epoch: 1508 loss_train: 0.3072 acc_train: 0.8984 loss_val: 0.3182 acc_val: 0.9033 time: 368.1999s\n",
            "Epoch: 1509 loss_train: 0.3059 acc_train: 0.9042 loss_val: 0.3187 acc_val: 0.9046 time: 368.4412s\n",
            "Epoch: 1510 loss_train: 0.2990 acc_train: 0.9077 loss_val: 0.3315 acc_val: 0.9004 time: 368.6852s\n",
            "Epoch: 1511 loss_train: 0.3055 acc_train: 0.8989 loss_val: 0.3207 acc_val: 0.9025 time: 368.9332s\n",
            "Epoch: 1512 loss_train: 0.3105 acc_train: 0.9033 loss_val: 0.3229 acc_val: 0.9029 time: 369.1796s\n",
            "Epoch: 1513 loss_train: 0.2976 acc_train: 0.9013 loss_val: 0.3225 acc_val: 0.9025 time: 369.4286s\n",
            "Epoch: 1514 loss_train: 0.2978 acc_train: 0.9040 loss_val: 0.3219 acc_val: 0.9083 time: 369.6729s\n",
            "Epoch: 1515 loss_train: 0.3052 acc_train: 0.9092 loss_val: 0.3431 acc_val: 0.8954 time: 369.9181s\n",
            "Epoch: 1516 loss_train: 0.3119 acc_train: 0.8958 loss_val: 0.3261 acc_val: 0.9008 time: 370.1679s\n",
            "Epoch: 1517 loss_train: 0.3157 acc_train: 0.8947 loss_val: 0.3288 acc_val: 0.9071 time: 370.4231s\n",
            "Epoch: 1518 loss_train: 0.3151 acc_train: 0.9010 loss_val: 0.3409 acc_val: 0.8962 time: 370.6688s\n",
            "Epoch: 1519 loss_train: 0.3058 acc_train: 0.8992 loss_val: 0.3262 acc_val: 0.9025 time: 370.9186s\n",
            "Epoch: 1520 loss_train: 0.2973 acc_train: 0.9041 loss_val: 0.3320 acc_val: 0.9029 time: 371.1658s\n",
            "Epoch: 1521 loss_train: 0.3223 acc_train: 0.9020 loss_val: 0.3403 acc_val: 0.8954 time: 371.4151s\n",
            "Epoch: 1522 loss_train: 0.3094 acc_train: 0.8989 loss_val: 0.3183 acc_val: 0.9050 time: 371.6686s\n",
            "Epoch: 1523 loss_train: 0.2950 acc_train: 0.9086 loss_val: 0.3275 acc_val: 0.9008 time: 371.9146s\n",
            "Epoch: 1524 loss_train: 0.3083 acc_train: 0.9078 loss_val: 0.3278 acc_val: 0.9029 time: 372.1615s\n",
            "Epoch: 1525 loss_train: 0.2997 acc_train: 0.8971 loss_val: 0.3174 acc_val: 0.9062 time: 372.4085s\n",
            "Epoch: 1526 loss_train: 0.2921 acc_train: 0.9054 loss_val: 0.3173 acc_val: 0.9046 time: 372.6560s\n",
            "Epoch: 1527 loss_train: 0.3054 acc_train: 0.9060 loss_val: 0.3311 acc_val: 0.9004 time: 372.9016s\n",
            "Epoch: 1528 loss_train: 0.2995 acc_train: 0.8998 loss_val: 0.3213 acc_val: 0.9025 time: 373.1475s\n",
            "Epoch: 1529 loss_train: 0.3020 acc_train: 0.9043 loss_val: 0.3182 acc_val: 0.9050 time: 373.3956s\n",
            "Epoch: 1530 loss_train: 0.3054 acc_train: 0.9027 loss_val: 0.3289 acc_val: 0.9012 time: 373.6421s\n",
            "Epoch: 1531 loss_train: 0.3039 acc_train: 0.8993 loss_val: 0.3184 acc_val: 0.9033 time: 373.8877s\n",
            "Epoch: 1532 loss_train: 0.2948 acc_train: 0.9068 loss_val: 0.3198 acc_val: 0.9054 time: 374.1299s\n",
            "Epoch: 1533 loss_train: 0.2914 acc_train: 0.9045 loss_val: 0.3187 acc_val: 0.9062 time: 374.3892s\n",
            "Epoch: 1534 loss_train: 0.3039 acc_train: 0.9011 loss_val: 0.3166 acc_val: 0.9029 time: 374.5979s\n",
            "Epoch: 1535 loss_train: 0.2890 acc_train: 0.9066 loss_val: 0.3174 acc_val: 0.9046 time: 374.8533s\n",
            "Epoch: 1536 loss_train: 0.2963 acc_train: 0.9045 loss_val: 0.3217 acc_val: 0.9042 time: 375.1058s\n",
            "Epoch: 1537 loss_train: 0.2911 acc_train: 0.9063 loss_val: 0.3183 acc_val: 0.9046 time: 375.3541s\n",
            "Epoch: 1538 loss_train: 0.2958 acc_train: 0.9077 loss_val: 0.3199 acc_val: 0.9012 time: 375.5676s\n",
            "Epoch: 1539 loss_train: 0.2934 acc_train: 0.9041 loss_val: 0.3202 acc_val: 0.9058 time: 375.8178s\n",
            "Epoch: 1540 loss_train: 0.2943 acc_train: 0.9064 loss_val: 0.3171 acc_val: 0.9025 time: 376.0704s\n",
            "Epoch: 1541 loss_train: 0.2969 acc_train: 0.9045 loss_val: 0.3173 acc_val: 0.9054 time: 376.3004s\n",
            "Epoch: 1542 loss_train: 0.2977 acc_train: 0.9013 loss_val: 0.3188 acc_val: 0.9058 time: 376.5475s\n",
            "Epoch: 1543 loss_train: 0.2886 acc_train: 0.9086 loss_val: 0.3184 acc_val: 0.9004 time: 376.7721s\n",
            "Epoch: 1544 loss_train: 0.2973 acc_train: 0.9073 loss_val: 0.3205 acc_val: 0.9038 time: 376.9847s\n",
            "Epoch: 1545 loss_train: 0.3000 acc_train: 0.9036 loss_val: 0.3173 acc_val: 0.9033 time: 377.2418s\n",
            "Epoch: 1546 loss_train: 0.2904 acc_train: 0.9100 loss_val: 0.3244 acc_val: 0.9029 time: 377.4909s\n",
            "Epoch: 1547 loss_train: 0.3050 acc_train: 0.9003 loss_val: 0.3166 acc_val: 0.9025 time: 377.7278s\n",
            "Epoch: 1548 loss_train: 0.2944 acc_train: 0.9087 loss_val: 0.3166 acc_val: 0.9050 time: 377.9735s\n",
            "Epoch: 1549 loss_train: 0.2947 acc_train: 0.9046 loss_val: 0.3209 acc_val: 0.9046 time: 378.2199s\n",
            "Epoch: 1550 loss_train: 0.2981 acc_train: 0.9027 loss_val: 0.3210 acc_val: 0.9071 time: 378.4747s\n",
            "Epoch: 1551 loss_train: 0.3031 acc_train: 0.9065 loss_val: 0.3491 acc_val: 0.8954 time: 378.7220s\n",
            "Epoch: 1552 loss_train: 0.3236 acc_train: 0.8925 loss_val: 0.3234 acc_val: 0.9012 time: 378.9703s\n",
            "Epoch: 1553 loss_train: 0.3159 acc_train: 0.8984 loss_val: 0.3195 acc_val: 0.9067 time: 379.2178s\n",
            "Epoch: 1554 loss_train: 0.3004 acc_train: 0.9041 loss_val: 0.3410 acc_val: 0.8958 time: 379.4744s\n",
            "Epoch: 1555 loss_train: 0.3120 acc_train: 0.8963 loss_val: 0.3244 acc_val: 0.9008 time: 379.7261s\n",
            "Epoch: 1556 loss_train: 0.3028 acc_train: 0.9056 loss_val: 0.3200 acc_val: 0.9042 time: 379.9803s\n",
            "Epoch: 1557 loss_train: 0.3015 acc_train: 0.9034 loss_val: 0.3438 acc_val: 0.8933 time: 380.2257s\n",
            "Epoch: 1558 loss_train: 0.3163 acc_train: 0.8956 loss_val: 0.3237 acc_val: 0.9058 time: 380.4712s\n",
            "Epoch: 1559 loss_train: 0.3083 acc_train: 0.9032 loss_val: 0.3288 acc_val: 0.8996 time: 380.7156s\n",
            "Epoch: 1560 loss_train: 0.3054 acc_train: 0.8981 loss_val: 0.3286 acc_val: 0.8996 time: 380.9737s\n",
            "Epoch: 1561 loss_train: 0.3055 acc_train: 0.8982 loss_val: 0.3244 acc_val: 0.9054 time: 381.2170s\n",
            "Epoch: 1562 loss_train: 0.3184 acc_train: 0.9021 loss_val: 0.3194 acc_val: 0.9038 time: 381.4662s\n",
            "Epoch: 1563 loss_train: 0.3031 acc_train: 0.9056 loss_val: 0.3352 acc_val: 0.9000 time: 381.7075s\n",
            "Epoch: 1564 loss_train: 0.3032 acc_train: 0.9023 loss_val: 0.3223 acc_val: 0.9038 time: 381.9559s\n",
            "Epoch: 1565 loss_train: 0.3033 acc_train: 0.9026 loss_val: 0.3320 acc_val: 0.9000 time: 382.2025s\n",
            "Epoch: 1566 loss_train: 0.3018 acc_train: 0.9022 loss_val: 0.3186 acc_val: 0.9042 time: 382.4501s\n",
            "Epoch: 1567 loss_train: 0.3014 acc_train: 0.9025 loss_val: 0.3275 acc_val: 0.9062 time: 382.6942s\n",
            "Epoch: 1568 loss_train: 0.3212 acc_train: 0.9030 loss_val: 0.3464 acc_val: 0.8946 time: 382.9380s\n",
            "Epoch: 1569 loss_train: 0.3081 acc_train: 0.8989 loss_val: 0.3272 acc_val: 0.9025 time: 383.1881s\n",
            "Epoch: 1570 loss_train: 0.3024 acc_train: 0.9030 loss_val: 0.3409 acc_val: 0.8983 time: 383.4437s\n",
            "Epoch: 1571 loss_train: 0.3311 acc_train: 0.9000 loss_val: 0.4267 acc_val: 0.8696 time: 383.6944s\n",
            "Epoch: 1572 loss_train: 0.3920 acc_train: 0.8704 loss_val: 0.3742 acc_val: 0.8883 time: 383.9374s\n",
            "Epoch: 1573 loss_train: 0.3796 acc_train: 0.8819 loss_val: 0.3899 acc_val: 0.8883 time: 384.1863s\n",
            "Epoch: 1574 loss_train: 0.3947 acc_train: 0.8774 loss_val: 0.3609 acc_val: 0.8946 time: 384.4344s\n",
            "Epoch: 1575 loss_train: 0.3420 acc_train: 0.8936 loss_val: 0.3637 acc_val: 0.8971 time: 384.6842s\n",
            "Epoch: 1576 loss_train: 0.3746 acc_train: 0.8864 loss_val: 0.3649 acc_val: 0.9004 time: 384.9271s\n",
            "Epoch: 1577 loss_train: 0.3850 acc_train: 0.8846 loss_val: 0.3854 acc_val: 0.8879 time: 385.1895s\n",
            "Epoch: 1578 loss_train: 0.3711 acc_train: 0.8856 loss_val: 0.3776 acc_val: 0.8933 time: 385.4369s\n",
            "Epoch: 1579 loss_train: 0.3765 acc_train: 0.8873 loss_val: 0.3669 acc_val: 0.8812 time: 385.6808s\n",
            "Epoch: 1580 loss_train: 0.3512 acc_train: 0.8792 loss_val: 0.3584 acc_val: 0.8921 time: 385.9305s\n",
            "Epoch: 1581 loss_train: 0.3512 acc_train: 0.8822 loss_val: 0.4161 acc_val: 0.8712 time: 386.1822s\n",
            "Epoch: 1582 loss_train: 0.4335 acc_train: 0.8637 loss_val: 0.4139 acc_val: 0.8742 time: 386.4430s\n",
            "Epoch: 1583 loss_train: 0.3831 acc_train: 0.8788 loss_val: 0.3775 acc_val: 0.8838 time: 386.6853s\n",
            "Epoch: 1584 loss_train: 0.3687 acc_train: 0.8804 loss_val: 0.4523 acc_val: 0.8612 time: 386.9349s\n",
            "Epoch: 1585 loss_train: 0.4626 acc_train: 0.8543 loss_val: 0.5934 acc_val: 0.8242 time: 387.1511s\n",
            "Epoch: 1586 loss_train: 0.5449 acc_train: 0.8290 loss_val: 0.4730 acc_val: 0.8562 time: 387.4095s\n",
            "Epoch: 1587 loss_train: 0.4527 acc_train: 0.8505 loss_val: 0.6430 acc_val: 0.7692 time: 387.6545s\n",
            "Epoch: 1588 loss_train: 0.7194 acc_train: 0.7229 loss_val: 1.2527 acc_val: 0.6338 time: 387.9162s\n",
            "Epoch: 1589 loss_train: 1.2933 acc_train: 0.6290 loss_val: 1.8741 acc_val: 0.5917 time: 388.1616s\n",
            "Epoch: 1590 loss_train: 1.8995 acc_train: 0.5767 loss_val: 3.2423 acc_val: 0.3029 time: 388.4077s\n",
            "Epoch: 1591 loss_train: 3.2662 acc_train: 0.3175 loss_val: 1.1238 acc_val: 0.5708 time: 388.6655s\n",
            "Epoch: 1592 loss_train: 1.1654 acc_train: 0.5267 loss_val: 1.7950 acc_val: 0.2371 time: 388.9134s\n",
            "Epoch: 1593 loss_train: 1.7760 acc_train: 0.2409 loss_val: 1.7469 acc_val: 0.2575 time: 389.1601s\n",
            "Epoch: 1594 loss_train: 1.7025 acc_train: 0.2634 loss_val: 1.6007 acc_val: 0.4521 time: 389.4042s\n",
            "Epoch: 1595 loss_train: 1.6019 acc_train: 0.4460 loss_val: 1.6430 acc_val: 0.4975 time: 389.6572s\n",
            "Epoch: 1596 loss_train: 1.6024 acc_train: 0.5037 loss_val: 1.4698 acc_val: 0.5175 time: 389.8980s\n",
            "Epoch: 1597 loss_train: 1.4648 acc_train: 0.5249 loss_val: 1.2931 acc_val: 0.5946 time: 390.1366s\n",
            "Epoch: 1598 loss_train: 1.3037 acc_train: 0.5959 loss_val: 1.4760 acc_val: 0.5671 time: 390.3870s\n",
            "Epoch: 1599 loss_train: 1.5166 acc_train: 0.5419 loss_val: 1.1577 acc_val: 0.6221 time: 390.6306s\n",
            "Epoch: 1600 loss_train: 1.1650 acc_train: 0.6231 loss_val: 1.3647 acc_val: 0.5733 time: 390.8844s\n",
            "Epoch: 1601 loss_train: 1.3407 acc_train: 0.5863 loss_val: 1.3003 acc_val: 0.5829 time: 391.1421s\n",
            "Epoch: 1602 loss_train: 1.2702 acc_train: 0.5885 loss_val: 1.1989 acc_val: 0.5867 time: 391.3823s\n",
            "Epoch: 1603 loss_train: 1.1929 acc_train: 0.5870 loss_val: 1.3057 acc_val: 0.5742 time: 391.6381s\n",
            "Epoch: 1604 loss_train: 1.3000 acc_train: 0.5710 loss_val: 1.2628 acc_val: 0.5854 time: 391.8831s\n",
            "Epoch: 1605 loss_train: 1.2713 acc_train: 0.5895 loss_val: 1.1376 acc_val: 0.6254 time: 392.1301s\n",
            "Epoch: 1606 loss_train: 1.1492 acc_train: 0.6134 loss_val: 1.1703 acc_val: 0.6683 time: 392.3776s\n",
            "Epoch: 1607 loss_train: 1.1597 acc_train: 0.6498 loss_val: 1.1706 acc_val: 0.6833 time: 392.6245s\n",
            "Epoch: 1608 loss_train: 1.1391 acc_train: 0.6768 loss_val: 1.1369 acc_val: 0.6700 time: 392.8741s\n",
            "Epoch: 1609 loss_train: 1.1299 acc_train: 0.6634 loss_val: 1.1035 acc_val: 0.6750 time: 393.1282s\n",
            "Epoch: 1610 loss_train: 1.1163 acc_train: 0.6625 loss_val: 1.1060 acc_val: 0.6800 time: 393.3782s\n",
            "Epoch: 1611 loss_train: 1.1112 acc_train: 0.6693 loss_val: 1.0729 acc_val: 0.7150 time: 393.6343s\n",
            "Epoch: 1612 loss_train: 1.0860 acc_train: 0.6891 loss_val: 1.0893 acc_val: 0.6992 time: 393.8864s\n",
            "Epoch: 1613 loss_train: 1.0928 acc_train: 0.6713 loss_val: 1.0297 acc_val: 0.7004 time: 394.1319s\n",
            "Epoch: 1614 loss_train: 1.0536 acc_train: 0.6856 loss_val: 1.0152 acc_val: 0.6483 time: 394.3802s\n",
            "Epoch: 1615 loss_train: 1.0307 acc_train: 0.6527 loss_val: 1.0309 acc_val: 0.6296 time: 394.6288s\n",
            "Epoch: 1616 loss_train: 1.0609 acc_train: 0.6274 loss_val: 1.0015 acc_val: 0.6521 time: 394.8178s\n",
            "Epoch: 1617 loss_train: 0.9936 acc_train: 0.6654 loss_val: 1.0004 acc_val: 0.6833 time: 395.0579s\n",
            "Epoch: 1618 loss_train: 1.0279 acc_train: 0.6626 loss_val: 0.9625 acc_val: 0.7046 time: 395.3007s\n",
            "Epoch: 1619 loss_train: 0.9725 acc_train: 0.7023 loss_val: 0.9645 acc_val: 0.7075 time: 395.5554s\n",
            "Epoch: 1620 loss_train: 0.9782 acc_train: 0.7026 loss_val: 0.9631 acc_val: 0.7167 time: 395.8059s\n",
            "Epoch: 1621 loss_train: 0.9861 acc_train: 0.6977 loss_val: 0.9305 acc_val: 0.7392 time: 396.0434s\n",
            "Epoch: 1622 loss_train: 0.9530 acc_train: 0.7113 loss_val: 0.9263 acc_val: 0.7446 time: 396.2940s\n",
            "Epoch: 1623 loss_train: 0.9367 acc_train: 0.7192 loss_val: 0.8988 acc_val: 0.7267 time: 396.5580s\n",
            "Epoch: 1624 loss_train: 0.9141 acc_train: 0.7211 loss_val: 0.9116 acc_val: 0.7312 time: 396.7985s\n",
            "Epoch: 1625 loss_train: 0.9467 acc_train: 0.7025 loss_val: 0.8824 acc_val: 0.7504 time: 397.0559s\n",
            "Epoch: 1626 loss_train: 0.9148 acc_train: 0.7255 loss_val: 0.8801 acc_val: 0.7771 time: 397.3030s\n",
            "Epoch: 1627 loss_train: 0.9113 acc_train: 0.7455 loss_val: 0.8582 acc_val: 0.7742 time: 397.5589s\n",
            "Epoch: 1628 loss_train: 0.8974 acc_train: 0.7484 loss_val: 0.8582 acc_val: 0.7688 time: 397.7899s\n",
            "Epoch: 1629 loss_train: 0.8925 acc_train: 0.7437 loss_val: 0.8396 acc_val: 0.7750 time: 398.0075s\n",
            "Epoch: 1630 loss_train: 0.8669 acc_train: 0.7580 loss_val: 0.8363 acc_val: 0.7758 time: 398.2542s\n",
            "Epoch: 1631 loss_train: 0.8672 acc_train: 0.7598 loss_val: 0.8265 acc_val: 0.7700 time: 398.4981s\n",
            "Epoch: 1632 loss_train: 0.8550 acc_train: 0.7568 loss_val: 0.8156 acc_val: 0.7738 time: 398.7457s\n",
            "Epoch: 1633 loss_train: 0.8603 acc_train: 0.7591 loss_val: 0.8042 acc_val: 0.7858 time: 398.9924s\n",
            "Epoch: 1634 loss_train: 0.8493 acc_train: 0.7605 loss_val: 0.8031 acc_val: 0.7837 time: 399.2474s\n",
            "Epoch: 1635 loss_train: 0.8355 acc_train: 0.7718 loss_val: 0.7947 acc_val: 0.7758 time: 399.4905s\n",
            "Epoch: 1636 loss_train: 0.8336 acc_train: 0.7667 loss_val: 0.7936 acc_val: 0.7762 time: 399.7316s\n",
            "Epoch: 1637 loss_train: 0.8208 acc_train: 0.7641 loss_val: 0.7865 acc_val: 0.7754 time: 399.9812s\n",
            "Epoch: 1638 loss_train: 0.8301 acc_train: 0.7620 loss_val: 0.7717 acc_val: 0.7767 time: 400.2274s\n",
            "Epoch: 1639 loss_train: 0.8056 acc_train: 0.7693 loss_val: 0.7633 acc_val: 0.7829 time: 400.4716s\n",
            "Epoch: 1640 loss_train: 0.8101 acc_train: 0.7737 loss_val: 0.7587 acc_val: 0.7846 time: 400.6886s\n",
            "Epoch: 1641 loss_train: 0.8105 acc_train: 0.7684 loss_val: 0.7515 acc_val: 0.7833 time: 400.9294s\n",
            "Epoch: 1642 loss_train: 0.7968 acc_train: 0.7699 loss_val: 0.7517 acc_val: 0.7783 time: 401.1786s\n",
            "Epoch: 1643 loss_train: 0.7801 acc_train: 0.7746 loss_val: 0.7487 acc_val: 0.7775 time: 401.4323s\n",
            "Epoch: 1644 loss_train: 0.7965 acc_train: 0.7673 loss_val: 0.7445 acc_val: 0.7804 time: 401.6790s\n",
            "Epoch: 1645 loss_train: 0.7925 acc_train: 0.7678 loss_val: 0.7414 acc_val: 0.7850 time: 401.9263s\n",
            "Epoch: 1646 loss_train: 0.7894 acc_train: 0.7695 loss_val: 0.7321 acc_val: 0.7846 time: 402.1728s\n",
            "Epoch: 1647 loss_train: 0.7834 acc_train: 0.7689 loss_val: 0.7231 acc_val: 0.7858 time: 402.4155s\n",
            "Epoch: 1648 loss_train: 0.7777 acc_train: 0.7730 loss_val: 0.7252 acc_val: 0.7846 time: 402.6639s\n",
            "Epoch: 1649 loss_train: 0.7856 acc_train: 0.7686 loss_val: 0.7162 acc_val: 0.7842 time: 402.8553s\n",
            "Epoch: 1650 loss_train: 0.7685 acc_train: 0.7733 loss_val: 0.7178 acc_val: 0.7871 time: 403.1213s\n",
            "Epoch: 1651 loss_train: 0.7677 acc_train: 0.7767 loss_val: 0.7166 acc_val: 0.7842 time: 403.3671s\n",
            "Epoch: 1652 loss_train: 0.7730 acc_train: 0.7719 loss_val: 0.7107 acc_val: 0.7871 time: 403.6174s\n",
            "Epoch: 1653 loss_train: 0.7568 acc_train: 0.7735 loss_val: 0.7202 acc_val: 0.7967 time: 403.8065s\n",
            "Epoch: 1654 loss_train: 0.7738 acc_train: 0.7755 loss_val: 0.7073 acc_val: 0.7825 time: 404.0514s\n",
            "Epoch: 1655 loss_train: 0.7550 acc_train: 0.7759 loss_val: 0.6961 acc_val: 0.7896 time: 404.2929s\n",
            "Epoch: 1656 loss_train: 0.7618 acc_train: 0.7774 loss_val: 0.7184 acc_val: 0.7992 time: 404.5321s\n",
            "Epoch: 1657 loss_train: 0.7774 acc_train: 0.7786 loss_val: 0.7048 acc_val: 0.7837 time: 404.7896s\n",
            "Epoch: 1658 loss_train: 0.7582 acc_train: 0.7792 loss_val: 0.6939 acc_val: 0.7875 time: 405.0356s\n",
            "Epoch: 1659 loss_train: 0.7427 acc_train: 0.7823 loss_val: 0.7161 acc_val: 0.8037 time: 405.2772s\n",
            "Epoch: 1660 loss_train: 0.7568 acc_train: 0.7778 loss_val: 0.6879 acc_val: 0.7913 time: 405.5206s\n",
            "Epoch: 1661 loss_train: 0.7435 acc_train: 0.7751 loss_val: 0.6897 acc_val: 0.7900 time: 405.7761s\n",
            "Epoch: 1662 loss_train: 0.7546 acc_train: 0.7777 loss_val: 0.6962 acc_val: 0.7987 time: 406.0255s\n",
            "Epoch: 1663 loss_train: 0.7580 acc_train: 0.7773 loss_val: 0.6786 acc_val: 0.7996 time: 406.2740s\n",
            "Epoch: 1664 loss_train: 0.7301 acc_train: 0.7846 loss_val: 0.6961 acc_val: 0.7863 time: 406.5157s\n",
            "Epoch: 1665 loss_train: 0.7481 acc_train: 0.7755 loss_val: 0.6731 acc_val: 0.7950 time: 406.7659s\n",
            "Epoch: 1666 loss_train: 0.7149 acc_train: 0.7864 loss_val: 0.6876 acc_val: 0.8013 time: 407.0074s\n",
            "Epoch: 1667 loss_train: 0.7357 acc_train: 0.7791 loss_val: 0.6865 acc_val: 0.7900 time: 407.2553s\n",
            "Epoch: 1668 loss_train: 0.7361 acc_train: 0.7766 loss_val: 0.6735 acc_val: 0.7946 time: 407.4737s\n",
            "Epoch: 1669 loss_train: 0.7283 acc_train: 0.7851 loss_val: 0.6935 acc_val: 0.8079 time: 407.7666s\n",
            "Epoch: 1670 loss_train: 0.7610 acc_train: 0.7747 loss_val: 0.6713 acc_val: 0.7950 time: 407.9973s\n",
            "Epoch: 1671 loss_train: 0.7297 acc_train: 0.7810 loss_val: 0.6683 acc_val: 0.7946 time: 408.2417s\n",
            "Epoch: 1672 loss_train: 0.7367 acc_train: 0.7798 loss_val: 0.6846 acc_val: 0.8096 time: 408.4803s\n",
            "Epoch: 1673 loss_train: 0.7579 acc_train: 0.7725 loss_val: 0.6634 acc_val: 0.7958 time: 408.7361s\n",
            "Epoch: 1674 loss_train: 0.7010 acc_train: 0.7888 loss_val: 0.6658 acc_val: 0.8017 time: 408.9790s\n",
            "Epoch: 1675 loss_train: 0.7262 acc_train: 0.7808 loss_val: 0.6673 acc_val: 0.8083 time: 409.2303s\n",
            "Epoch: 1676 loss_train: 0.7164 acc_train: 0.7863 loss_val: 0.6591 acc_val: 0.8100 time: 409.4724s\n",
            "Epoch: 1677 loss_train: 0.7170 acc_train: 0.7805 loss_val: 0.6743 acc_val: 0.7921 time: 409.7263s\n",
            "Epoch: 1678 loss_train: 0.7132 acc_train: 0.7873 loss_val: 0.6478 acc_val: 0.8104 time: 409.9725s\n",
            "Epoch: 1679 loss_train: 0.7061 acc_train: 0.7865 loss_val: 0.6864 acc_val: 0.8108 time: 410.2265s\n",
            "Epoch: 1680 loss_train: 0.7393 acc_train: 0.7811 loss_val: 0.6614 acc_val: 0.8021 time: 410.4738s\n",
            "Epoch: 1681 loss_train: 0.7229 acc_train: 0.7816 loss_val: 0.6505 acc_val: 0.8021 time: 410.7236s\n",
            "Epoch: 1682 loss_train: 0.7160 acc_train: 0.7802 loss_val: 0.6787 acc_val: 0.8087 time: 410.9686s\n",
            "Epoch: 1683 loss_train: 0.7338 acc_train: 0.7859 loss_val: 0.6516 acc_val: 0.8042 time: 411.2103s\n",
            "Epoch: 1684 loss_train: 0.7069 acc_train: 0.7884 loss_val: 0.6550 acc_val: 0.8029 time: 411.4379s\n",
            "Epoch: 1685 loss_train: 0.7117 acc_train: 0.7758 loss_val: 0.6560 acc_val: 0.8104 time: 411.6143s\n",
            "Epoch: 1686 loss_train: 0.6967 acc_train: 0.7922 loss_val: 0.6396 acc_val: 0.7987 time: 411.8786s\n",
            "Epoch: 1687 loss_train: 0.6994 acc_train: 0.7857 loss_val: 0.6403 acc_val: 0.8017 time: 412.0901s\n",
            "Epoch: 1688 loss_train: 0.6931 acc_train: 0.7887 loss_val: 0.6533 acc_val: 0.8087 time: 412.2663s\n",
            "Epoch: 1689 loss_train: 0.7193 acc_train: 0.7785 loss_val: 0.6404 acc_val: 0.8083 time: 412.4970s\n",
            "Epoch: 1690 loss_train: 0.6992 acc_train: 0.7893 loss_val: 0.6471 acc_val: 0.7933 time: 412.7099s\n",
            "Epoch: 1691 loss_train: 0.7052 acc_train: 0.7874 loss_val: 0.6416 acc_val: 0.8096 time: 412.9272s\n",
            "Epoch: 1692 loss_train: 0.6916 acc_train: 0.7924 loss_val: 0.6372 acc_val: 0.8100 time: 413.1028s\n",
            "Epoch: 1693 loss_train: 0.7015 acc_train: 0.7851 loss_val: 0.6435 acc_val: 0.8058 time: 413.2831s\n",
            "Epoch: 1694 loss_train: 0.7071 acc_train: 0.7795 loss_val: 0.6394 acc_val: 0.8146 time: 413.4600s\n",
            "Epoch: 1695 loss_train: 0.6934 acc_train: 0.7912 loss_val: 0.6312 acc_val: 0.8154 time: 413.7094s\n",
            "Epoch: 1696 loss_train: 0.6947 acc_train: 0.7900 loss_val: 0.6393 acc_val: 0.8075 time: 413.9573s\n",
            "Epoch: 1697 loss_train: 0.7030 acc_train: 0.7904 loss_val: 0.6368 acc_val: 0.8142 time: 414.2035s\n",
            "Epoch: 1698 loss_train: 0.6992 acc_train: 0.7889 loss_val: 0.6340 acc_val: 0.8129 time: 414.4438s\n",
            "Epoch: 1699 loss_train: 0.7076 acc_train: 0.7833 loss_val: 0.6448 acc_val: 0.8100 time: 414.6939s\n",
            "Epoch: 1700 loss_train: 0.7009 acc_train: 0.7878 loss_val: 0.6263 acc_val: 0.8179 time: 414.9435s\n",
            "Epoch: 1701 loss_train: 0.6854 acc_train: 0.7996 loss_val: 0.6630 acc_val: 0.8142 time: 415.1924s\n",
            "Epoch: 1702 loss_train: 0.7061 acc_train: 0.7874 loss_val: 0.6626 acc_val: 0.8021 time: 415.4318s\n",
            "Epoch: 1703 loss_train: 0.7112 acc_train: 0.7821 loss_val: 0.6395 acc_val: 0.8079 time: 415.6697s\n",
            "Epoch: 1704 loss_train: 0.6990 acc_train: 0.7925 loss_val: 0.6764 acc_val: 0.8104 time: 415.9196s\n",
            "Epoch: 1705 loss_train: 0.7309 acc_train: 0.7856 loss_val: 0.6355 acc_val: 0.8050 time: 416.1651s\n",
            "Epoch: 1706 loss_train: 0.6941 acc_train: 0.7865 loss_val: 0.6300 acc_val: 0.8096 time: 416.4079s\n",
            "Epoch: 1707 loss_train: 0.6699 acc_train: 0.7948 loss_val: 0.6658 acc_val: 0.8100 time: 416.6546s\n",
            "Epoch: 1708 loss_train: 0.7210 acc_train: 0.7804 loss_val: 0.6213 acc_val: 0.8129 time: 416.8953s\n",
            "Epoch: 1709 loss_train: 0.6775 acc_train: 0.7966 loss_val: 0.6339 acc_val: 0.8021 time: 417.0724s\n",
            "Epoch: 1710 loss_train: 0.6764 acc_train: 0.7954 loss_val: 0.6355 acc_val: 0.8137 time: 417.3179s\n",
            "Epoch: 1711 loss_train: 0.6724 acc_train: 0.7948 loss_val: 0.6295 acc_val: 0.8121 time: 417.5631s\n",
            "Epoch: 1712 loss_train: 0.6831 acc_train: 0.7918 loss_val: 0.6514 acc_val: 0.8054 time: 417.8123s\n",
            "Epoch: 1713 loss_train: 0.6994 acc_train: 0.7918 loss_val: 0.6224 acc_val: 0.8121 time: 418.0630s\n",
            "Epoch: 1714 loss_train: 0.6722 acc_train: 0.7912 loss_val: 0.6518 acc_val: 0.8133 time: 418.3202s\n",
            "Epoch: 1715 loss_train: 0.6998 acc_train: 0.7897 loss_val: 0.6291 acc_val: 0.8092 time: 418.5705s\n",
            "Epoch: 1716 loss_train: 0.6867 acc_train: 0.7977 loss_val: 0.6200 acc_val: 0.8154 time: 418.8177s\n",
            "Epoch: 1717 loss_train: 0.6629 acc_train: 0.7942 loss_val: 0.6445 acc_val: 0.8129 time: 419.0618s\n",
            "Epoch: 1718 loss_train: 0.7012 acc_train: 0.7831 loss_val: 0.6142 acc_val: 0.8163 time: 419.3104s\n",
            "Epoch: 1719 loss_train: 0.6654 acc_train: 0.7934 loss_val: 0.6283 acc_val: 0.8129 time: 419.5525s\n",
            "Epoch: 1720 loss_train: 0.6793 acc_train: 0.7924 loss_val: 0.6163 acc_val: 0.8150 time: 419.7748s\n",
            "Epoch: 1721 loss_train: 0.6728 acc_train: 0.7903 loss_val: 0.6223 acc_val: 0.8158 time: 420.0370s\n",
            "Epoch: 1722 loss_train: 0.6869 acc_train: 0.7923 loss_val: 0.6222 acc_val: 0.8163 time: 420.2821s\n",
            "Epoch: 1723 loss_train: 0.6924 acc_train: 0.7896 loss_val: 0.6088 acc_val: 0.8187 time: 420.5264s\n",
            "Epoch: 1724 loss_train: 0.6646 acc_train: 0.7968 loss_val: 0.6244 acc_val: 0.8175 time: 420.7496s\n",
            "Epoch: 1725 loss_train: 0.6760 acc_train: 0.7987 loss_val: 0.6122 acc_val: 0.8192 time: 421.0093s\n",
            "Epoch: 1726 loss_train: 0.6600 acc_train: 0.7993 loss_val: 0.6146 acc_val: 0.8171 time: 421.2595s\n",
            "Epoch: 1727 loss_train: 0.6605 acc_train: 0.8036 loss_val: 0.6311 acc_val: 0.8154 time: 421.4953s\n",
            "Epoch: 1728 loss_train: 0.6957 acc_train: 0.7892 loss_val: 0.6181 acc_val: 0.8104 time: 421.7511s\n",
            "Epoch: 1729 loss_train: 0.6609 acc_train: 0.7937 loss_val: 0.6253 acc_val: 0.8129 time: 421.9969s\n",
            "Epoch: 1730 loss_train: 0.6817 acc_train: 0.7937 loss_val: 0.6317 acc_val: 0.8137 time: 422.2467s\n",
            "Epoch: 1731 loss_train: 0.6769 acc_train: 0.7943 loss_val: 0.6121 acc_val: 0.8175 time: 422.4926s\n",
            "Epoch: 1732 loss_train: 0.6545 acc_train: 0.7998 loss_val: 0.6197 acc_val: 0.8121 time: 422.7399s\n",
            "Epoch: 1733 loss_train: 0.6592 acc_train: 0.8015 loss_val: 0.6151 acc_val: 0.8104 time: 422.9911s\n",
            "Epoch: 1734 loss_train: 0.6577 acc_train: 0.7977 loss_val: 0.6114 acc_val: 0.8154 time: 423.2349s\n",
            "Epoch: 1735 loss_train: 0.6625 acc_train: 0.7967 loss_val: 0.6031 acc_val: 0.8137 time: 423.4941s\n",
            "Epoch: 1736 loss_train: 0.6578 acc_train: 0.7954 loss_val: 0.6046 acc_val: 0.8150 time: 423.7519s\n",
            "Epoch: 1737 loss_train: 0.6657 acc_train: 0.7896 loss_val: 0.6069 acc_val: 0.8179 time: 423.9972s\n",
            "Epoch: 1738 loss_train: 0.6590 acc_train: 0.7977 loss_val: 0.6017 acc_val: 0.8154 time: 424.2216s\n",
            "Epoch: 1739 loss_train: 0.6579 acc_train: 0.7920 loss_val: 0.6029 acc_val: 0.8158 time: 424.4634s\n",
            "Epoch: 1740 loss_train: 0.6552 acc_train: 0.7974 loss_val: 0.6025 acc_val: 0.8154 time: 424.7104s\n",
            "Epoch: 1741 loss_train: 0.6505 acc_train: 0.7984 loss_val: 0.6112 acc_val: 0.8154 time: 424.9601s\n",
            "Epoch: 1742 loss_train: 0.6618 acc_train: 0.7955 loss_val: 0.6058 acc_val: 0.8146 time: 425.2083s\n",
            "Epoch: 1743 loss_train: 0.6602 acc_train: 0.7940 loss_val: 0.5980 acc_val: 0.8175 time: 425.4565s\n",
            "Epoch: 1744 loss_train: 0.6492 acc_train: 0.7997 loss_val: 0.6059 acc_val: 0.8196 time: 425.6798s\n",
            "Epoch: 1745 loss_train: 0.6667 acc_train: 0.7968 loss_val: 0.6016 acc_val: 0.8175 time: 425.9387s\n",
            "Epoch: 1746 loss_train: 0.6679 acc_train: 0.7958 loss_val: 0.5963 acc_val: 0.8167 time: 426.1949s\n",
            "Epoch: 1747 loss_train: 0.6651 acc_train: 0.7938 loss_val: 0.6129 acc_val: 0.8183 time: 426.4410s\n",
            "Epoch: 1748 loss_train: 0.6716 acc_train: 0.7897 loss_val: 0.6051 acc_val: 0.8146 time: 426.6802s\n",
            "Epoch: 1749 loss_train: 0.6571 acc_train: 0.7973 loss_val: 0.6007 acc_val: 0.8129 time: 426.9432s\n",
            "Epoch: 1750 loss_train: 0.6553 acc_train: 0.7965 loss_val: 0.6138 acc_val: 0.8150 time: 427.1961s\n",
            "Epoch: 1751 loss_train: 0.6763 acc_train: 0.7879 loss_val: 0.5958 acc_val: 0.8196 time: 427.4413s\n",
            "Epoch: 1752 loss_train: 0.6440 acc_train: 0.7999 loss_val: 0.5982 acc_val: 0.8213 time: 427.6829s\n",
            "Epoch: 1753 loss_train: 0.6611 acc_train: 0.7998 loss_val: 0.5963 acc_val: 0.8183 time: 427.9300s\n",
            "Epoch: 1754 loss_train: 0.6458 acc_train: 0.8020 loss_val: 0.6034 acc_val: 0.8133 time: 428.1551s\n",
            "Epoch: 1755 loss_train: 0.6552 acc_train: 0.7968 loss_val: 0.5961 acc_val: 0.8158 time: 428.4048s\n",
            "Epoch: 1756 loss_train: 0.6608 acc_train: 0.7966 loss_val: 0.6031 acc_val: 0.8167 time: 428.6628s\n",
            "Epoch: 1757 loss_train: 0.6697 acc_train: 0.7936 loss_val: 0.5910 acc_val: 0.8187 time: 428.9072s\n",
            "Epoch: 1758 loss_train: 0.6504 acc_train: 0.8020 loss_val: 0.5928 acc_val: 0.8179 time: 429.1581s\n",
            "Epoch: 1759 loss_train: 0.6613 acc_train: 0.7978 loss_val: 0.5947 acc_val: 0.8200 time: 429.4043s\n",
            "Epoch: 1760 loss_train: 0.6562 acc_train: 0.7948 loss_val: 0.5920 acc_val: 0.8208 time: 429.6415s\n",
            "Epoch: 1761 loss_train: 0.6656 acc_train: 0.7960 loss_val: 0.5943 acc_val: 0.8187 time: 429.8557s\n",
            "Epoch: 1762 loss_train: 0.6507 acc_train: 0.7976 loss_val: 0.5929 acc_val: 0.8175 time: 430.1145s\n",
            "Epoch: 1763 loss_train: 0.6328 acc_train: 0.8029 loss_val: 0.5933 acc_val: 0.8175 time: 430.3650s\n",
            "Epoch: 1764 loss_train: 0.6517 acc_train: 0.7958 loss_val: 0.5920 acc_val: 0.8183 time: 430.6135s\n",
            "Epoch: 1765 loss_train: 0.6472 acc_train: 0.7924 loss_val: 0.5895 acc_val: 0.8221 time: 430.8741s\n",
            "Epoch: 1766 loss_train: 0.6510 acc_train: 0.7993 loss_val: 0.5869 acc_val: 0.8246 time: 431.1267s\n",
            "Epoch: 1767 loss_train: 0.6450 acc_train: 0.8007 loss_val: 0.6050 acc_val: 0.8221 time: 431.3803s\n",
            "Epoch: 1768 loss_train: 0.6547 acc_train: 0.7986 loss_val: 0.5902 acc_val: 0.8196 time: 431.6249s\n",
            "Epoch: 1769 loss_train: 0.6371 acc_train: 0.8033 loss_val: 0.5916 acc_val: 0.8171 time: 431.8810s\n",
            "Epoch: 1770 loss_train: 0.6337 acc_train: 0.8040 loss_val: 0.6103 acc_val: 0.8154 time: 432.1461s\n",
            "Epoch: 1771 loss_train: 0.6612 acc_train: 0.7953 loss_val: 0.5879 acc_val: 0.8187 time: 432.3493s\n",
            "Epoch: 1772 loss_train: 0.6505 acc_train: 0.7976 loss_val: 0.5946 acc_val: 0.8204 time: 432.5969s\n",
            "Epoch: 1773 loss_train: 0.6360 acc_train: 0.8060 loss_val: 0.6043 acc_val: 0.8213 time: 432.8551s\n",
            "Epoch: 1774 loss_train: 0.6581 acc_train: 0.7974 loss_val: 0.5850 acc_val: 0.8217 time: 433.1098s\n",
            "Epoch: 1775 loss_train: 0.6429 acc_train: 0.8013 loss_val: 0.5987 acc_val: 0.8163 time: 433.3659s\n",
            "Epoch: 1776 loss_train: 0.6502 acc_train: 0.8004 loss_val: 0.6071 acc_val: 0.8150 time: 433.6188s\n",
            "Epoch: 1777 loss_train: 0.6535 acc_train: 0.7960 loss_val: 0.5907 acc_val: 0.8167 time: 433.8773s\n",
            "Epoch: 1778 loss_train: 0.6367 acc_train: 0.8042 loss_val: 0.5914 acc_val: 0.8187 time: 434.1305s\n",
            "Epoch: 1779 loss_train: 0.6527 acc_train: 0.7974 loss_val: 0.5853 acc_val: 0.8217 time: 434.3790s\n",
            "Epoch: 1780 loss_train: 0.6325 acc_train: 0.8023 loss_val: 0.5878 acc_val: 0.8242 time: 434.6376s\n",
            "Epoch: 1781 loss_train: 0.6429 acc_train: 0.8008 loss_val: 0.5898 acc_val: 0.8200 time: 434.8817s\n",
            "Epoch: 1782 loss_train: 0.6395 acc_train: 0.8038 loss_val: 0.5884 acc_val: 0.8179 time: 435.1397s\n",
            "Epoch: 1783 loss_train: 0.6290 acc_train: 0.8031 loss_val: 0.5899 acc_val: 0.8175 time: 435.3870s\n",
            "Epoch: 1784 loss_train: 0.6343 acc_train: 0.8044 loss_val: 0.5848 acc_val: 0.8171 time: 435.6424s\n",
            "Epoch: 1785 loss_train: 0.6409 acc_train: 0.7991 loss_val: 0.5887 acc_val: 0.8187 time: 435.8958s\n",
            "Epoch: 1786 loss_train: 0.6537 acc_train: 0.8009 loss_val: 0.6003 acc_val: 0.8200 time: 436.1537s\n",
            "Epoch: 1787 loss_train: 0.6524 acc_train: 0.7941 loss_val: 0.5818 acc_val: 0.8229 time: 436.4022s\n",
            "Epoch: 1788 loss_train: 0.6296 acc_train: 0.8042 loss_val: 0.5790 acc_val: 0.8233 time: 436.6565s\n",
            "Epoch: 1789 loss_train: 0.6389 acc_train: 0.7957 loss_val: 0.5803 acc_val: 0.8237 time: 436.9177s\n",
            "Epoch: 1790 loss_train: 0.6350 acc_train: 0.8041 loss_val: 0.5835 acc_val: 0.8208 time: 437.1625s\n",
            "Epoch: 1791 loss_train: 0.6379 acc_train: 0.7969 loss_val: 0.5887 acc_val: 0.8221 time: 437.4140s\n",
            "Epoch: 1792 loss_train: 0.6609 acc_train: 0.7940 loss_val: 0.5929 acc_val: 0.8208 time: 437.6595s\n",
            "Epoch: 1793 loss_train: 0.6378 acc_train: 0.7982 loss_val: 0.5785 acc_val: 0.8250 time: 437.9046s\n",
            "Epoch: 1794 loss_train: 0.6275 acc_train: 0.8049 loss_val: 0.5918 acc_val: 0.8221 time: 438.1581s\n",
            "Epoch: 1795 loss_train: 0.6560 acc_train: 0.7996 loss_val: 0.5771 acc_val: 0.8242 time: 438.4107s\n",
            "Epoch: 1796 loss_train: 0.6317 acc_train: 0.8010 loss_val: 0.5836 acc_val: 0.8217 time: 438.6586s\n",
            "Epoch: 1797 loss_train: 0.6172 acc_train: 0.8089 loss_val: 0.5783 acc_val: 0.8237 time: 438.9065s\n",
            "Epoch: 1798 loss_train: 0.6266 acc_train: 0.8053 loss_val: 0.5842 acc_val: 0.8246 time: 439.1679s\n",
            "Epoch: 1799 loss_train: 0.6389 acc_train: 0.8065 loss_val: 0.5790 acc_val: 0.8233 time: 439.4158s\n",
            "Epoch: 1800 loss_train: 0.6286 acc_train: 0.8076 loss_val: 0.5805 acc_val: 0.8200 time: 439.6087s\n",
            "Epoch: 1801 loss_train: 0.6261 acc_train: 0.8063 loss_val: 0.5824 acc_val: 0.8204 time: 439.8491s\n",
            "Epoch: 1802 loss_train: 0.6276 acc_train: 0.8014 loss_val: 0.5764 acc_val: 0.8233 time: 440.0817s\n",
            "Epoch: 1803 loss_train: 0.6167 acc_train: 0.8081 loss_val: 0.5787 acc_val: 0.8237 time: 440.3281s\n",
            "Epoch: 1804 loss_train: 0.6341 acc_train: 0.8032 loss_val: 0.5832 acc_val: 0.8233 time: 440.5817s\n",
            "Epoch: 1805 loss_train: 0.6299 acc_train: 0.8041 loss_val: 0.5806 acc_val: 0.8192 time: 440.8249s\n",
            "Epoch: 1806 loss_train: 0.6373 acc_train: 0.7993 loss_val: 0.5902 acc_val: 0.8192 time: 441.0800s\n",
            "Epoch: 1807 loss_train: 0.6320 acc_train: 0.8048 loss_val: 0.5889 acc_val: 0.8242 time: 441.3306s\n",
            "Epoch: 1808 loss_train: 0.6383 acc_train: 0.7986 loss_val: 0.5839 acc_val: 0.8175 time: 441.5794s\n",
            "Epoch: 1809 loss_train: 0.6233 acc_train: 0.8060 loss_val: 0.5854 acc_val: 0.8167 time: 441.8195s\n",
            "Epoch: 1810 loss_train: 0.6271 acc_train: 0.7991 loss_val: 0.5730 acc_val: 0.8233 time: 442.0746s\n",
            "Epoch: 1811 loss_train: 0.6348 acc_train: 0.8012 loss_val: 0.5780 acc_val: 0.8258 time: 442.3319s\n",
            "Epoch: 1812 loss_train: 0.6432 acc_train: 0.7963 loss_val: 0.5702 acc_val: 0.8250 time: 442.5799s\n",
            "Epoch: 1813 loss_train: 0.6278 acc_train: 0.8090 loss_val: 0.5771 acc_val: 0.8221 time: 442.8257s\n",
            "Epoch: 1814 loss_train: 0.6390 acc_train: 0.7957 loss_val: 0.5699 acc_val: 0.8263 time: 443.0737s\n",
            "Epoch: 1815 loss_train: 0.6214 acc_train: 0.8084 loss_val: 0.5771 acc_val: 0.8258 time: 443.3205s\n",
            "Epoch: 1816 loss_train: 0.6333 acc_train: 0.8048 loss_val: 0.5713 acc_val: 0.8250 time: 443.5757s\n",
            "Epoch: 1817 loss_train: 0.6065 acc_train: 0.8085 loss_val: 0.5801 acc_val: 0.8192 time: 443.8264s\n",
            "Epoch: 1818 loss_train: 0.6254 acc_train: 0.8058 loss_val: 0.5738 acc_val: 0.8221 time: 444.0762s\n",
            "Epoch: 1819 loss_train: 0.6208 acc_train: 0.8073 loss_val: 0.5763 acc_val: 0.8233 time: 444.3255s\n",
            "Epoch: 1820 loss_train: 0.6187 acc_train: 0.8090 loss_val: 0.5703 acc_val: 0.8254 time: 444.5737s\n",
            "Epoch: 1821 loss_train: 0.6281 acc_train: 0.8054 loss_val: 0.5842 acc_val: 0.8167 time: 444.8314s\n",
            "Epoch: 1822 loss_train: 0.6435 acc_train: 0.7976 loss_val: 0.5690 acc_val: 0.8263 time: 445.0742s\n",
            "Epoch: 1823 loss_train: 0.6243 acc_train: 0.8077 loss_val: 0.5711 acc_val: 0.8300 time: 445.3266s\n",
            "Epoch: 1824 loss_train: 0.6231 acc_train: 0.8062 loss_val: 0.5678 acc_val: 0.8283 time: 445.5713s\n",
            "Epoch: 1825 loss_train: 0.6123 acc_train: 0.8107 loss_val: 0.5725 acc_val: 0.8229 time: 445.7567s\n",
            "Epoch: 1826 loss_train: 0.6239 acc_train: 0.8019 loss_val: 0.5693 acc_val: 0.8237 time: 445.9875s\n",
            "Epoch: 1827 loss_train: 0.6138 acc_train: 0.8045 loss_val: 0.5721 acc_val: 0.8254 time: 446.2469s\n",
            "Epoch: 1828 loss_train: 0.6257 acc_train: 0.8013 loss_val: 0.5674 acc_val: 0.8250 time: 446.4933s\n",
            "Epoch: 1829 loss_train: 0.6159 acc_train: 0.8057 loss_val: 0.5712 acc_val: 0.8250 time: 446.7424s\n",
            "Epoch: 1830 loss_train: 0.6188 acc_train: 0.8055 loss_val: 0.5661 acc_val: 0.8246 time: 446.9522s\n",
            "Epoch: 1831 loss_train: 0.6246 acc_train: 0.8046 loss_val: 0.5739 acc_val: 0.8292 time: 447.2028s\n",
            "Epoch: 1832 loss_train: 0.6123 acc_train: 0.8085 loss_val: 0.5675 acc_val: 0.8271 time: 447.4291s\n",
            "Epoch: 1833 loss_train: 0.6163 acc_train: 0.8074 loss_val: 0.5814 acc_val: 0.8167 time: 447.6666s\n",
            "Epoch: 1834 loss_train: 0.6443 acc_train: 0.8001 loss_val: 0.5714 acc_val: 0.8246 time: 447.9133s\n",
            "Epoch: 1835 loss_train: 0.6163 acc_train: 0.8054 loss_val: 0.5660 acc_val: 0.8313 time: 448.1327s\n",
            "Epoch: 1836 loss_train: 0.6172 acc_train: 0.8058 loss_val: 0.5696 acc_val: 0.8275 time: 448.3587s\n",
            "Epoch: 1837 loss_train: 0.6244 acc_train: 0.8118 loss_val: 0.5919 acc_val: 0.8213 time: 448.6054s\n",
            "Epoch: 1838 loss_train: 0.6205 acc_train: 0.8065 loss_val: 0.5704 acc_val: 0.8246 time: 448.8540s\n",
            "Epoch: 1839 loss_train: 0.6042 acc_train: 0.8085 loss_val: 0.5817 acc_val: 0.8246 time: 449.1005s\n",
            "Epoch: 1840 loss_train: 0.6431 acc_train: 0.8036 loss_val: 0.5990 acc_val: 0.8246 time: 449.3506s\n",
            "Epoch: 1841 loss_train: 0.6428 acc_train: 0.7970 loss_val: 0.5807 acc_val: 0.8171 time: 449.5966s\n",
            "Epoch: 1842 loss_train: 0.6278 acc_train: 0.7973 loss_val: 0.5910 acc_val: 0.8250 time: 449.8572s\n",
            "Epoch: 1843 loss_train: 0.6427 acc_train: 0.8045 loss_val: 0.6675 acc_val: 0.8117 time: 450.1045s\n",
            "Epoch: 1844 loss_train: 0.7071 acc_train: 0.7851 loss_val: 0.5734 acc_val: 0.8258 time: 450.3518s\n",
            "Epoch: 1845 loss_train: 0.6261 acc_train: 0.8049 loss_val: 0.6002 acc_val: 0.8246 time: 450.6030s\n",
            "Epoch: 1846 loss_train: 0.6583 acc_train: 0.8007 loss_val: 0.6784 acc_val: 0.8017 time: 450.8499s\n",
            "Epoch: 1847 loss_train: 0.7047 acc_train: 0.7835 loss_val: 0.5683 acc_val: 0.8237 time: 451.1055s\n",
            "Epoch: 1848 loss_train: 0.6195 acc_train: 0.8016 loss_val: 0.6323 acc_val: 0.8125 time: 451.3667s\n",
            "Epoch: 1849 loss_train: 0.6902 acc_train: 0.7884 loss_val: 0.5998 acc_val: 0.8213 time: 451.6315s\n",
            "Epoch: 1850 loss_train: 0.6361 acc_train: 0.8003 loss_val: 0.5889 acc_val: 0.8187 time: 451.8941s\n",
            "Epoch: 1851 loss_train: 0.6223 acc_train: 0.7973 loss_val: 0.6221 acc_val: 0.8142 time: 452.1418s\n",
            "Epoch: 1852 loss_train: 0.6775 acc_train: 0.7922 loss_val: 0.5753 acc_val: 0.8254 time: 452.3992s\n",
            "Epoch: 1853 loss_train: 0.6181 acc_train: 0.8033 loss_val: 0.6168 acc_val: 0.8208 time: 452.6453s\n",
            "Epoch: 1854 loss_train: 0.6674 acc_train: 0.7925 loss_val: 0.5933 acc_val: 0.8233 time: 452.8946s\n",
            "Epoch: 1855 loss_train: 0.6486 acc_train: 0.7990 loss_val: 0.5856 acc_val: 0.8275 time: 453.1388s\n",
            "Epoch: 1856 loss_train: 0.6450 acc_train: 0.8002 loss_val: 0.6140 acc_val: 0.8237 time: 453.3878s\n",
            "Epoch: 1857 loss_train: 0.6509 acc_train: 0.7970 loss_val: 0.5754 acc_val: 0.8246 time: 453.6374s\n",
            "Epoch: 1858 loss_train: 0.6079 acc_train: 0.8064 loss_val: 0.5990 acc_val: 0.8233 time: 453.8890s\n",
            "Epoch: 1859 loss_train: 0.6450 acc_train: 0.8041 loss_val: 0.5651 acc_val: 0.8254 time: 454.1045s\n",
            "Epoch: 1860 loss_train: 0.6182 acc_train: 0.8038 loss_val: 0.6010 acc_val: 0.8233 time: 454.3665s\n",
            "Epoch: 1861 loss_train: 0.6516 acc_train: 0.7954 loss_val: 0.5681 acc_val: 0.8267 time: 454.6148s\n",
            "Epoch: 1862 loss_train: 0.6122 acc_train: 0.8085 loss_val: 0.5837 acc_val: 0.8287 time: 454.8656s\n",
            "Epoch: 1863 loss_train: 0.6408 acc_train: 0.8010 loss_val: 0.5746 acc_val: 0.8275 time: 455.1141s\n",
            "Epoch: 1864 loss_train: 0.6092 acc_train: 0.8058 loss_val: 0.5767 acc_val: 0.8304 time: 455.3789s\n",
            "Epoch: 1865 loss_train: 0.6232 acc_train: 0.8034 loss_val: 0.5797 acc_val: 0.8325 time: 455.6304s\n",
            "Epoch: 1866 loss_train: 0.6328 acc_train: 0.8101 loss_val: 0.5719 acc_val: 0.8213 time: 455.8829s\n",
            "Epoch: 1867 loss_train: 0.6084 acc_train: 0.8095 loss_val: 0.5991 acc_val: 0.8254 time: 456.1371s\n",
            "Epoch: 1868 loss_train: 0.6453 acc_train: 0.8047 loss_val: 0.5632 acc_val: 0.8337 time: 456.3936s\n",
            "Epoch: 1869 loss_train: 0.6125 acc_train: 0.8085 loss_val: 0.5829 acc_val: 0.8325 time: 456.6515s\n",
            "Epoch: 1870 loss_train: 0.6461 acc_train: 0.8038 loss_val: 0.6125 acc_val: 0.8154 time: 456.9019s\n",
            "Epoch: 1871 loss_train: 0.6479 acc_train: 0.7976 loss_val: 0.5661 acc_val: 0.8279 time: 457.1240s\n",
            "Epoch: 1872 loss_train: 0.6227 acc_train: 0.8041 loss_val: 0.6033 acc_val: 0.8183 time: 457.3783s\n",
            "Epoch: 1873 loss_train: 0.6445 acc_train: 0.8036 loss_val: 0.5646 acc_val: 0.8292 time: 457.6317s\n",
            "Epoch: 1874 loss_train: 0.6211 acc_train: 0.8008 loss_val: 0.6126 acc_val: 0.8142 time: 457.8819s\n",
            "Epoch: 1875 loss_train: 0.6499 acc_train: 0.7962 loss_val: 0.5793 acc_val: 0.8296 time: 458.1240s\n",
            "Epoch: 1876 loss_train: 0.6275 acc_train: 0.8120 loss_val: 0.5623 acc_val: 0.8300 time: 458.3752s\n",
            "Epoch: 1877 loss_train: 0.6246 acc_train: 0.8041 loss_val: 0.6059 acc_val: 0.8204 time: 458.6249s\n",
            "Epoch: 1878 loss_train: 0.6620 acc_train: 0.7945 loss_val: 0.5587 acc_val: 0.8296 time: 458.8326s\n",
            "Epoch: 1879 loss_train: 0.6108 acc_train: 0.8048 loss_val: 0.5753 acc_val: 0.8325 time: 459.0780s\n",
            "Epoch: 1880 loss_train: 0.6352 acc_train: 0.8011 loss_val: 0.5869 acc_val: 0.8179 time: 459.3268s\n",
            "Epoch: 1881 loss_train: 0.6367 acc_train: 0.7960 loss_val: 0.5623 acc_val: 0.8304 time: 459.5868s\n",
            "Epoch: 1882 loss_train: 0.6200 acc_train: 0.8068 loss_val: 0.5780 acc_val: 0.8279 time: 459.8360s\n",
            "Epoch: 1883 loss_train: 0.6277 acc_train: 0.8114 loss_val: 0.5660 acc_val: 0.8254 time: 460.0870s\n",
            "Epoch: 1884 loss_train: 0.5951 acc_train: 0.8108 loss_val: 0.6071 acc_val: 0.8142 time: 460.3525s\n",
            "Epoch: 1885 loss_train: 0.6566 acc_train: 0.7947 loss_val: 0.5709 acc_val: 0.8308 time: 460.6043s\n",
            "Epoch: 1886 loss_train: 0.6094 acc_train: 0.8140 loss_val: 0.5726 acc_val: 0.8258 time: 460.8564s\n",
            "Epoch: 1887 loss_train: 0.6360 acc_train: 0.8044 loss_val: 0.5930 acc_val: 0.8208 time: 461.1149s\n",
            "Epoch: 1888 loss_train: 0.6172 acc_train: 0.7999 loss_val: 0.5730 acc_val: 0.8229 time: 461.3677s\n",
            "Epoch: 1889 loss_train: 0.6223 acc_train: 0.8004 loss_val: 0.5991 acc_val: 0.8250 time: 461.6181s\n",
            "Epoch: 1890 loss_train: 0.6651 acc_train: 0.8032 loss_val: 0.5562 acc_val: 0.8304 time: 461.8639s\n",
            "Epoch: 1891 loss_train: 0.6062 acc_train: 0.8062 loss_val: 0.5866 acc_val: 0.8233 time: 462.1328s\n",
            "Epoch: 1892 loss_train: 0.6529 acc_train: 0.7923 loss_val: 0.5632 acc_val: 0.8342 time: 462.3852s\n",
            "Epoch: 1893 loss_train: 0.6106 acc_train: 0.8165 loss_val: 0.5694 acc_val: 0.8296 time: 462.6409s\n",
            "Epoch: 1894 loss_train: 0.6091 acc_train: 0.8095 loss_val: 0.5783 acc_val: 0.8300 time: 462.8390s\n",
            "Epoch: 1895 loss_train: 0.6350 acc_train: 0.7981 loss_val: 0.5592 acc_val: 0.8300 time: 463.0843s\n",
            "Epoch: 1896 loss_train: 0.6042 acc_train: 0.8071 loss_val: 0.5816 acc_val: 0.8321 time: 463.3309s\n",
            "Epoch: 1897 loss_train: 0.6515 acc_train: 0.8010 loss_val: 0.5663 acc_val: 0.8275 time: 463.5869s\n",
            "Epoch: 1898 loss_train: 0.6172 acc_train: 0.8016 loss_val: 0.5778 acc_val: 0.8279 time: 463.8377s\n",
            "Epoch: 1899 loss_train: 0.6424 acc_train: 0.7960 loss_val: 0.5803 acc_val: 0.8308 time: 464.0942s\n",
            "Epoch: 1900 loss_train: 0.6341 acc_train: 0.8047 loss_val: 0.5557 acc_val: 0.8275 time: 464.3452s\n",
            "Epoch: 1901 loss_train: 0.6066 acc_train: 0.8102 loss_val: 0.5720 acc_val: 0.8258 time: 464.5932s\n",
            "Epoch: 1902 loss_train: 0.6156 acc_train: 0.8000 loss_val: 0.5553 acc_val: 0.8337 time: 464.8462s\n",
            "Epoch: 1903 loss_train: 0.5949 acc_train: 0.8102 loss_val: 0.5588 acc_val: 0.8321 time: 465.1182s\n",
            "Epoch: 1904 loss_train: 0.6143 acc_train: 0.8043 loss_val: 0.5680 acc_val: 0.8183 time: 465.3670s\n",
            "Epoch: 1905 loss_train: 0.6162 acc_train: 0.8045 loss_val: 0.5547 acc_val: 0.8296 time: 465.5969s\n",
            "Epoch: 1906 loss_train: 0.5929 acc_train: 0.8143 loss_val: 0.5521 acc_val: 0.8337 time: 465.8423s\n",
            "Epoch: 1907 loss_train: 0.6061 acc_train: 0.8093 loss_val: 0.5524 acc_val: 0.8304 time: 466.0950s\n",
            "Epoch: 1908 loss_train: 0.6040 acc_train: 0.8090 loss_val: 0.5520 acc_val: 0.8292 time: 466.3437s\n",
            "Epoch: 1909 loss_train: 0.6024 acc_train: 0.8060 loss_val: 0.5566 acc_val: 0.8329 time: 466.5999s\n",
            "Epoch: 1910 loss_train: 0.6169 acc_train: 0.8108 loss_val: 0.5650 acc_val: 0.8279 time: 466.8484s\n",
            "Epoch: 1911 loss_train: 0.6071 acc_train: 0.8030 loss_val: 0.5554 acc_val: 0.8263 time: 467.1054s\n",
            "Epoch: 1912 loss_train: 0.5980 acc_train: 0.8141 loss_val: 0.5558 acc_val: 0.8304 time: 467.3557s\n",
            "Epoch: 1913 loss_train: 0.6050 acc_train: 0.8125 loss_val: 0.5529 acc_val: 0.8337 time: 467.5811s\n",
            "Epoch: 1914 loss_train: 0.6109 acc_train: 0.8027 loss_val: 0.5499 acc_val: 0.8350 time: 467.8155s\n",
            "Epoch: 1915 loss_train: 0.5939 acc_train: 0.8127 loss_val: 0.5632 acc_val: 0.8200 time: 468.0659s\n",
            "Epoch: 1916 loss_train: 0.6167 acc_train: 0.8020 loss_val: 0.5536 acc_val: 0.8296 time: 468.3129s\n",
            "Epoch: 1917 loss_train: 0.5982 acc_train: 0.8115 loss_val: 0.5541 acc_val: 0.8342 time: 468.5681s\n",
            "Epoch: 1918 loss_train: 0.6030 acc_train: 0.8153 loss_val: 0.5675 acc_val: 0.8279 time: 468.8014s\n",
            "Epoch: 1919 loss_train: 0.6067 acc_train: 0.8080 loss_val: 0.5581 acc_val: 0.8225 time: 469.0646s\n",
            "Epoch: 1920 loss_train: 0.5901 acc_train: 0.8096 loss_val: 0.5671 acc_val: 0.8342 time: 469.3200s\n",
            "Epoch: 1921 loss_train: 0.6247 acc_train: 0.8124 loss_val: 0.5606 acc_val: 0.8283 time: 469.5697s\n",
            "Epoch: 1922 loss_train: 0.6139 acc_train: 0.8003 loss_val: 0.5665 acc_val: 0.8258 time: 469.7916s\n",
            "Epoch: 1923 loss_train: 0.6174 acc_train: 0.7997 loss_val: 0.5743 acc_val: 0.8317 time: 470.0325s\n",
            "Epoch: 1924 loss_train: 0.6251 acc_train: 0.8132 loss_val: 0.5515 acc_val: 0.8287 time: 470.2768s\n",
            "Epoch: 1925 loss_train: 0.6063 acc_train: 0.8084 loss_val: 0.5912 acc_val: 0.8258 time: 470.5314s\n",
            "Epoch: 1926 loss_train: 0.6424 acc_train: 0.8002 loss_val: 0.5497 acc_val: 0.8346 time: 470.7829s\n",
            "Epoch: 1927 loss_train: 0.6067 acc_train: 0.8081 loss_val: 0.5661 acc_val: 0.8354 time: 471.0312s\n",
            "Epoch: 1928 loss_train: 0.6153 acc_train: 0.8185 loss_val: 0.5737 acc_val: 0.8213 time: 471.2134s\n",
            "Epoch: 1929 loss_train: 0.6105 acc_train: 0.8086 loss_val: 0.5653 acc_val: 0.8296 time: 471.4262s\n",
            "Epoch: 1930 loss_train: 0.6161 acc_train: 0.8026 loss_val: 0.5588 acc_val: 0.8337 time: 471.6816s\n",
            "Epoch: 1931 loss_train: 0.6005 acc_train: 0.8151 loss_val: 0.5505 acc_val: 0.8317 time: 471.9257s\n",
            "Epoch: 1932 loss_train: 0.5900 acc_train: 0.8159 loss_val: 0.5647 acc_val: 0.8242 time: 472.1761s\n",
            "Epoch: 1933 loss_train: 0.6111 acc_train: 0.8102 loss_val: 0.5487 acc_val: 0.8354 time: 472.3803s\n",
            "Epoch: 1934 loss_train: 0.5993 acc_train: 0.8115 loss_val: 0.5502 acc_val: 0.8342 time: 472.6311s\n",
            "Epoch: 1935 loss_train: 0.5980 acc_train: 0.8116 loss_val: 0.5574 acc_val: 0.8300 time: 472.8236s\n",
            "Epoch: 1936 loss_train: 0.6046 acc_train: 0.8087 loss_val: 0.5507 acc_val: 0.8271 time: 473.0795s\n",
            "Epoch: 1937 loss_train: 0.6016 acc_train: 0.8092 loss_val: 0.5470 acc_val: 0.8350 time: 473.3220s\n",
            "Epoch: 1938 loss_train: 0.5980 acc_train: 0.8131 loss_val: 0.5508 acc_val: 0.8337 time: 473.5695s\n",
            "Epoch: 1939 loss_train: 0.6022 acc_train: 0.8105 loss_val: 0.5482 acc_val: 0.8346 time: 473.8162s\n",
            "Epoch: 1940 loss_train: 0.5875 acc_train: 0.8196 loss_val: 0.5495 acc_val: 0.8287 time: 474.0661s\n",
            "Epoch: 1941 loss_train: 0.5808 acc_train: 0.8122 loss_val: 0.5508 acc_val: 0.8325 time: 474.3144s\n",
            "Epoch: 1942 loss_train: 0.5919 acc_train: 0.8116 loss_val: 0.5525 acc_val: 0.8271 time: 474.5639s\n",
            "Epoch: 1943 loss_train: 0.5880 acc_train: 0.8144 loss_val: 0.5507 acc_val: 0.8254 time: 474.8090s\n",
            "Epoch: 1944 loss_train: 0.5879 acc_train: 0.8100 loss_val: 0.5486 acc_val: 0.8363 time: 475.0609s\n",
            "Epoch: 1945 loss_train: 0.5956 acc_train: 0.8134 loss_val: 0.5508 acc_val: 0.8325 time: 475.3101s\n",
            "Epoch: 1946 loss_train: 0.5991 acc_train: 0.8130 loss_val: 0.5475 acc_val: 0.8271 time: 475.5711s\n",
            "Epoch: 1947 loss_train: 0.6107 acc_train: 0.7992 loss_val: 0.5530 acc_val: 0.8329 time: 475.8161s\n",
            "Epoch: 1948 loss_train: 0.6043 acc_train: 0.8143 loss_val: 0.5518 acc_val: 0.8279 time: 476.0645s\n",
            "Epoch: 1949 loss_train: 0.5907 acc_train: 0.8130 loss_val: 0.5533 acc_val: 0.8296 time: 476.3160s\n",
            "Epoch: 1950 loss_train: 0.5954 acc_train: 0.8148 loss_val: 0.5454 acc_val: 0.8354 time: 476.5637s\n",
            "Epoch: 1951 loss_train: 0.5978 acc_train: 0.8165 loss_val: 0.5453 acc_val: 0.8313 time: 476.8106s\n",
            "Epoch: 1952 loss_train: 0.6010 acc_train: 0.8065 loss_val: 0.5444 acc_val: 0.8337 time: 477.0560s\n",
            "Epoch: 1953 loss_train: 0.5994 acc_train: 0.8148 loss_val: 0.5506 acc_val: 0.8263 time: 477.3059s\n",
            "Epoch: 1954 loss_train: 0.5949 acc_train: 0.8103 loss_val: 0.5494 acc_val: 0.8321 time: 477.5573s\n",
            "Epoch: 1955 loss_train: 0.5830 acc_train: 0.8105 loss_val: 0.5456 acc_val: 0.8354 time: 477.8103s\n",
            "Epoch: 1956 loss_train: 0.5927 acc_train: 0.8153 loss_val: 0.5441 acc_val: 0.8313 time: 478.0643s\n",
            "Epoch: 1957 loss_train: 0.5798 acc_train: 0.8146 loss_val: 0.5506 acc_val: 0.8263 time: 478.3155s\n",
            "Epoch: 1958 loss_train: 0.5808 acc_train: 0.8158 loss_val: 0.5438 acc_val: 0.8313 time: 478.5628s\n",
            "Epoch: 1959 loss_train: 0.5985 acc_train: 0.8102 loss_val: 0.5566 acc_val: 0.8404 time: 478.8149s\n",
            "Epoch: 1960 loss_train: 0.6198 acc_train: 0.8132 loss_val: 0.5415 acc_val: 0.8404 time: 479.0702s\n",
            "Epoch: 1961 loss_train: 0.5854 acc_train: 0.8192 loss_val: 0.5642 acc_val: 0.8192 time: 479.3029s\n",
            "Epoch: 1962 loss_train: 0.5959 acc_train: 0.8112 loss_val: 0.5405 acc_val: 0.8367 time: 479.5440s\n",
            "Epoch: 1963 loss_train: 0.5728 acc_train: 0.8266 loss_val: 0.5443 acc_val: 0.8387 time: 479.7966s\n",
            "Epoch: 1964 loss_train: 0.5933 acc_train: 0.8142 loss_val: 0.5391 acc_val: 0.8421 time: 480.0433s\n",
            "Epoch: 1965 loss_train: 0.5956 acc_train: 0.8133 loss_val: 0.5464 acc_val: 0.8387 time: 480.2946s\n",
            "Epoch: 1966 loss_train: 0.5962 acc_train: 0.8256 loss_val: 0.5314 acc_val: 0.8367 time: 480.5455s\n",
            "Epoch: 1967 loss_train: 0.5652 acc_train: 0.8251 loss_val: 0.5388 acc_val: 0.8404 time: 480.7952s\n",
            "Epoch: 1968 loss_train: 0.5801 acc_train: 0.8197 loss_val: 0.5212 acc_val: 0.8500 time: 481.0444s\n",
            "Epoch: 1969 loss_train: 0.5673 acc_train: 0.8280 loss_val: 0.5242 acc_val: 0.8492 time: 481.2992s\n",
            "Epoch: 1970 loss_train: 0.5822 acc_train: 0.8257 loss_val: 0.5172 acc_val: 0.8421 time: 481.5647s\n",
            "Epoch: 1971 loss_train: 0.5591 acc_train: 0.8292 loss_val: 0.5147 acc_val: 0.8462 time: 481.8173s\n",
            "Epoch: 1972 loss_train: 0.5710 acc_train: 0.8237 loss_val: 0.5053 acc_val: 0.8554 time: 482.0266s\n",
            "Epoch: 1973 loss_train: 0.5573 acc_train: 0.8353 loss_val: 0.5010 acc_val: 0.8496 time: 482.2723s\n",
            "Epoch: 1974 loss_train: 0.5446 acc_train: 0.8348 loss_val: 0.5060 acc_val: 0.8454 time: 482.5227s\n",
            "Epoch: 1975 loss_train: 0.5526 acc_train: 0.8235 loss_val: 0.4914 acc_val: 0.8588 time: 482.7793s\n",
            "Epoch: 1976 loss_train: 0.5482 acc_train: 0.8347 loss_val: 0.4931 acc_val: 0.8567 time: 483.0500s\n",
            "Epoch: 1977 loss_train: 0.5485 acc_train: 0.8280 loss_val: 0.4916 acc_val: 0.8592 time: 483.2926s\n",
            "Epoch: 1978 loss_train: 0.5379 acc_train: 0.8386 loss_val: 0.4964 acc_val: 0.8479 time: 483.5389s\n",
            "Epoch: 1979 loss_train: 0.5319 acc_train: 0.8354 loss_val: 0.4783 acc_val: 0.8650 time: 483.7895s\n",
            "Epoch: 1980 loss_train: 0.5282 acc_train: 0.8431 loss_val: 0.4810 acc_val: 0.8629 time: 484.0327s\n",
            "Epoch: 1981 loss_train: 0.5359 acc_train: 0.8334 loss_val: 0.4838 acc_val: 0.8550 time: 484.2815s\n",
            "Epoch: 1982 loss_train: 0.5261 acc_train: 0.8387 loss_val: 0.4760 acc_val: 0.8650 time: 484.5241s\n",
            "Epoch: 1983 loss_train: 0.5235 acc_train: 0.8377 loss_val: 0.4762 acc_val: 0.8688 time: 484.7738s\n",
            "Epoch: 1984 loss_train: 0.5224 acc_train: 0.8476 loss_val: 0.4815 acc_val: 0.8562 time: 485.0202s\n",
            "Epoch: 1985 loss_train: 0.5212 acc_train: 0.8359 loss_val: 0.4767 acc_val: 0.8608 time: 485.2710s\n",
            "Epoch: 1986 loss_train: 0.5131 acc_train: 0.8367 loss_val: 0.4645 acc_val: 0.8654 time: 485.5242s\n",
            "Epoch: 1987 loss_train: 0.5063 acc_train: 0.8465 loss_val: 0.4737 acc_val: 0.8679 time: 485.7773s\n",
            "Epoch: 1988 loss_train: 0.5134 acc_train: 0.8440 loss_val: 0.4648 acc_val: 0.8642 time: 486.0213s\n",
            "Epoch: 1989 loss_train: 0.4896 acc_train: 0.8470 loss_val: 0.4676 acc_val: 0.8633 time: 486.2725s\n",
            "Epoch: 1990 loss_train: 0.5070 acc_train: 0.8464 loss_val: 0.4681 acc_val: 0.8625 time: 486.5206s\n",
            "Epoch: 1991 loss_train: 0.5078 acc_train: 0.8416 loss_val: 0.4586 acc_val: 0.8717 time: 486.7859s\n",
            "Epoch: 1992 loss_train: 0.5050 acc_train: 0.8490 loss_val: 0.4532 acc_val: 0.8696 time: 487.0336s\n",
            "Epoch: 1993 loss_train: 0.5045 acc_train: 0.8475 loss_val: 0.4579 acc_val: 0.8617 time: 487.2806s\n",
            "Epoch: 1994 loss_train: 0.5016 acc_train: 0.8443 loss_val: 0.4407 acc_val: 0.8671 time: 487.5262s\n",
            "Epoch: 1995 loss_train: 0.4932 acc_train: 0.8433 loss_val: 0.4472 acc_val: 0.8708 time: 487.7885s\n",
            "Epoch: 1996 loss_train: 0.4958 acc_train: 0.8501 loss_val: 0.4405 acc_val: 0.8783 time: 488.0305s\n",
            "Epoch: 1997 loss_train: 0.4968 acc_train: 0.8503 loss_val: 0.4335 acc_val: 0.8812 time: 488.2804s\n",
            "Epoch: 1998 loss_train: 0.4766 acc_train: 0.8590 loss_val: 0.4441 acc_val: 0.8708 time: 488.5220s\n",
            "Epoch: 1999 loss_train: 0.4886 acc_train: 0.8484 loss_val: 0.4431 acc_val: 0.8654 time: 488.7822s\n",
            "Epoch: 2000 loss_train: 0.4869 acc_train: 0.8427 loss_val: 0.4551 acc_val: 0.8692 time: 489.0298s\n",
            "Epoch: 2001 loss_train: 0.4860 acc_train: 0.8555 loss_val: 0.4298 acc_val: 0.8733 time: 489.2809s\n",
            "Epoch: 2002 loss_train: 0.4932 acc_train: 0.8499 loss_val: 0.4776 acc_val: 0.8650 time: 489.5114s\n",
            "Epoch: 2003 loss_train: 0.5307 acc_train: 0.8410 loss_val: 0.4326 acc_val: 0.8617 time: 489.7719s\n",
            "Epoch: 2004 loss_train: 0.4769 acc_train: 0.8404 loss_val: 0.4709 acc_val: 0.8688 time: 490.0168s\n",
            "Epoch: 2005 loss_train: 0.5373 acc_train: 0.8420 loss_val: 0.4695 acc_val: 0.8621 time: 490.2746s\n",
            "Epoch: 2006 loss_train: 0.4969 acc_train: 0.8415 loss_val: 0.4795 acc_val: 0.8646 time: 490.5207s\n",
            "Epoch: 2007 loss_train: 0.5206 acc_train: 0.8452 loss_val: 0.4629 acc_val: 0.8608 time: 490.7721s\n",
            "Epoch: 2008 loss_train: 0.5289 acc_train: 0.8404 loss_val: 0.4629 acc_val: 0.8650 time: 491.0146s\n",
            "Epoch: 2009 loss_train: 0.4902 acc_train: 0.8500 loss_val: 0.4531 acc_val: 0.8633 time: 491.2621s\n",
            "Epoch: 2010 loss_train: 0.4759 acc_train: 0.8480 loss_val: 0.4500 acc_val: 0.8700 time: 491.5111s\n",
            "Epoch: 2011 loss_train: 0.5029 acc_train: 0.8474 loss_val: 0.4305 acc_val: 0.8708 time: 491.7698s\n",
            "Epoch: 2012 loss_train: 0.4718 acc_train: 0.8488 loss_val: 0.4365 acc_val: 0.8596 time: 491.9999s\n",
            "Epoch: 2013 loss_train: 0.4626 acc_train: 0.8484 loss_val: 0.4376 acc_val: 0.8721 time: 492.2477s\n",
            "Epoch: 2014 loss_train: 0.4784 acc_train: 0.8558 loss_val: 0.4213 acc_val: 0.8788 time: 492.5024s\n",
            "Epoch: 2015 loss_train: 0.4690 acc_train: 0.8537 loss_val: 0.4333 acc_val: 0.8704 time: 492.7598s\n",
            "Epoch: 2016 loss_train: 0.4784 acc_train: 0.8493 loss_val: 0.4296 acc_val: 0.8708 time: 493.0049s\n",
            "Epoch: 2017 loss_train: 0.4580 acc_train: 0.8520 loss_val: 0.4173 acc_val: 0.8838 time: 493.2602s\n",
            "Epoch: 2018 loss_train: 0.4568 acc_train: 0.8607 loss_val: 0.4223 acc_val: 0.8796 time: 493.5077s\n",
            "Epoch: 2019 loss_train: 0.4745 acc_train: 0.8571 loss_val: 0.4152 acc_val: 0.8800 time: 493.7659s\n",
            "Epoch: 2020 loss_train: 0.4455 acc_train: 0.8611 loss_val: 0.4228 acc_val: 0.8825 time: 494.0207s\n",
            "Epoch: 2021 loss_train: 0.4573 acc_train: 0.8626 loss_val: 0.4156 acc_val: 0.8821 time: 494.2835s\n",
            "Epoch: 2022 loss_train: 0.4583 acc_train: 0.8636 loss_val: 0.4079 acc_val: 0.8842 time: 494.5462s\n",
            "Epoch: 2023 loss_train: 0.4454 acc_train: 0.8633 loss_val: 0.4088 acc_val: 0.8808 time: 494.7983s\n",
            "Epoch: 2024 loss_train: 0.4559 acc_train: 0.8586 loss_val: 0.4108 acc_val: 0.8788 time: 495.0446s\n",
            "Epoch: 2025 loss_train: 0.4470 acc_train: 0.8569 loss_val: 0.4157 acc_val: 0.8862 time: 495.2715s\n",
            "Epoch: 2026 loss_train: 0.4442 acc_train: 0.8699 loss_val: 0.4064 acc_val: 0.8792 time: 495.5129s\n",
            "Epoch: 2027 loss_train: 0.4430 acc_train: 0.8576 loss_val: 0.4043 acc_val: 0.8792 time: 495.7779s\n",
            "Epoch: 2028 loss_train: 0.4395 acc_train: 0.8576 loss_val: 0.4033 acc_val: 0.8796 time: 496.0201s\n",
            "Epoch: 2029 loss_train: 0.4475 acc_train: 0.8597 loss_val: 0.4160 acc_val: 0.8850 time: 496.2696s\n",
            "Epoch: 2030 loss_train: 0.4394 acc_train: 0.8671 loss_val: 0.4049 acc_val: 0.8875 time: 496.5232s\n",
            "Epoch: 2031 loss_train: 0.4365 acc_train: 0.8687 loss_val: 0.4055 acc_val: 0.8871 time: 496.7813s\n",
            "Epoch: 2032 loss_train: 0.4323 acc_train: 0.8696 loss_val: 0.4089 acc_val: 0.8829 time: 497.0318s\n",
            "Epoch: 2033 loss_train: 0.4407 acc_train: 0.8655 loss_val: 0.4056 acc_val: 0.8875 time: 497.2822s\n",
            "Epoch: 2034 loss_train: 0.4439 acc_train: 0.8629 loss_val: 0.4198 acc_val: 0.8858 time: 497.5279s\n",
            "Epoch: 2035 loss_train: 0.4592 acc_train: 0.8667 loss_val: 0.4083 acc_val: 0.8812 time: 497.7838s\n",
            "Epoch: 2036 loss_train: 0.4352 acc_train: 0.8616 loss_val: 0.4139 acc_val: 0.8771 time: 498.0371s\n",
            "Epoch: 2037 loss_train: 0.4483 acc_train: 0.8607 loss_val: 0.4036 acc_val: 0.8862 time: 498.2981s\n",
            "Epoch: 2038 loss_train: 0.4511 acc_train: 0.8660 loss_val: 0.4004 acc_val: 0.8900 time: 498.5485s\n",
            "Epoch: 2039 loss_train: 0.4420 acc_train: 0.8692 loss_val: 0.4092 acc_val: 0.8762 time: 498.7981s\n",
            "Epoch: 2040 loss_train: 0.4451 acc_train: 0.8570 loss_val: 0.4004 acc_val: 0.8858 time: 499.0512s\n",
            "Epoch: 2041 loss_train: 0.4389 acc_train: 0.8631 loss_val: 0.4011 acc_val: 0.8892 time: 499.2757s\n",
            "Epoch: 2042 loss_train: 0.4461 acc_train: 0.8697 loss_val: 0.4166 acc_val: 0.8829 time: 499.5042s\n",
            "Epoch: 2043 loss_train: 0.4474 acc_train: 0.8641 loss_val: 0.3977 acc_val: 0.8858 time: 499.7530s\n",
            "Epoch: 2044 loss_train: 0.4262 acc_train: 0.8711 loss_val: 0.3978 acc_val: 0.8912 time: 500.0032s\n",
            "Epoch: 2045 loss_train: 0.4281 acc_train: 0.8748 loss_val: 0.4087 acc_val: 0.8846 time: 500.2580s\n",
            "Epoch: 2046 loss_train: 0.4309 acc_train: 0.8641 loss_val: 0.4051 acc_val: 0.8850 time: 500.5077s\n",
            "Epoch: 2047 loss_train: 0.4262 acc_train: 0.8682 loss_val: 0.3958 acc_val: 0.8883 time: 500.7540s\n",
            "Epoch: 2048 loss_train: 0.4281 acc_train: 0.8704 loss_val: 0.3924 acc_val: 0.8892 time: 501.0049s\n",
            "Epoch: 2049 loss_train: 0.4155 acc_train: 0.8721 loss_val: 0.4136 acc_val: 0.8854 time: 501.2637s\n",
            "Epoch: 2050 loss_train: 0.4292 acc_train: 0.8705 loss_val: 0.3936 acc_val: 0.8871 time: 501.5129s\n",
            "Epoch: 2051 loss_train: 0.4146 acc_train: 0.8740 loss_val: 0.3957 acc_val: 0.8854 time: 501.7563s\n",
            "Epoch: 2052 loss_train: 0.4184 acc_train: 0.8730 loss_val: 0.3984 acc_val: 0.8867 time: 502.0043s\n",
            "Epoch: 2053 loss_train: 0.4171 acc_train: 0.8723 loss_val: 0.4006 acc_val: 0.8879 time: 502.2578s\n",
            "Epoch: 2054 loss_train: 0.4240 acc_train: 0.8742 loss_val: 0.3949 acc_val: 0.8850 time: 502.4756s\n",
            "Epoch: 2055 loss_train: 0.4348 acc_train: 0.8631 loss_val: 0.3960 acc_val: 0.8862 time: 502.7269s\n",
            "Epoch: 2056 loss_train: 0.4268 acc_train: 0.8673 loss_val: 0.3944 acc_val: 0.8867 time: 502.9669s\n",
            "Epoch: 2057 loss_train: 0.4353 acc_train: 0.8718 loss_val: 0.4194 acc_val: 0.8783 time: 503.1442s\n",
            "Epoch: 2058 loss_train: 0.4530 acc_train: 0.8592 loss_val: 0.4048 acc_val: 0.8762 time: 503.3787s\n",
            "Epoch: 2059 loss_train: 0.4262 acc_train: 0.8643 loss_val: 0.3965 acc_val: 0.8862 time: 503.5859s\n",
            "Epoch: 2060 loss_train: 0.4219 acc_train: 0.8735 loss_val: 0.4024 acc_val: 0.8854 time: 503.8414s\n",
            "Epoch: 2061 loss_train: 0.4355 acc_train: 0.8743 loss_val: 0.3992 acc_val: 0.8904 time: 504.0667s\n",
            "Epoch: 2062 loss_train: 0.4226 acc_train: 0.8723 loss_val: 0.3992 acc_val: 0.8829 time: 504.2410s\n",
            "Epoch: 2063 loss_train: 0.4277 acc_train: 0.8621 loss_val: 0.3894 acc_val: 0.8896 time: 504.4753s\n",
            "Epoch: 2064 loss_train: 0.4310 acc_train: 0.8696 loss_val: 0.3949 acc_val: 0.8875 time: 504.7210s\n",
            "Epoch: 2065 loss_train: 0.4119 acc_train: 0.8816 loss_val: 0.3964 acc_val: 0.8908 time: 504.9696s\n",
            "Epoch: 2066 loss_train: 0.4133 acc_train: 0.8747 loss_val: 0.3983 acc_val: 0.8854 time: 505.2202s\n",
            "Epoch: 2067 loss_train: 0.4193 acc_train: 0.8732 loss_val: 0.3969 acc_val: 0.8833 time: 505.4689s\n",
            "Epoch: 2068 loss_train: 0.4292 acc_train: 0.8657 loss_val: 0.3941 acc_val: 0.8904 time: 505.6835s\n",
            "Epoch: 2069 loss_train: 0.4258 acc_train: 0.8708 loss_val: 0.3999 acc_val: 0.8892 time: 505.9409s\n",
            "Epoch: 2070 loss_train: 0.4210 acc_train: 0.8701 loss_val: 0.3947 acc_val: 0.8900 time: 506.1862s\n",
            "Epoch: 2071 loss_train: 0.4228 acc_train: 0.8691 loss_val: 0.3886 acc_val: 0.8892 time: 506.4405s\n",
            "Epoch: 2072 loss_train: 0.4142 acc_train: 0.8733 loss_val: 0.3873 acc_val: 0.8883 time: 506.6904s\n",
            "Epoch: 2073 loss_train: 0.4054 acc_train: 0.8766 loss_val: 0.3912 acc_val: 0.8904 time: 506.9449s\n",
            "Epoch: 2074 loss_train: 0.4077 acc_train: 0.8755 loss_val: 0.3909 acc_val: 0.8896 time: 507.1325s\n",
            "Epoch: 2075 loss_train: 0.4089 acc_train: 0.8747 loss_val: 0.3868 acc_val: 0.8867 time: 507.3687s\n",
            "Epoch: 2076 loss_train: 0.4156 acc_train: 0.8752 loss_val: 0.3905 acc_val: 0.8883 time: 507.6179s\n",
            "Epoch: 2077 loss_train: 0.4153 acc_train: 0.8732 loss_val: 0.3940 acc_val: 0.8908 time: 507.8715s\n",
            "Epoch: 2078 loss_train: 0.4110 acc_train: 0.8738 loss_val: 0.3850 acc_val: 0.8912 time: 508.1258s\n",
            "Epoch: 2079 loss_train: 0.4151 acc_train: 0.8777 loss_val: 0.3865 acc_val: 0.8883 time: 508.3765s\n",
            "Epoch: 2080 loss_train: 0.4062 acc_train: 0.8724 loss_val: 0.3920 acc_val: 0.8879 time: 508.6230s\n",
            "Epoch: 2081 loss_train: 0.4120 acc_train: 0.8731 loss_val: 0.3877 acc_val: 0.8900 time: 508.8658s\n",
            "Epoch: 2082 loss_train: 0.4088 acc_train: 0.8725 loss_val: 0.3872 acc_val: 0.8879 time: 509.1057s\n",
            "Epoch: 2083 loss_train: 0.4123 acc_train: 0.8763 loss_val: 0.3918 acc_val: 0.8871 time: 509.3387s\n",
            "Epoch: 2084 loss_train: 0.4152 acc_train: 0.8699 loss_val: 0.3934 acc_val: 0.8892 time: 509.5985s\n",
            "Epoch: 2085 loss_train: 0.4151 acc_train: 0.8725 loss_val: 0.3836 acc_val: 0.8912 time: 509.8444s\n",
            "Epoch: 2086 loss_train: 0.4059 acc_train: 0.8777 loss_val: 0.3851 acc_val: 0.8904 time: 510.0971s\n",
            "Epoch: 2087 loss_train: 0.4223 acc_train: 0.8713 loss_val: 0.3915 acc_val: 0.8912 time: 510.3389s\n",
            "Epoch: 2088 loss_train: 0.4125 acc_train: 0.8722 loss_val: 0.3863 acc_val: 0.8929 time: 510.5911s\n",
            "Epoch: 2089 loss_train: 0.4025 acc_train: 0.8780 loss_val: 0.3885 acc_val: 0.8879 time: 510.8337s\n",
            "Epoch: 2090 loss_train: 0.4052 acc_train: 0.8725 loss_val: 0.3915 acc_val: 0.8904 time: 511.0862s\n",
            "Epoch: 2091 loss_train: 0.4009 acc_train: 0.8770 loss_val: 0.3844 acc_val: 0.8917 time: 511.3304s\n",
            "Epoch: 2092 loss_train: 0.4073 acc_train: 0.8753 loss_val: 0.3844 acc_val: 0.8921 time: 511.5843s\n",
            "Epoch: 2093 loss_train: 0.4092 acc_train: 0.8759 loss_val: 0.4262 acc_val: 0.8758 time: 511.8016s\n",
            "Epoch: 2094 loss_train: 0.4452 acc_train: 0.8564 loss_val: 0.3903 acc_val: 0.8896 time: 512.0587s\n",
            "Epoch: 2095 loss_train: 0.4182 acc_train: 0.8764 loss_val: 0.4000 acc_val: 0.8846 time: 512.3039s\n",
            "Epoch: 2096 loss_train: 0.4339 acc_train: 0.8692 loss_val: 0.4167 acc_val: 0.8767 time: 512.5539s\n",
            "Epoch: 2097 loss_train: 0.4360 acc_train: 0.8587 loss_val: 0.4103 acc_val: 0.8912 time: 512.7990s\n",
            "Epoch: 2098 loss_train: 0.4392 acc_train: 0.8742 loss_val: 0.4237 acc_val: 0.8721 time: 513.0687s\n",
            "Epoch: 2099 loss_train: 0.4602 acc_train: 0.8531 loss_val: 0.4500 acc_val: 0.8608 time: 513.3295s\n",
            "Epoch: 2100 loss_train: 0.4641 acc_train: 0.8462 loss_val: 0.4366 acc_val: 0.8808 time: 513.5786s\n",
            "Epoch: 2101 loss_train: 0.4494 acc_train: 0.8743 loss_val: 0.4004 acc_val: 0.8850 time: 513.8260s\n",
            "Epoch: 2102 loss_train: 0.4244 acc_train: 0.8749 loss_val: 0.4232 acc_val: 0.8675 time: 514.0765s\n",
            "Epoch: 2103 loss_train: 0.4357 acc_train: 0.8584 loss_val: 0.4089 acc_val: 0.8788 time: 514.3324s\n",
            "Epoch: 2104 loss_train: 0.4170 acc_train: 0.8722 loss_val: 0.4330 acc_val: 0.8842 time: 514.5778s\n",
            "Epoch: 2105 loss_train: 0.4722 acc_train: 0.8687 loss_val: 0.4255 acc_val: 0.8750 time: 514.8233s\n",
            "Epoch: 2106 loss_train: 0.4334 acc_train: 0.8612 loss_val: 0.4184 acc_val: 0.8779 time: 515.0647s\n",
            "Epoch: 2107 loss_train: 0.4249 acc_train: 0.8652 loss_val: 0.3914 acc_val: 0.8879 time: 515.2435s\n",
            "Epoch: 2108 loss_train: 0.4188 acc_train: 0.8759 loss_val: 0.4108 acc_val: 0.8812 time: 515.4747s\n",
            "Epoch: 2109 loss_train: 0.4220 acc_train: 0.8744 loss_val: 0.3964 acc_val: 0.8904 time: 515.7327s\n",
            "Epoch: 2110 loss_train: 0.4186 acc_train: 0.8731 loss_val: 0.3975 acc_val: 0.8900 time: 515.9797s\n",
            "Epoch: 2111 loss_train: 0.4172 acc_train: 0.8731 loss_val: 0.3940 acc_val: 0.8896 time: 516.2301s\n",
            "Epoch: 2112 loss_train: 0.4102 acc_train: 0.8735 loss_val: 0.3966 acc_val: 0.8862 time: 516.4773s\n",
            "Epoch: 2113 loss_train: 0.4289 acc_train: 0.8624 loss_val: 0.3958 acc_val: 0.8888 time: 516.7322s\n",
            "Epoch: 2114 loss_train: 0.4097 acc_train: 0.8779 loss_val: 0.3887 acc_val: 0.8900 time: 516.9791s\n",
            "Epoch: 2115 loss_train: 0.3996 acc_train: 0.8824 loss_val: 0.3949 acc_val: 0.8892 time: 517.2335s\n",
            "Epoch: 2116 loss_train: 0.4121 acc_train: 0.8704 loss_val: 0.3840 acc_val: 0.8912 time: 517.4785s\n",
            "Epoch: 2117 loss_train: 0.4148 acc_train: 0.8720 loss_val: 0.3862 acc_val: 0.8908 time: 517.7330s\n",
            "Epoch: 2118 loss_train: 0.4087 acc_train: 0.8774 loss_val: 0.3897 acc_val: 0.8900 time: 517.9828s\n",
            "Epoch: 2119 loss_train: 0.4051 acc_train: 0.8771 loss_val: 0.3878 acc_val: 0.8908 time: 518.2391s\n",
            "Epoch: 2120 loss_train: 0.3965 acc_train: 0.8795 loss_val: 0.3932 acc_val: 0.8888 time: 518.4865s\n",
            "Epoch: 2121 loss_train: 0.4103 acc_train: 0.8721 loss_val: 0.3872 acc_val: 0.8896 time: 518.7373s\n",
            "Epoch: 2122 loss_train: 0.4078 acc_train: 0.8715 loss_val: 0.3912 acc_val: 0.8904 time: 518.9817s\n",
            "Epoch: 2123 loss_train: 0.4114 acc_train: 0.8799 loss_val: 0.3924 acc_val: 0.8875 time: 519.2295s\n",
            "Epoch: 2124 loss_train: 0.4167 acc_train: 0.8705 loss_val: 0.3905 acc_val: 0.8867 time: 519.4785s\n",
            "Epoch: 2125 loss_train: 0.3983 acc_train: 0.8744 loss_val: 0.3876 acc_val: 0.8929 time: 519.7254s\n",
            "Epoch: 2126 loss_train: 0.3960 acc_train: 0.8784 loss_val: 0.3836 acc_val: 0.8929 time: 519.9745s\n",
            "Epoch: 2127 loss_train: 0.3990 acc_train: 0.8822 loss_val: 0.3976 acc_val: 0.8862 time: 520.2224s\n",
            "Epoch: 2128 loss_train: 0.4045 acc_train: 0.8734 loss_val: 0.3831 acc_val: 0.8904 time: 520.4717s\n",
            "Epoch: 2129 loss_train: 0.4082 acc_train: 0.8779 loss_val: 0.3815 acc_val: 0.8917 time: 520.7237s\n",
            "Epoch: 2130 loss_train: 0.4102 acc_train: 0.8730 loss_val: 0.3874 acc_val: 0.8954 time: 520.9728s\n",
            "Epoch: 2131 loss_train: 0.3909 acc_train: 0.8807 loss_val: 0.3843 acc_val: 0.8938 time: 521.2191s\n",
            "Epoch: 2132 loss_train: 0.3923 acc_train: 0.8807 loss_val: 0.3819 acc_val: 0.8933 time: 521.4659s\n",
            "Epoch: 2133 loss_train: 0.3966 acc_train: 0.8808 loss_val: 0.3793 acc_val: 0.8900 time: 521.7129s\n",
            "Epoch: 2134 loss_train: 0.4093 acc_train: 0.8725 loss_val: 0.3826 acc_val: 0.8954 time: 521.9621s\n",
            "Epoch: 2135 loss_train: 0.3972 acc_train: 0.8776 loss_val: 0.3849 acc_val: 0.8942 time: 522.1898s\n",
            "Epoch: 2136 loss_train: 0.4005 acc_train: 0.8811 loss_val: 0.3838 acc_val: 0.8925 time: 522.4349s\n",
            "Epoch: 2137 loss_train: 0.4018 acc_train: 0.8749 loss_val: 0.3812 acc_val: 0.8917 time: 522.6846s\n",
            "Epoch: 2138 loss_train: 0.3961 acc_train: 0.8808 loss_val: 0.3833 acc_val: 0.8929 time: 522.9324s\n",
            "Epoch: 2139 loss_train: 0.3901 acc_train: 0.8793 loss_val: 0.3941 acc_val: 0.8896 time: 523.1892s\n",
            "Epoch: 2140 loss_train: 0.4050 acc_train: 0.8752 loss_val: 0.3807 acc_val: 0.8921 time: 523.4563s\n",
            "Epoch: 2141 loss_train: 0.4042 acc_train: 0.8812 loss_val: 0.3788 acc_val: 0.8908 time: 523.7031s\n",
            "Epoch: 2142 loss_train: 0.3895 acc_train: 0.8823 loss_val: 0.3983 acc_val: 0.8833 time: 523.9061s\n",
            "Epoch: 2143 loss_train: 0.3956 acc_train: 0.8719 loss_val: 0.3838 acc_val: 0.8946 time: 524.1611s\n",
            "Epoch: 2144 loss_train: 0.3948 acc_train: 0.8789 loss_val: 0.3804 acc_val: 0.8954 time: 524.4135s\n",
            "Epoch: 2145 loss_train: 0.3923 acc_train: 0.8833 loss_val: 0.3856 acc_val: 0.8879 time: 524.6592s\n",
            "Epoch: 2146 loss_train: 0.4080 acc_train: 0.8715 loss_val: 0.3834 acc_val: 0.8912 time: 524.8827s\n",
            "Epoch: 2147 loss_train: 0.3989 acc_train: 0.8768 loss_val: 0.3944 acc_val: 0.8896 time: 525.1295s\n",
            "Epoch: 2148 loss_train: 0.4092 acc_train: 0.8809 loss_val: 0.3866 acc_val: 0.8879 time: 525.3775s\n",
            "Epoch: 2149 loss_train: 0.3886 acc_train: 0.8788 loss_val: 0.4015 acc_val: 0.8775 time: 525.6304s\n",
            "Epoch: 2150 loss_train: 0.4045 acc_train: 0.8681 loss_val: 0.3795 acc_val: 0.8904 time: 525.8759s\n",
            "Epoch: 2151 loss_train: 0.4106 acc_train: 0.8774 loss_val: 0.3900 acc_val: 0.8921 time: 526.1275s\n",
            "Epoch: 2152 loss_train: 0.4101 acc_train: 0.8797 loss_val: 0.3901 acc_val: 0.8875 time: 526.3808s\n",
            "Epoch: 2153 loss_train: 0.3945 acc_train: 0.8785 loss_val: 0.3814 acc_val: 0.8912 time: 526.6263s\n",
            "Epoch: 2154 loss_train: 0.3948 acc_train: 0.8779 loss_val: 0.3775 acc_val: 0.8917 time: 526.8777s\n",
            "Epoch: 2155 loss_train: 0.3951 acc_train: 0.8836 loss_val: 0.3899 acc_val: 0.8912 time: 527.1244s\n",
            "Epoch: 2156 loss_train: 0.3940 acc_train: 0.8795 loss_val: 0.3894 acc_val: 0.8883 time: 527.3776s\n",
            "Epoch: 2157 loss_train: 0.4082 acc_train: 0.8722 loss_val: 0.3779 acc_val: 0.8921 time: 527.6249s\n",
            "Epoch: 2158 loss_train: 0.3953 acc_train: 0.8824 loss_val: 0.3795 acc_val: 0.8933 time: 527.8233s\n",
            "Epoch: 2159 loss_train: 0.3990 acc_train: 0.8800 loss_val: 0.4053 acc_val: 0.8850 time: 528.0558s\n",
            "Epoch: 2160 loss_train: 0.4092 acc_train: 0.8741 loss_val: 0.3916 acc_val: 0.8933 time: 528.3077s\n",
            "Epoch: 2161 loss_train: 0.4198 acc_train: 0.8775 loss_val: 0.3787 acc_val: 0.8912 time: 528.5525s\n",
            "Epoch: 2162 loss_train: 0.3934 acc_train: 0.8799 loss_val: 0.3952 acc_val: 0.8838 time: 528.7794s\n",
            "Epoch: 2163 loss_train: 0.4230 acc_train: 0.8636 loss_val: 0.3914 acc_val: 0.8908 time: 529.0242s\n",
            "Epoch: 2164 loss_train: 0.4081 acc_train: 0.8800 loss_val: 0.3838 acc_val: 0.8921 time: 529.2709s\n",
            "Epoch: 2165 loss_train: 0.4049 acc_train: 0.8831 loss_val: 0.4165 acc_val: 0.8725 time: 529.5166s\n",
            "Epoch: 2166 loss_train: 0.4178 acc_train: 0.8575 loss_val: 0.3863 acc_val: 0.8912 time: 529.7654s\n",
            "Epoch: 2167 loss_train: 0.4115 acc_train: 0.8740 loss_val: 0.3835 acc_val: 0.8900 time: 530.0141s\n",
            "Epoch: 2168 loss_train: 0.3945 acc_train: 0.8816 loss_val: 0.3946 acc_val: 0.8862 time: 530.2645s\n",
            "Epoch: 2169 loss_train: 0.3970 acc_train: 0.8759 loss_val: 0.3857 acc_val: 0.8862 time: 530.5125s\n",
            "Epoch: 2170 loss_train: 0.3926 acc_train: 0.8770 loss_val: 0.3851 acc_val: 0.8908 time: 530.7667s\n",
            "Epoch: 2171 loss_train: 0.4035 acc_train: 0.8789 loss_val: 0.3846 acc_val: 0.8938 time: 531.0124s\n",
            "Epoch: 2172 loss_train: 0.3984 acc_train: 0.8767 loss_val: 0.3894 acc_val: 0.8883 time: 531.2719s\n",
            "Epoch: 2173 loss_train: 0.3892 acc_train: 0.8798 loss_val: 0.3783 acc_val: 0.8925 time: 531.5335s\n",
            "Epoch: 2174 loss_train: 0.3843 acc_train: 0.8829 loss_val: 0.3761 acc_val: 0.8933 time: 531.7808s\n",
            "Epoch: 2175 loss_train: 0.3915 acc_train: 0.8792 loss_val: 0.3807 acc_val: 0.8942 time: 532.0388s\n",
            "Epoch: 2176 loss_train: 0.3855 acc_train: 0.8851 loss_val: 0.3815 acc_val: 0.8929 time: 532.2874s\n",
            "Epoch: 2177 loss_train: 0.3898 acc_train: 0.8790 loss_val: 0.3794 acc_val: 0.8929 time: 532.5308s\n",
            "Epoch: 2178 loss_train: 0.3928 acc_train: 0.8819 loss_val: 0.3789 acc_val: 0.8938 time: 532.7792s\n",
            "Epoch: 2179 loss_train: 0.3967 acc_train: 0.8756 loss_val: 0.3834 acc_val: 0.8921 time: 533.0330s\n",
            "Epoch: 2180 loss_train: 0.3896 acc_train: 0.8771 loss_val: 0.3744 acc_val: 0.8921 time: 533.2316s\n",
            "Epoch: 2181 loss_train: 0.3819 acc_train: 0.8832 loss_val: 0.3748 acc_val: 0.8946 time: 533.4747s\n",
            "Epoch: 2182 loss_train: 0.3885 acc_train: 0.8833 loss_val: 0.3864 acc_val: 0.8912 time: 533.7307s\n",
            "Epoch: 2183 loss_train: 0.4038 acc_train: 0.8736 loss_val: 0.3752 acc_val: 0.8938 time: 533.9684s\n",
            "Epoch: 2184 loss_train: 0.3872 acc_train: 0.8816 loss_val: 0.3744 acc_val: 0.8933 time: 534.2216s\n",
            "Epoch: 2185 loss_train: 0.3818 acc_train: 0.8831 loss_val: 0.3821 acc_val: 0.8888 time: 534.4787s\n",
            "Epoch: 2186 loss_train: 0.4000 acc_train: 0.8737 loss_val: 0.3785 acc_val: 0.8938 time: 534.7308s\n",
            "Epoch: 2187 loss_train: 0.3854 acc_train: 0.8824 loss_val: 0.3735 acc_val: 0.8938 time: 534.9785s\n",
            "Epoch: 2188 loss_train: 0.3939 acc_train: 0.8829 loss_val: 0.3806 acc_val: 0.8900 time: 535.2236s\n",
            "Epoch: 2189 loss_train: 0.3864 acc_train: 0.8770 loss_val: 0.3716 acc_val: 0.8950 time: 535.4744s\n",
            "Epoch: 2190 loss_train: 0.3785 acc_train: 0.8819 loss_val: 0.3765 acc_val: 0.8925 time: 535.7226s\n",
            "Epoch: 2191 loss_train: 0.3780 acc_train: 0.8833 loss_val: 0.3753 acc_val: 0.8929 time: 535.9723s\n",
            "Epoch: 2192 loss_train: 0.3803 acc_train: 0.8805 loss_val: 0.3787 acc_val: 0.8938 time: 536.2324s\n",
            "Epoch: 2193 loss_train: 0.3884 acc_train: 0.8810 loss_val: 0.3673 acc_val: 0.8917 time: 536.4867s\n",
            "Epoch: 2194 loss_train: 0.3762 acc_train: 0.8825 loss_val: 0.3663 acc_val: 0.8912 time: 536.7254s\n",
            "Epoch: 2195 loss_train: 0.3803 acc_train: 0.8841 loss_val: 0.3803 acc_val: 0.8946 time: 536.9711s\n",
            "Epoch: 2196 loss_train: 0.3858 acc_train: 0.8792 loss_val: 0.3744 acc_val: 0.8892 time: 537.2261s\n",
            "Epoch: 2197 loss_train: 0.3804 acc_train: 0.8744 loss_val: 0.3712 acc_val: 0.8929 time: 537.4771s\n",
            "Epoch: 2198 loss_train: 0.3726 acc_train: 0.8862 loss_val: 0.3696 acc_val: 0.8917 time: 537.7239s\n",
            "Epoch: 2199 loss_train: 0.3918 acc_train: 0.8804 loss_val: 0.3824 acc_val: 0.8892 time: 537.9646s\n",
            "Epoch: 2200 loss_train: 0.3827 acc_train: 0.8753 loss_val: 0.3817 acc_val: 0.8946 time: 538.2155s\n",
            "Epoch: 2201 loss_train: 0.3913 acc_train: 0.8801 loss_val: 0.3857 acc_val: 0.8833 time: 538.4705s\n",
            "Epoch: 2202 loss_train: 0.4239 acc_train: 0.8605 loss_val: 0.3820 acc_val: 0.8958 time: 538.7254s\n",
            "Epoch: 2203 loss_train: 0.3743 acc_train: 0.8851 loss_val: 0.3912 acc_val: 0.8950 time: 538.9698s\n",
            "Epoch: 2204 loss_train: 0.3943 acc_train: 0.8833 loss_val: 0.3901 acc_val: 0.8850 time: 539.2157s\n",
            "Epoch: 2205 loss_train: 0.4042 acc_train: 0.8696 loss_val: 0.3867 acc_val: 0.8875 time: 539.4649s\n",
            "Epoch: 2206 loss_train: 0.4099 acc_train: 0.8720 loss_val: 0.3795 acc_val: 0.8962 time: 539.7168s\n",
            "Epoch: 2207 loss_train: 0.3873 acc_train: 0.8830 loss_val: 0.4063 acc_val: 0.8925 time: 539.9618s\n",
            "Epoch: 2208 loss_train: 0.4117 acc_train: 0.8786 loss_val: 0.3935 acc_val: 0.8825 time: 540.2098s\n",
            "Epoch: 2209 loss_train: 0.4016 acc_train: 0.8714 loss_val: 0.3986 acc_val: 0.8829 time: 540.4587s\n",
            "Epoch: 2210 loss_train: 0.4288 acc_train: 0.8670 loss_val: 0.3820 acc_val: 0.8917 time: 540.7074s\n",
            "Epoch: 2211 loss_train: 0.3797 acc_train: 0.8809 loss_val: 0.4237 acc_val: 0.8883 time: 540.9552s\n",
            "Epoch: 2212 loss_train: 0.4282 acc_train: 0.8775 loss_val: 0.3915 acc_val: 0.8850 time: 541.2072s\n",
            "Epoch: 2213 loss_train: 0.3879 acc_train: 0.8768 loss_val: 0.4352 acc_val: 0.8608 time: 541.4505s\n",
            "Epoch: 2214 loss_train: 0.4639 acc_train: 0.8514 loss_val: 0.3835 acc_val: 0.8879 time: 541.7037s\n",
            "Epoch: 2215 loss_train: 0.3986 acc_train: 0.8774 loss_val: 0.4013 acc_val: 0.8921 time: 541.9603s\n",
            "Epoch: 2216 loss_train: 0.4014 acc_train: 0.8813 loss_val: 0.4274 acc_val: 0.8875 time: 542.2177s\n",
            "Epoch: 2217 loss_train: 0.4401 acc_train: 0.8756 loss_val: 0.3789 acc_val: 0.8921 time: 542.4892s\n",
            "Epoch: 2218 loss_train: 0.3825 acc_train: 0.8807 loss_val: 0.4230 acc_val: 0.8738 time: 542.7354s\n",
            "Epoch: 2219 loss_train: 0.4433 acc_train: 0.8578 loss_val: 0.4087 acc_val: 0.8779 time: 542.9799s\n",
            "Epoch: 2220 loss_train: 0.4157 acc_train: 0.8660 loss_val: 0.3896 acc_val: 0.8912 time: 543.2270s\n",
            "Epoch: 2221 loss_train: 0.3967 acc_train: 0.8756 loss_val: 0.4075 acc_val: 0.8912 time: 543.4827s\n",
            "Epoch: 2222 loss_train: 0.4173 acc_train: 0.8818 loss_val: 0.4116 acc_val: 0.8925 time: 543.7384s\n",
            "Epoch: 2223 loss_train: 0.4188 acc_train: 0.8799 loss_val: 0.3915 acc_val: 0.8900 time: 543.9822s\n",
            "Epoch: 2224 loss_train: 0.3945 acc_train: 0.8746 loss_val: 0.3904 acc_val: 0.8892 time: 544.2271s\n",
            "Epoch: 2225 loss_train: 0.4005 acc_train: 0.8767 loss_val: 0.4181 acc_val: 0.8746 time: 544.4866s\n",
            "Epoch: 2226 loss_train: 0.4203 acc_train: 0.8648 loss_val: 0.3829 acc_val: 0.8917 time: 544.7448s\n",
            "Epoch: 2227 loss_train: 0.3900 acc_train: 0.8799 loss_val: 0.3968 acc_val: 0.8954 time: 544.9964s\n",
            "Epoch: 2228 loss_train: 0.3988 acc_train: 0.8832 loss_val: 0.4098 acc_val: 0.8900 time: 545.2461s\n",
            "Epoch: 2229 loss_train: 0.3989 acc_train: 0.8767 loss_val: 0.3949 acc_val: 0.8938 time: 545.4948s\n",
            "Epoch: 2230 loss_train: 0.3866 acc_train: 0.8832 loss_val: 0.3906 acc_val: 0.8908 time: 545.7436s\n",
            "Epoch: 2231 loss_train: 0.3963 acc_train: 0.8836 loss_val: 0.3827 acc_val: 0.8912 time: 545.9971s\n",
            "Epoch: 2232 loss_train: 0.3899 acc_train: 0.8797 loss_val: 0.3869 acc_val: 0.8892 time: 546.2471s\n",
            "Epoch: 2233 loss_train: 0.4022 acc_train: 0.8754 loss_val: 0.3844 acc_val: 0.8921 time: 546.4950s\n",
            "Epoch: 2234 loss_train: 0.4092 acc_train: 0.8760 loss_val: 0.3973 acc_val: 0.8842 time: 546.7423s\n",
            "Epoch: 2235 loss_train: 0.3903 acc_train: 0.8809 loss_val: 0.3871 acc_val: 0.8933 time: 546.9882s\n",
            "Epoch: 2236 loss_train: 0.3860 acc_train: 0.8826 loss_val: 0.3876 acc_val: 0.8942 time: 547.2423s\n",
            "Epoch: 2237 loss_train: 0.3907 acc_train: 0.8845 loss_val: 0.4013 acc_val: 0.8846 time: 547.4921s\n",
            "Epoch: 2238 loss_train: 0.3879 acc_train: 0.8812 loss_val: 0.3854 acc_val: 0.8921 time: 547.7413s\n",
            "Epoch: 2239 loss_train: 0.3949 acc_train: 0.8834 loss_val: 0.3834 acc_val: 0.8900 time: 547.9932s\n",
            "Epoch: 2240 loss_train: 0.3972 acc_train: 0.8831 loss_val: 0.4033 acc_val: 0.8808 time: 548.2419s\n",
            "Epoch: 2241 loss_train: 0.4042 acc_train: 0.8722 loss_val: 0.3822 acc_val: 0.8917 time: 548.5055s\n",
            "Epoch: 2242 loss_train: 0.3938 acc_train: 0.8770 loss_val: 0.3863 acc_val: 0.8929 time: 548.7543s\n",
            "Epoch: 2243 loss_train: 0.4020 acc_train: 0.8797 loss_val: 0.4195 acc_val: 0.8725 time: 548.9994s\n",
            "Epoch: 2244 loss_train: 0.4124 acc_train: 0.8662 loss_val: 0.3851 acc_val: 0.8929 time: 549.2457s\n",
            "Epoch: 2245 loss_train: 0.3952 acc_train: 0.8813 loss_val: 0.3943 acc_val: 0.8950 time: 549.4924s\n",
            "Epoch: 2246 loss_train: 0.4047 acc_train: 0.8835 loss_val: 0.3969 acc_val: 0.8858 time: 549.7411s\n",
            "Epoch: 2247 loss_train: 0.4003 acc_train: 0.8778 loss_val: 0.3894 acc_val: 0.8871 time: 549.9899s\n",
            "Epoch: 2248 loss_train: 0.3848 acc_train: 0.8771 loss_val: 0.3842 acc_val: 0.8929 time: 550.2357s\n",
            "Epoch: 2249 loss_train: 0.4000 acc_train: 0.8822 loss_val: 0.3775 acc_val: 0.8904 time: 550.4288s\n",
            "Epoch: 2250 loss_train: 0.3804 acc_train: 0.8769 loss_val: 0.3786 acc_val: 0.8912 time: 550.6576s\n",
            "Epoch: 2251 loss_train: 0.3788 acc_train: 0.8789 loss_val: 0.3769 acc_val: 0.8908 time: 550.8681s\n",
            "Epoch: 2252 loss_train: 0.3806 acc_train: 0.8816 loss_val: 0.3804 acc_val: 0.8912 time: 551.1189s\n",
            "Epoch: 2253 loss_train: 0.3969 acc_train: 0.8807 loss_val: 0.3819 acc_val: 0.8917 time: 551.3645s\n",
            "Epoch: 2254 loss_train: 0.3877 acc_train: 0.8835 loss_val: 0.3848 acc_val: 0.8892 time: 551.6052s\n",
            "Epoch: 2255 loss_train: 0.3813 acc_train: 0.8832 loss_val: 0.3755 acc_val: 0.8917 time: 551.8526s\n",
            "Epoch: 2256 loss_train: 0.3867 acc_train: 0.8845 loss_val: 0.3782 acc_val: 0.8938 time: 552.1015s\n",
            "Epoch: 2257 loss_train: 0.3846 acc_train: 0.8834 loss_val: 0.3965 acc_val: 0.8825 time: 552.3534s\n",
            "Epoch: 2258 loss_train: 0.4033 acc_train: 0.8734 loss_val: 0.3798 acc_val: 0.8942 time: 552.5835s\n",
            "Epoch: 2259 loss_train: 0.3883 acc_train: 0.8810 loss_val: 0.3802 acc_val: 0.8942 time: 552.8300s\n",
            "Epoch: 2260 loss_train: 0.3786 acc_train: 0.8858 loss_val: 0.3921 acc_val: 0.8888 time: 553.0755s\n",
            "Epoch: 2261 loss_train: 0.3980 acc_train: 0.8768 loss_val: 0.3731 acc_val: 0.8917 time: 553.3199s\n",
            "Epoch: 2262 loss_train: 0.3806 acc_train: 0.8825 loss_val: 0.3813 acc_val: 0.8892 time: 553.5723s\n",
            "Epoch: 2263 loss_train: 0.3868 acc_train: 0.8824 loss_val: 0.3831 acc_val: 0.8888 time: 553.8294s\n",
            "Epoch: 2264 loss_train: 0.3816 acc_train: 0.8767 loss_val: 0.3808 acc_val: 0.8929 time: 554.0809s\n",
            "Epoch: 2265 loss_train: 0.3823 acc_train: 0.8820 loss_val: 0.3769 acc_val: 0.8933 time: 554.3243s\n",
            "Epoch: 2266 loss_train: 0.3734 acc_train: 0.8843 loss_val: 0.3888 acc_val: 0.8896 time: 554.5758s\n",
            "Epoch: 2267 loss_train: 0.3894 acc_train: 0.8824 loss_val: 0.3890 acc_val: 0.8862 time: 554.8252s\n",
            "Epoch: 2268 loss_train: 0.3852 acc_train: 0.8804 loss_val: 0.3799 acc_val: 0.8938 time: 555.0802s\n",
            "Epoch: 2269 loss_train: 0.4032 acc_train: 0.8830 loss_val: 0.3783 acc_val: 0.8925 time: 555.3273s\n",
            "Epoch: 2270 loss_train: 0.3859 acc_train: 0.8786 loss_val: 0.3974 acc_val: 0.8846 time: 555.5719s\n",
            "Epoch: 2271 loss_train: 0.3989 acc_train: 0.8684 loss_val: 0.3800 acc_val: 0.8904 time: 555.8274s\n",
            "Epoch: 2272 loss_train: 0.3839 acc_train: 0.8789 loss_val: 0.3844 acc_val: 0.8912 time: 556.0786s\n",
            "Epoch: 2273 loss_train: 0.3932 acc_train: 0.8849 loss_val: 0.4071 acc_val: 0.8792 time: 556.3014s\n",
            "Epoch: 2274 loss_train: 0.3928 acc_train: 0.8732 loss_val: 0.3756 acc_val: 0.8933 time: 556.5484s\n",
            "Epoch: 2275 loss_train: 0.3743 acc_train: 0.8836 loss_val: 0.3943 acc_val: 0.8896 time: 556.7891s\n",
            "Epoch: 2276 loss_train: 0.4129 acc_train: 0.8766 loss_val: 0.4159 acc_val: 0.8767 time: 557.0374s\n",
            "Epoch: 2277 loss_train: 0.4104 acc_train: 0.8668 loss_val: 0.3908 acc_val: 0.8883 time: 557.2865s\n",
            "Epoch: 2278 loss_train: 0.3809 acc_train: 0.8805 loss_val: 0.3961 acc_val: 0.8912 time: 557.5448s\n",
            "Epoch: 2279 loss_train: 0.4117 acc_train: 0.8725 loss_val: 0.3787 acc_val: 0.8929 time: 557.7725s\n",
            "Epoch: 2280 loss_train: 0.3727 acc_train: 0.8827 loss_val: 0.4015 acc_val: 0.8804 time: 558.0061s\n",
            "Epoch: 2281 loss_train: 0.3858 acc_train: 0.8741 loss_val: 0.3780 acc_val: 0.8933 time: 558.2453s\n",
            "Epoch: 2282 loss_train: 0.3919 acc_train: 0.8856 loss_val: 0.3716 acc_val: 0.8933 time: 558.4949s\n",
            "Epoch: 2283 loss_train: 0.3907 acc_train: 0.8809 loss_val: 0.3993 acc_val: 0.8842 time: 558.7498s\n",
            "Epoch: 2284 loss_train: 0.3974 acc_train: 0.8735 loss_val: 0.3778 acc_val: 0.8925 time: 559.0032s\n",
            "Epoch: 2285 loss_train: 0.3720 acc_train: 0.8851 loss_val: 0.3846 acc_val: 0.8904 time: 559.2582s\n",
            "Epoch: 2286 loss_train: 0.3945 acc_train: 0.8843 loss_val: 0.3930 acc_val: 0.8833 time: 559.5044s\n",
            "Epoch: 2287 loss_train: 0.3863 acc_train: 0.8738 loss_val: 0.3852 acc_val: 0.8888 time: 559.7621s\n",
            "Epoch: 2288 loss_train: 0.3834 acc_train: 0.8743 loss_val: 0.3829 acc_val: 0.8883 time: 559.9906s\n",
            "Epoch: 2289 loss_train: 0.4115 acc_train: 0.8785 loss_val: 0.3938 acc_val: 0.8858 time: 560.2418s\n",
            "Epoch: 2290 loss_train: 0.3791 acc_train: 0.8786 loss_val: 0.3914 acc_val: 0.8846 time: 560.4970s\n",
            "Epoch: 2291 loss_train: 0.3881 acc_train: 0.8789 loss_val: 0.3795 acc_val: 0.8892 time: 560.7489s\n",
            "Epoch: 2292 loss_train: 0.3924 acc_train: 0.8832 loss_val: 0.3726 acc_val: 0.8908 time: 560.9995s\n",
            "Epoch: 2293 loss_train: 0.3803 acc_train: 0.8849 loss_val: 0.3791 acc_val: 0.8888 time: 561.2491s\n",
            "Epoch: 2294 loss_train: 0.3850 acc_train: 0.8771 loss_val: 0.3735 acc_val: 0.8946 time: 561.5031s\n",
            "Epoch: 2295 loss_train: 0.3825 acc_train: 0.8871 loss_val: 0.3809 acc_val: 0.8921 time: 561.7459s\n",
            "Epoch: 2296 loss_train: 0.3866 acc_train: 0.8822 loss_val: 0.3902 acc_val: 0.8875 time: 561.9944s\n",
            "Epoch: 2297 loss_train: 0.3720 acc_train: 0.8833 loss_val: 0.3718 acc_val: 0.8925 time: 562.2273s\n",
            "Epoch: 2298 loss_train: 0.3665 acc_train: 0.8858 loss_val: 0.3793 acc_val: 0.8900 time: 562.4837s\n",
            "Epoch: 2299 loss_train: 0.3993 acc_train: 0.8793 loss_val: 0.3959 acc_val: 0.8833 time: 562.7328s\n",
            "Epoch: 2300 loss_train: 0.3996 acc_train: 0.8688 loss_val: 0.3740 acc_val: 0.8946 time: 562.9699s\n",
            "Epoch: 2301 loss_train: 0.3758 acc_train: 0.8837 loss_val: 0.3821 acc_val: 0.8925 time: 563.2106s\n",
            "Epoch: 2302 loss_train: 0.3971 acc_train: 0.8781 loss_val: 0.3849 acc_val: 0.8892 time: 563.4674s\n",
            "Epoch: 2303 loss_train: 0.3807 acc_train: 0.8791 loss_val: 0.3775 acc_val: 0.8921 time: 563.7166s\n",
            "Epoch: 2304 loss_train: 0.3878 acc_train: 0.8766 loss_val: 0.3720 acc_val: 0.8946 time: 563.9628s\n",
            "Epoch: 2305 loss_train: 0.3694 acc_train: 0.8878 loss_val: 0.3812 acc_val: 0.8896 time: 564.2196s\n",
            "Epoch: 2306 loss_train: 0.3908 acc_train: 0.8769 loss_val: 0.3768 acc_val: 0.8942 time: 564.4781s\n",
            "Epoch: 2307 loss_train: 0.3751 acc_train: 0.8825 loss_val: 0.3777 acc_val: 0.8921 time: 564.7306s\n",
            "Epoch: 2308 loss_train: 0.3885 acc_train: 0.8802 loss_val: 0.3773 acc_val: 0.8929 time: 564.9756s\n",
            "Epoch: 2309 loss_train: 0.3597 acc_train: 0.8857 loss_val: 0.3821 acc_val: 0.8904 time: 565.2208s\n",
            "Epoch: 2310 loss_train: 0.3796 acc_train: 0.8815 loss_val: 0.3729 acc_val: 0.8908 time: 565.4729s\n",
            "Epoch: 2311 loss_train: 0.3754 acc_train: 0.8857 loss_val: 0.3683 acc_val: 0.8958 time: 565.7239s\n",
            "Epoch: 2312 loss_train: 0.3725 acc_train: 0.8882 loss_val: 0.3994 acc_val: 0.8838 time: 565.9747s\n",
            "Epoch: 2313 loss_train: 0.3950 acc_train: 0.8741 loss_val: 0.3674 acc_val: 0.8950 time: 566.2234s\n",
            "Epoch: 2314 loss_train: 0.3681 acc_train: 0.8900 loss_val: 0.3720 acc_val: 0.8933 time: 566.4762s\n",
            "Epoch: 2315 loss_train: 0.3801 acc_train: 0.8827 loss_val: 0.3883 acc_val: 0.8871 time: 566.7224s\n",
            "Epoch: 2316 loss_train: 0.3824 acc_train: 0.8715 loss_val: 0.3727 acc_val: 0.8938 time: 566.9694s\n",
            "Epoch: 2317 loss_train: 0.3704 acc_train: 0.8866 loss_val: 0.3806 acc_val: 0.8925 time: 567.2174s\n",
            "Epoch: 2318 loss_train: 0.3821 acc_train: 0.8862 loss_val: 0.3738 acc_val: 0.8933 time: 567.4748s\n",
            "Epoch: 2319 loss_train: 0.3749 acc_train: 0.8804 loss_val: 0.3914 acc_val: 0.8846 time: 567.7276s\n",
            "Epoch: 2320 loss_train: 0.3802 acc_train: 0.8749 loss_val: 0.3702 acc_val: 0.8942 time: 567.9718s\n",
            "Epoch: 2321 loss_train: 0.3796 acc_train: 0.8874 loss_val: 0.3678 acc_val: 0.8967 time: 568.2162s\n",
            "Epoch: 2322 loss_train: 0.3591 acc_train: 0.8902 loss_val: 0.3842 acc_val: 0.8908 time: 568.4674s\n",
            "Epoch: 2323 loss_train: 0.3662 acc_train: 0.8801 loss_val: 0.3666 acc_val: 0.8938 time: 568.7176s\n",
            "Epoch: 2324 loss_train: 0.3665 acc_train: 0.8847 loss_val: 0.3605 acc_val: 0.8971 time: 568.9673s\n",
            "Epoch: 2325 loss_train: 0.3631 acc_train: 0.8880 loss_val: 0.3764 acc_val: 0.8938 time: 569.1988s\n",
            "Epoch: 2326 loss_train: 0.3604 acc_train: 0.8855 loss_val: 0.3692 acc_val: 0.8971 time: 569.4523s\n",
            "Epoch: 2327 loss_train: 0.3640 acc_train: 0.8888 loss_val: 0.3612 acc_val: 0.8946 time: 569.7046s\n",
            "Epoch: 2328 loss_train: 0.3545 acc_train: 0.8895 loss_val: 0.3595 acc_val: 0.8942 time: 569.9574s\n",
            "Epoch: 2329 loss_train: 0.3567 acc_train: 0.8856 loss_val: 0.3676 acc_val: 0.8950 time: 570.2095s\n",
            "Epoch: 2330 loss_train: 0.3606 acc_train: 0.8858 loss_val: 0.3656 acc_val: 0.8962 time: 570.4615s\n",
            "Epoch: 2331 loss_train: 0.3677 acc_train: 0.8857 loss_val: 0.3642 acc_val: 0.8938 time: 570.7165s\n",
            "Epoch: 2332 loss_train: 0.3627 acc_train: 0.8884 loss_val: 0.3684 acc_val: 0.8942 time: 570.9733s\n",
            "Epoch: 2333 loss_train: 0.3638 acc_train: 0.8830 loss_val: 0.3655 acc_val: 0.8942 time: 571.2293s\n",
            "Epoch: 2334 loss_train: 0.3573 acc_train: 0.8847 loss_val: 0.3646 acc_val: 0.8967 time: 571.4743s\n",
            "Epoch: 2335 loss_train: 0.3721 acc_train: 0.8869 loss_val: 0.3724 acc_val: 0.8921 time: 571.7284s\n",
            "Epoch: 2336 loss_train: 0.3595 acc_train: 0.8855 loss_val: 0.3734 acc_val: 0.8929 time: 571.9580s\n",
            "Epoch: 2337 loss_train: 0.3604 acc_train: 0.8875 loss_val: 0.3614 acc_val: 0.8946 time: 572.1907s\n",
            "Epoch: 2338 loss_train: 0.3672 acc_train: 0.8882 loss_val: 0.3630 acc_val: 0.8946 time: 572.4379s\n",
            "Epoch: 2339 loss_train: 0.3592 acc_train: 0.8881 loss_val: 0.3673 acc_val: 0.8946 time: 572.6803s\n",
            "Epoch: 2340 loss_train: 0.3545 acc_train: 0.8866 loss_val: 0.3612 acc_val: 0.8958 time: 572.9315s\n",
            "Epoch: 2341 loss_train: 0.3663 acc_train: 0.8904 loss_val: 0.3629 acc_val: 0.8971 time: 573.1911s\n",
            "Epoch: 2342 loss_train: 0.3487 acc_train: 0.8920 loss_val: 0.3669 acc_val: 0.8942 time: 573.4500s\n",
            "Epoch: 2343 loss_train: 0.3521 acc_train: 0.8866 loss_val: 0.3630 acc_val: 0.8992 time: 573.7007s\n",
            "Epoch: 2344 loss_train: 0.3712 acc_train: 0.8892 loss_val: 0.3749 acc_val: 0.8946 time: 573.9597s\n",
            "Epoch: 2345 loss_train: 0.3583 acc_train: 0.8867 loss_val: 0.3689 acc_val: 0.8958 time: 574.2081s\n",
            "Epoch: 2346 loss_train: 0.3642 acc_train: 0.8833 loss_val: 0.3650 acc_val: 0.8958 time: 574.4599s\n",
            "Epoch: 2347 loss_train: 0.3679 acc_train: 0.8859 loss_val: 0.3675 acc_val: 0.8938 time: 574.7093s\n",
            "Epoch: 2348 loss_train: 0.3636 acc_train: 0.8840 loss_val: 0.3636 acc_val: 0.8958 time: 574.9635s\n",
            "Epoch: 2349 loss_train: 0.3601 acc_train: 0.8821 loss_val: 0.3636 acc_val: 0.8950 time: 575.2086s\n",
            "Epoch: 2350 loss_train: 0.3708 acc_train: 0.8880 loss_val: 0.3648 acc_val: 0.8983 time: 575.4577s\n",
            "Epoch: 2351 loss_train: 0.3558 acc_train: 0.8899 loss_val: 0.3806 acc_val: 0.8879 time: 575.7045s\n",
            "Epoch: 2352 loss_train: 0.3699 acc_train: 0.8802 loss_val: 0.3641 acc_val: 0.8971 time: 575.9616s\n",
            "Epoch: 2353 loss_train: 0.3793 acc_train: 0.8868 loss_val: 0.3812 acc_val: 0.8862 time: 576.2226s\n",
            "Epoch: 2354 loss_train: 0.3821 acc_train: 0.8810 loss_val: 0.3812 acc_val: 0.8904 time: 576.4700s\n",
            "Epoch: 2355 loss_train: 0.3688 acc_train: 0.8856 loss_val: 0.3737 acc_val: 0.8954 time: 576.7161s\n",
            "Epoch: 2356 loss_train: 0.3851 acc_train: 0.8834 loss_val: 0.3741 acc_val: 0.8954 time: 576.9658s\n",
            "Epoch: 2357 loss_train: 0.3800 acc_train: 0.8823 loss_val: 0.3790 acc_val: 0.8917 time: 577.2215s\n",
            "Epoch: 2358 loss_train: 0.3856 acc_train: 0.8743 loss_val: 0.3736 acc_val: 0.8929 time: 577.4649s\n",
            "Epoch: 2359 loss_train: 0.3698 acc_train: 0.8877 loss_val: 0.3769 acc_val: 0.8917 time: 577.7134s\n",
            "Epoch: 2360 loss_train: 0.3728 acc_train: 0.8860 loss_val: 0.3758 acc_val: 0.8912 time: 577.9638s\n",
            "Epoch: 2361 loss_train: 0.3677 acc_train: 0.8838 loss_val: 0.3718 acc_val: 0.8954 time: 578.2114s\n",
            "Epoch: 2362 loss_train: 0.3747 acc_train: 0.8862 loss_val: 0.3755 acc_val: 0.8921 time: 578.4637s\n",
            "Epoch: 2363 loss_train: 0.3639 acc_train: 0.8845 loss_val: 0.3834 acc_val: 0.8900 time: 578.7151s\n",
            "Epoch: 2364 loss_train: 0.3685 acc_train: 0.8843 loss_val: 0.3640 acc_val: 0.8950 time: 578.9688s\n",
            "Epoch: 2365 loss_train: 0.3512 acc_train: 0.8891 loss_val: 0.3631 acc_val: 0.8988 time: 579.2183s\n",
            "Epoch: 2366 loss_train: 0.3568 acc_train: 0.8900 loss_val: 0.4033 acc_val: 0.8821 time: 579.4677s\n",
            "Epoch: 2367 loss_train: 0.3870 acc_train: 0.8747 loss_val: 0.3659 acc_val: 0.8979 time: 579.7197s\n",
            "Epoch: 2368 loss_train: 0.3573 acc_train: 0.8879 loss_val: 0.3804 acc_val: 0.8917 time: 579.9689s\n",
            "Epoch: 2369 loss_train: 0.3831 acc_train: 0.8813 loss_val: 0.3869 acc_val: 0.8867 time: 580.1903s\n",
            "Epoch: 2370 loss_train: 0.3688 acc_train: 0.8833 loss_val: 0.3767 acc_val: 0.8875 time: 580.4150s\n",
            "Epoch: 2371 loss_train: 0.3613 acc_train: 0.8824 loss_val: 0.3848 acc_val: 0.8933 time: 580.6649s\n",
            "Epoch: 2372 loss_train: 0.3887 acc_train: 0.8798 loss_val: 0.3652 acc_val: 0.8996 time: 580.9078s\n",
            "Epoch: 2373 loss_train: 0.3458 acc_train: 0.8955 loss_val: 0.4065 acc_val: 0.8804 time: 581.1677s\n",
            "Epoch: 2374 loss_train: 0.3784 acc_train: 0.8784 loss_val: 0.3593 acc_val: 0.8967 time: 581.4133s\n",
            "Epoch: 2375 loss_train: 0.3607 acc_train: 0.8918 loss_val: 0.3620 acc_val: 0.8954 time: 581.6625s\n",
            "Epoch: 2376 loss_train: 0.3590 acc_train: 0.8900 loss_val: 0.3855 acc_val: 0.8867 time: 581.9083s\n",
            "Epoch: 2377 loss_train: 0.3748 acc_train: 0.8754 loss_val: 0.3614 acc_val: 0.8983 time: 582.1592s\n",
            "Epoch: 2378 loss_train: 0.3493 acc_train: 0.8931 loss_val: 0.3739 acc_val: 0.8942 time: 582.4038s\n",
            "Epoch: 2379 loss_train: 0.3861 acc_train: 0.8863 loss_val: 0.3847 acc_val: 0.8892 time: 582.6598s\n",
            "Epoch: 2380 loss_train: 0.3680 acc_train: 0.8827 loss_val: 0.3894 acc_val: 0.8896 time: 582.9101s\n",
            "Epoch: 2381 loss_train: 0.3670 acc_train: 0.8798 loss_val: 0.3711 acc_val: 0.8942 time: 583.1590s\n",
            "Epoch: 2382 loss_train: 0.3737 acc_train: 0.8842 loss_val: 0.3612 acc_val: 0.8967 time: 583.3995s\n",
            "Epoch: 2383 loss_train: 0.3564 acc_train: 0.8901 loss_val: 0.3874 acc_val: 0.8854 time: 583.6382s\n",
            "Epoch: 2384 loss_train: 0.3746 acc_train: 0.8793 loss_val: 0.3592 acc_val: 0.8938 time: 583.8471s\n",
            "Epoch: 2385 loss_train: 0.3647 acc_train: 0.8898 loss_val: 0.3614 acc_val: 0.8958 time: 584.1085s\n",
            "Epoch: 2386 loss_train: 0.3452 acc_train: 0.8920 loss_val: 0.3670 acc_val: 0.8950 time: 584.3573s\n",
            "Epoch: 2387 loss_train: 0.3562 acc_train: 0.8921 loss_val: 0.3588 acc_val: 0.8983 time: 584.6029s\n",
            "Epoch: 2388 loss_train: 0.3594 acc_train: 0.8907 loss_val: 0.3569 acc_val: 0.8950 time: 584.8464s\n",
            "Epoch: 2389 loss_train: 0.3454 acc_train: 0.8895 loss_val: 0.3598 acc_val: 0.8975 time: 585.0962s\n",
            "Epoch: 2390 loss_train: 0.3615 acc_train: 0.8840 loss_val: 0.3606 acc_val: 0.8971 time: 585.3417s\n",
            "Epoch: 2391 loss_train: 0.3490 acc_train: 0.8929 loss_val: 0.3582 acc_val: 0.8988 time: 585.5873s\n",
            "Epoch: 2392 loss_train: 0.3474 acc_train: 0.8912 loss_val: 0.3617 acc_val: 0.8950 time: 585.8305s\n",
            "Epoch: 2393 loss_train: 0.3488 acc_train: 0.8876 loss_val: 0.3568 acc_val: 0.8958 time: 586.0137s\n",
            "Epoch: 2394 loss_train: 0.3457 acc_train: 0.8879 loss_val: 0.3568 acc_val: 0.8967 time: 586.2611s\n",
            "Epoch: 2395 loss_train: 0.3481 acc_train: 0.8957 loss_val: 0.3684 acc_val: 0.8933 time: 586.5182s\n",
            "Epoch: 2396 loss_train: 0.3491 acc_train: 0.8897 loss_val: 0.3598 acc_val: 0.8971 time: 586.7707s\n",
            "Epoch: 2397 loss_train: 0.3510 acc_train: 0.8884 loss_val: 0.3559 acc_val: 0.9004 time: 587.0259s\n",
            "Epoch: 2398 loss_train: 0.3394 acc_train: 0.8958 loss_val: 0.3592 acc_val: 0.8962 time: 587.2776s\n",
            "Epoch: 2399 loss_train: 0.3574 acc_train: 0.8857 loss_val: 0.3608 acc_val: 0.9000 time: 587.5173s\n",
            "Epoch: 2400 loss_train: 0.3523 acc_train: 0.8876 loss_val: 0.3601 acc_val: 0.8958 time: 587.7662s\n",
            "Epoch: 2401 loss_train: 0.3513 acc_train: 0.8880 loss_val: 0.3581 acc_val: 0.8967 time: 588.0157s\n",
            "Epoch: 2402 loss_train: 0.3424 acc_train: 0.8932 loss_val: 0.3678 acc_val: 0.8925 time: 588.2165s\n",
            "Epoch: 2403 loss_train: 0.3587 acc_train: 0.8825 loss_val: 0.3594 acc_val: 0.8967 time: 588.4736s\n",
            "Epoch: 2404 loss_train: 0.3537 acc_train: 0.8848 loss_val: 0.3676 acc_val: 0.8950 time: 588.7212s\n",
            "Epoch: 2405 loss_train: 0.3654 acc_train: 0.8890 loss_val: 0.3776 acc_val: 0.8879 time: 588.9911s\n",
            "Epoch: 2406 loss_train: 0.3576 acc_train: 0.8807 loss_val: 0.3587 acc_val: 0.8938 time: 589.2405s\n",
            "Epoch: 2407 loss_train: 0.3506 acc_train: 0.8898 loss_val: 0.3688 acc_val: 0.8967 time: 589.4916s\n",
            "Epoch: 2408 loss_train: 0.3722 acc_train: 0.8869 loss_val: 0.3943 acc_val: 0.8833 time: 589.7369s\n",
            "Epoch: 2409 loss_train: 0.3669 acc_train: 0.8779 loss_val: 0.3736 acc_val: 0.8938 time: 590.0006s\n",
            "Epoch: 2410 loss_train: 0.3657 acc_train: 0.8877 loss_val: 0.3749 acc_val: 0.8954 time: 590.2475s\n",
            "Epoch: 2411 loss_train: 0.3844 acc_train: 0.8871 loss_val: 0.3821 acc_val: 0.8842 time: 590.4996s\n",
            "Epoch: 2412 loss_train: 0.3704 acc_train: 0.8775 loss_val: 0.3657 acc_val: 0.8933 time: 590.7511s\n",
            "Epoch: 2413 loss_train: 0.3572 acc_train: 0.8821 loss_val: 0.3800 acc_val: 0.8967 time: 591.0071s\n",
            "Epoch: 2414 loss_train: 0.3852 acc_train: 0.8915 loss_val: 0.3941 acc_val: 0.8842 time: 591.2496s\n",
            "Epoch: 2415 loss_train: 0.3710 acc_train: 0.8823 loss_val: 0.3783 acc_val: 0.8883 time: 591.4554s\n",
            "Epoch: 2416 loss_train: 0.3760 acc_train: 0.8821 loss_val: 0.3838 acc_val: 0.8938 time: 591.7049s\n",
            "Epoch: 2417 loss_train: 0.3891 acc_train: 0.8827 loss_val: 0.3689 acc_val: 0.8925 time: 591.9728s\n",
            "Epoch: 2418 loss_train: 0.3616 acc_train: 0.8819 loss_val: 0.3708 acc_val: 0.8962 time: 592.2165s\n",
            "Epoch: 2419 loss_train: 0.3469 acc_train: 0.8938 loss_val: 0.3735 acc_val: 0.8967 time: 592.4688s\n",
            "Epoch: 2420 loss_train: 0.3774 acc_train: 0.8868 loss_val: 0.4316 acc_val: 0.8775 time: 592.7183s\n",
            "Epoch: 2421 loss_train: 0.4100 acc_train: 0.8670 loss_val: 0.3872 acc_val: 0.8854 time: 592.9643s\n",
            "Epoch: 2422 loss_train: 0.3750 acc_train: 0.8825 loss_val: 0.4193 acc_val: 0.8883 time: 593.2208s\n",
            "Epoch: 2423 loss_train: 0.4467 acc_train: 0.8674 loss_val: 0.3974 acc_val: 0.8779 time: 593.4756s\n",
            "Epoch: 2424 loss_train: 0.3967 acc_train: 0.8663 loss_val: 0.3777 acc_val: 0.8933 time: 593.7249s\n",
            "Epoch: 2425 loss_train: 0.3644 acc_train: 0.8856 loss_val: 0.3813 acc_val: 0.8950 time: 593.9719s\n",
            "Epoch: 2426 loss_train: 0.4137 acc_train: 0.8791 loss_val: 0.4394 acc_val: 0.8750 time: 594.2319s\n",
            "Epoch: 2427 loss_train: 0.4181 acc_train: 0.8668 loss_val: 0.3601 acc_val: 0.8971 time: 594.4825s\n",
            "Epoch: 2428 loss_train: 0.3465 acc_train: 0.8945 loss_val: 0.3996 acc_val: 0.8892 time: 594.7281s\n",
            "Epoch: 2429 loss_train: 0.3965 acc_train: 0.8781 loss_val: 0.3696 acc_val: 0.8912 time: 594.9720s\n",
            "Epoch: 2430 loss_train: 0.3470 acc_train: 0.8899 loss_val: 0.3989 acc_val: 0.8825 time: 595.2207s\n",
            "Epoch: 2431 loss_train: 0.3705 acc_train: 0.8782 loss_val: 0.3735 acc_val: 0.8938 time: 595.4727s\n",
            "Epoch: 2432 loss_train: 0.3757 acc_train: 0.8897 loss_val: 0.3823 acc_val: 0.8888 time: 595.7075s\n",
            "Epoch: 2433 loss_train: 0.3922 acc_train: 0.8808 loss_val: 0.4579 acc_val: 0.8604 time: 595.9460s\n",
            "Epoch: 2434 loss_train: 0.4117 acc_train: 0.8578 loss_val: 0.3888 acc_val: 0.8858 time: 596.2001s\n",
            "Epoch: 2435 loss_train: 0.3762 acc_train: 0.8811 loss_val: 0.4311 acc_val: 0.8842 time: 596.4460s\n",
            "Epoch: 2436 loss_train: 0.4497 acc_train: 0.8711 loss_val: 0.4092 acc_val: 0.8754 time: 596.6988s\n",
            "Epoch: 2437 loss_train: 0.3997 acc_train: 0.8657 loss_val: 0.4304 acc_val: 0.8662 time: 596.9573s\n",
            "Epoch: 2438 loss_train: 0.4098 acc_train: 0.8624 loss_val: 0.3896 acc_val: 0.8896 time: 597.2070s\n",
            "Epoch: 2439 loss_train: 0.3949 acc_train: 0.8758 loss_val: 0.4153 acc_val: 0.8867 time: 597.4558s\n",
            "Epoch: 2440 loss_train: 0.4222 acc_train: 0.8821 loss_val: 0.4263 acc_val: 0.8675 time: 597.6911s\n",
            "Epoch: 2441 loss_train: 0.4224 acc_train: 0.8601 loss_val: 0.4195 acc_val: 0.8712 time: 597.9419s\n",
            "Epoch: 2442 loss_train: 0.4198 acc_train: 0.8560 loss_val: 0.4214 acc_val: 0.8854 time: 598.1616s\n",
            "Epoch: 2443 loss_train: 0.4378 acc_train: 0.8721 loss_val: 0.4259 acc_val: 0.8829 time: 598.3459s\n",
            "Epoch: 2444 loss_train: 0.4243 acc_train: 0.8751 loss_val: 0.4400 acc_val: 0.8625 time: 598.6002s\n",
            "Epoch: 2445 loss_train: 0.4328 acc_train: 0.8542 loss_val: 0.3912 acc_val: 0.8929 time: 598.8386s\n",
            "Epoch: 2446 loss_train: 0.4003 acc_train: 0.8815 loss_val: 0.4108 acc_val: 0.8862 time: 599.0951s\n",
            "Epoch: 2447 loss_train: 0.4191 acc_train: 0.8760 loss_val: 0.4648 acc_val: 0.8558 time: 599.3504s\n",
            "Epoch: 2448 loss_train: 0.4464 acc_train: 0.8499 loss_val: 0.4110 acc_val: 0.8812 time: 599.6021s\n",
            "Epoch: 2449 loss_train: 0.3840 acc_train: 0.8792 loss_val: 0.4172 acc_val: 0.8912 time: 599.8434s\n",
            "Epoch: 2450 loss_train: 0.4158 acc_train: 0.8847 loss_val: 0.3950 acc_val: 0.8917 time: 600.1052s\n",
            "Epoch: 2451 loss_train: 0.3911 acc_train: 0.8766 loss_val: 0.4245 acc_val: 0.8717 time: 600.3502s\n",
            "Epoch: 2452 loss_train: 0.4100 acc_train: 0.8655 loss_val: 0.3821 acc_val: 0.8908 time: 600.5982s\n",
            "Epoch: 2453 loss_train: 0.3868 acc_train: 0.8754 loss_val: 0.4129 acc_val: 0.8888 time: 600.8441s\n",
            "Epoch: 2454 loss_train: 0.4294 acc_train: 0.8766 loss_val: 0.4215 acc_val: 0.8779 time: 601.0866s\n",
            "Epoch: 2455 loss_train: 0.4002 acc_train: 0.8715 loss_val: 0.4435 acc_val: 0.8729 time: 601.3258s\n",
            "Epoch: 2456 loss_train: 0.4168 acc_train: 0.8610 loss_val: 0.3885 acc_val: 0.8933 time: 601.5672s\n",
            "Epoch: 2457 loss_train: 0.3838 acc_train: 0.8824 loss_val: 0.4288 acc_val: 0.8833 time: 601.8267s\n",
            "Epoch: 2458 loss_train: 0.4451 acc_train: 0.8713 loss_val: 0.4161 acc_val: 0.8758 time: 602.0873s\n",
            "Epoch: 2459 loss_train: 0.4036 acc_train: 0.8686 loss_val: 0.4400 acc_val: 0.8717 time: 602.3325s\n",
            "Epoch: 2460 loss_train: 0.4207 acc_train: 0.8613 loss_val: 0.3968 acc_val: 0.8900 time: 602.5452s\n",
            "Epoch: 2461 loss_train: 0.4089 acc_train: 0.8733 loss_val: 0.3985 acc_val: 0.8900 time: 602.7922s\n",
            "Epoch: 2462 loss_train: 0.4155 acc_train: 0.8730 loss_val: 0.4159 acc_val: 0.8754 time: 603.0403s\n",
            "Epoch: 2463 loss_train: 0.4038 acc_train: 0.8745 loss_val: 0.4006 acc_val: 0.8871 time: 603.2919s\n",
            "Epoch: 2464 loss_train: 0.3886 acc_train: 0.8762 loss_val: 0.3941 acc_val: 0.8900 time: 603.5182s\n",
            "Epoch: 2465 loss_train: 0.4065 acc_train: 0.8731 loss_val: 0.3724 acc_val: 0.8958 time: 603.7665s\n",
            "Epoch: 2466 loss_train: 0.3743 acc_train: 0.8879 loss_val: 0.3886 acc_val: 0.8842 time: 603.9746s\n",
            "Epoch: 2467 loss_train: 0.3884 acc_train: 0.8744 loss_val: 0.3824 acc_val: 0.8908 time: 604.2290s\n",
            "Epoch: 2468 loss_train: 0.3767 acc_train: 0.8818 loss_val: 0.3808 acc_val: 0.8929 time: 604.4767s\n",
            "Epoch: 2469 loss_train: 0.3850 acc_train: 0.8787 loss_val: 0.3809 acc_val: 0.8950 time: 604.7304s\n",
            "Epoch: 2470 loss_train: 0.3907 acc_train: 0.8812 loss_val: 0.3819 acc_val: 0.8921 time: 604.9767s\n",
            "Epoch: 2471 loss_train: 0.3729 acc_train: 0.8813 loss_val: 0.3950 acc_val: 0.8829 time: 605.2224s\n",
            "Epoch: 2472 loss_train: 0.3863 acc_train: 0.8788 loss_val: 0.3697 acc_val: 0.8946 time: 605.4626s\n",
            "Epoch: 2473 loss_train: 0.3654 acc_train: 0.8891 loss_val: 0.3853 acc_val: 0.8912 time: 605.7146s\n",
            "Epoch: 2474 loss_train: 0.3826 acc_train: 0.8820 loss_val: 0.3919 acc_val: 0.8871 time: 605.9701s\n",
            "Epoch: 2475 loss_train: 0.3885 acc_train: 0.8788 loss_val: 0.3730 acc_val: 0.8950 time: 606.2287s\n",
            "Epoch: 2476 loss_train: 0.3771 acc_train: 0.8818 loss_val: 0.3760 acc_val: 0.8954 time: 606.4725s\n",
            "Epoch: 2477 loss_train: 0.3701 acc_train: 0.8880 loss_val: 0.3673 acc_val: 0.8971 time: 606.7328s\n",
            "Epoch: 2478 loss_train: 0.3687 acc_train: 0.8849 loss_val: 0.3797 acc_val: 0.8908 time: 606.9861s\n",
            "Epoch: 2479 loss_train: 0.3655 acc_train: 0.8847 loss_val: 0.3780 acc_val: 0.8938 time: 607.2370s\n",
            "Epoch: 2480 loss_train: 0.3616 acc_train: 0.8875 loss_val: 0.3696 acc_val: 0.8967 time: 607.4900s\n",
            "Epoch: 2481 loss_train: 0.3641 acc_train: 0.8907 loss_val: 0.3769 acc_val: 0.8929 time: 607.7382s\n",
            "Epoch: 2482 loss_train: 0.3788 acc_train: 0.8808 loss_val: 0.3750 acc_val: 0.8938 time: 607.9694s\n",
            "Epoch: 2483 loss_train: 0.3723 acc_train: 0.8838 loss_val: 0.3682 acc_val: 0.8975 time: 608.2327s\n",
            "Epoch: 2484 loss_train: 0.3651 acc_train: 0.8870 loss_val: 0.3702 acc_val: 0.8925 time: 608.4724s\n",
            "Epoch: 2485 loss_train: 0.3645 acc_train: 0.8871 loss_val: 0.3733 acc_val: 0.8938 time: 608.7176s\n",
            "Epoch: 2486 loss_train: 0.3680 acc_train: 0.8838 loss_val: 0.3763 acc_val: 0.8938 time: 608.9727s\n",
            "Epoch: 2487 loss_train: 0.3732 acc_train: 0.8776 loss_val: 0.3700 acc_val: 0.8954 time: 609.2361s\n",
            "Epoch: 2488 loss_train: 0.3801 acc_train: 0.8827 loss_val: 0.3684 acc_val: 0.8971 time: 609.4409s\n",
            "Epoch: 2489 loss_train: 0.3567 acc_train: 0.8889 loss_val: 0.3770 acc_val: 0.8925 time: 609.6945s\n",
            "Epoch: 2490 loss_train: 0.3678 acc_train: 0.8863 loss_val: 0.3691 acc_val: 0.8975 time: 609.9388s\n",
            "Epoch: 2491 loss_train: 0.3653 acc_train: 0.8905 loss_val: 0.3654 acc_val: 0.8983 time: 610.2044s\n",
            "Epoch: 2492 loss_train: 0.3690 acc_train: 0.8874 loss_val: 0.3661 acc_val: 0.8971 time: 610.4592s\n",
            "Epoch: 2493 loss_train: 0.3687 acc_train: 0.8866 loss_val: 0.3707 acc_val: 0.8942 time: 610.7077s\n",
            "Epoch: 2494 loss_train: 0.3664 acc_train: 0.8841 loss_val: 0.3697 acc_val: 0.8958 time: 610.9569s\n",
            "Epoch: 2495 loss_train: 0.3726 acc_train: 0.8813 loss_val: 0.3666 acc_val: 0.8954 time: 611.2217s\n",
            "Epoch: 2496 loss_train: 0.3636 acc_train: 0.8851 loss_val: 0.3642 acc_val: 0.8983 time: 611.4670s\n",
            "Epoch: 2497 loss_train: 0.3687 acc_train: 0.8826 loss_val: 0.3707 acc_val: 0.8967 time: 611.7138s\n",
            "Epoch: 2498 loss_train: 0.3752 acc_train: 0.8849 loss_val: 0.3646 acc_val: 0.8975 time: 611.9774s\n",
            "Epoch: 2499 loss_train: 0.3657 acc_train: 0.8854 loss_val: 0.3673 acc_val: 0.8971 time: 612.2157s\n",
            "Epoch: 2500 loss_train: 0.3660 acc_train: 0.8838 loss_val: 0.3695 acc_val: 0.8954 time: 612.4654s\n",
            "Epoch: 2501 loss_train: 0.3732 acc_train: 0.8822 loss_val: 0.3636 acc_val: 0.8983 time: 612.7133s\n",
            "Epoch: 2502 loss_train: 0.3515 acc_train: 0.8913 loss_val: 0.3669 acc_val: 0.8942 time: 612.9736s\n",
            "Epoch: 2503 loss_train: 0.3690 acc_train: 0.8852 loss_val: 0.3702 acc_val: 0.8958 time: 613.2298s\n",
            "Epoch: 2504 loss_train: 0.3707 acc_train: 0.8840 loss_val: 0.3654 acc_val: 0.8992 time: 613.4784s\n",
            "Epoch: 2505 loss_train: 0.3581 acc_train: 0.8858 loss_val: 0.3669 acc_val: 0.8929 time: 613.7314s\n",
            "Epoch: 2506 loss_train: 0.3720 acc_train: 0.8865 loss_val: 0.3725 acc_val: 0.8942 time: 613.9892s\n",
            "Epoch: 2507 loss_train: 0.3655 acc_train: 0.8847 loss_val: 0.3768 acc_val: 0.8929 time: 614.1897s\n",
            "Epoch: 2508 loss_train: 0.3667 acc_train: 0.8815 loss_val: 0.3668 acc_val: 0.8975 time: 614.4204s\n",
            "Epoch: 2509 loss_train: 0.3665 acc_train: 0.8880 loss_val: 0.3657 acc_val: 0.8979 time: 614.6708s\n",
            "Epoch: 2510 loss_train: 0.3732 acc_train: 0.8847 loss_val: 0.3714 acc_val: 0.8942 time: 614.9245s\n",
            "Epoch: 2511 loss_train: 0.3700 acc_train: 0.8837 loss_val: 0.3648 acc_val: 0.8988 time: 615.1588s\n",
            "Epoch: 2512 loss_train: 0.3688 acc_train: 0.8860 loss_val: 0.3633 acc_val: 0.8971 time: 615.4113s\n",
            "Epoch: 2513 loss_train: 0.3556 acc_train: 0.8889 loss_val: 0.3730 acc_val: 0.8950 time: 615.6668s\n",
            "Epoch: 2514 loss_train: 0.3763 acc_train: 0.8845 loss_val: 0.3649 acc_val: 0.8979 time: 615.9140s\n",
            "Epoch: 2515 loss_train: 0.3546 acc_train: 0.8897 loss_val: 0.3635 acc_val: 0.8967 time: 616.1666s\n",
            "Epoch: 2516 loss_train: 0.3650 acc_train: 0.8841 loss_val: 0.3712 acc_val: 0.8942 time: 616.4154s\n",
            "Epoch: 2517 loss_train: 0.3560 acc_train: 0.8874 loss_val: 0.3664 acc_val: 0.8979 time: 616.6636s\n",
            "Epoch: 2518 loss_train: 0.3625 acc_train: 0.8833 loss_val: 0.3699 acc_val: 0.8988 time: 616.9162s\n",
            "Epoch: 2519 loss_train: 0.3829 acc_train: 0.8810 loss_val: 0.3669 acc_val: 0.8946 time: 617.1853s\n",
            "Epoch: 2520 loss_train: 0.3609 acc_train: 0.8940 loss_val: 0.3886 acc_val: 0.8858 time: 617.4344s\n",
            "Epoch: 2521 loss_train: 0.3873 acc_train: 0.8757 loss_val: 0.3649 acc_val: 0.8942 time: 617.6836s\n",
            "Epoch: 2522 loss_train: 0.3751 acc_train: 0.8846 loss_val: 0.3685 acc_val: 0.8925 time: 617.9284s\n",
            "Epoch: 2523 loss_train: 0.3614 acc_train: 0.8901 loss_val: 0.3755 acc_val: 0.8938 time: 618.1810s\n",
            "Epoch: 2524 loss_train: 0.3656 acc_train: 0.8822 loss_val: 0.3782 acc_val: 0.8912 time: 618.4528s\n",
            "Epoch: 2525 loss_train: 0.3806 acc_train: 0.8771 loss_val: 0.3722 acc_val: 0.8929 time: 618.6977s\n",
            "Epoch: 2526 loss_train: 0.3850 acc_train: 0.8871 loss_val: 0.3656 acc_val: 0.8938 time: 618.9415s\n",
            "Epoch: 2527 loss_train: 0.3614 acc_train: 0.8870 loss_val: 0.3976 acc_val: 0.8825 time: 619.2015s\n",
            "Epoch: 2528 loss_train: 0.3898 acc_train: 0.8725 loss_val: 0.3623 acc_val: 0.8962 time: 619.4434s\n",
            "Epoch: 2529 loss_train: 0.3537 acc_train: 0.8891 loss_val: 0.3816 acc_val: 0.8933 time: 619.7014s\n",
            "Epoch: 2530 loss_train: 0.3870 acc_train: 0.8858 loss_val: 0.3897 acc_val: 0.8850 time: 619.9511s\n",
            "Epoch: 2531 loss_train: 0.3685 acc_train: 0.8830 loss_val: 0.3938 acc_val: 0.8829 time: 620.2036s\n",
            "Epoch: 2532 loss_train: 0.3804 acc_train: 0.8775 loss_val: 0.3789 acc_val: 0.8888 time: 620.4577s\n",
            "Epoch: 2533 loss_train: 0.3880 acc_train: 0.8798 loss_val: 0.3691 acc_val: 0.8938 time: 620.7201s\n",
            "Epoch: 2534 loss_train: 0.3605 acc_train: 0.8927 loss_val: 0.4092 acc_val: 0.8775 time: 620.9710s\n",
            "Epoch: 2535 loss_train: 0.4005 acc_train: 0.8684 loss_val: 0.3681 acc_val: 0.8971 time: 621.2299s\n",
            "Epoch: 2536 loss_train: 0.3767 acc_train: 0.8769 loss_val: 0.3960 acc_val: 0.8904 time: 621.4833s\n",
            "Epoch: 2537 loss_train: 0.4155 acc_train: 0.8767 loss_val: 0.3932 acc_val: 0.8858 time: 621.7422s\n",
            "Epoch: 2538 loss_train: 0.3841 acc_train: 0.8785 loss_val: 0.4102 acc_val: 0.8792 time: 622.0108s\n",
            "Epoch: 2539 loss_train: 0.3995 acc_train: 0.8727 loss_val: 0.3700 acc_val: 0.8933 time: 622.2759s\n",
            "Epoch: 2540 loss_train: 0.3768 acc_train: 0.8880 loss_val: 0.3727 acc_val: 0.8950 time: 622.5408s\n",
            "Epoch: 2541 loss_train: 0.3883 acc_train: 0.8854 loss_val: 0.3878 acc_val: 0.8879 time: 622.8004s\n",
            "Epoch: 2542 loss_train: 0.3795 acc_train: 0.8793 loss_val: 0.3752 acc_val: 0.8921 time: 623.0521s\n",
            "Epoch: 2543 loss_train: 0.3658 acc_train: 0.8876 loss_val: 0.3650 acc_val: 0.8938 time: 623.3029s\n",
            "Epoch: 2544 loss_train: 0.3599 acc_train: 0.8865 loss_val: 0.3636 acc_val: 0.8958 time: 623.5606s\n",
            "Epoch: 2545 loss_train: 0.3753 acc_train: 0.8829 loss_val: 0.3742 acc_val: 0.8942 time: 623.8131s\n",
            "Epoch: 2546 loss_train: 0.3680 acc_train: 0.8798 loss_val: 0.3719 acc_val: 0.8954 time: 624.0737s\n",
            "Epoch: 2547 loss_train: 0.3749 acc_train: 0.8813 loss_val: 0.3710 acc_val: 0.8933 time: 624.3227s\n",
            "Epoch: 2548 loss_train: 0.3778 acc_train: 0.8880 loss_val: 0.3658 acc_val: 0.8933 time: 624.5763s\n",
            "Epoch: 2549 loss_train: 0.3590 acc_train: 0.8866 loss_val: 0.4011 acc_val: 0.8758 time: 624.8318s\n",
            "Epoch: 2550 loss_train: 0.3901 acc_train: 0.8720 loss_val: 0.3626 acc_val: 0.8967 time: 625.0892s\n",
            "Epoch: 2551 loss_train: 0.3566 acc_train: 0.8890 loss_val: 0.3756 acc_val: 0.8933 time: 625.3461s\n",
            "Epoch: 2552 loss_train: 0.3750 acc_train: 0.8864 loss_val: 0.3706 acc_val: 0.8958 time: 625.6005s\n",
            "Epoch: 2553 loss_train: 0.3617 acc_train: 0.8816 loss_val: 0.3784 acc_val: 0.8892 time: 625.8591s\n",
            "Epoch: 2554 loss_train: 0.3694 acc_train: 0.8790 loss_val: 0.3609 acc_val: 0.8958 time: 626.1198s\n",
            "Epoch: 2555 loss_train: 0.3527 acc_train: 0.8885 loss_val: 0.3627 acc_val: 0.8946 time: 626.3749s\n",
            "Epoch: 2556 loss_train: 0.3545 acc_train: 0.8895 loss_val: 0.3672 acc_val: 0.8967 time: 626.6323s\n",
            "Epoch: 2557 loss_train: 0.3594 acc_train: 0.8875 loss_val: 0.3754 acc_val: 0.8946 time: 626.8856s\n",
            "Epoch: 2558 loss_train: 0.3579 acc_train: 0.8858 loss_val: 0.3617 acc_val: 0.8988 time: 627.1382s\n",
            "Epoch: 2559 loss_train: 0.3513 acc_train: 0.8896 loss_val: 0.3649 acc_val: 0.8942 time: 627.3964s\n",
            "Epoch: 2560 loss_train: 0.3552 acc_train: 0.8887 loss_val: 0.3656 acc_val: 0.8942 time: 627.6678s\n",
            "Epoch: 2561 loss_train: 0.3527 acc_train: 0.8871 loss_val: 0.3604 acc_val: 0.8975 time: 627.9158s\n",
            "Epoch: 2562 loss_train: 0.3574 acc_train: 0.8844 loss_val: 0.3643 acc_val: 0.8983 time: 628.1700s\n",
            "Epoch: 2563 loss_train: 0.3530 acc_train: 0.8901 loss_val: 0.3619 acc_val: 0.8975 time: 628.4313s\n",
            "Epoch: 2564 loss_train: 0.3402 acc_train: 0.8948 loss_val: 0.3690 acc_val: 0.8938 time: 628.6669s\n",
            "Epoch: 2565 loss_train: 0.3536 acc_train: 0.8880 loss_val: 0.3629 acc_val: 0.8958 time: 628.9584s\n",
            "Epoch: 2566 loss_train: 0.3502 acc_train: 0.8871 loss_val: 0.3630 acc_val: 0.8996 time: 629.2201s\n",
            "Epoch: 2567 loss_train: 0.3592 acc_train: 0.8915 loss_val: 0.3655 acc_val: 0.8967 time: 629.4857s\n",
            "Epoch: 2568 loss_train: 0.3502 acc_train: 0.8919 loss_val: 0.3759 acc_val: 0.8950 time: 629.7379s\n",
            "Epoch: 2569 loss_train: 0.3672 acc_train: 0.8836 loss_val: 0.3583 acc_val: 0.8933 time: 629.9901s\n",
            "Epoch: 2570 loss_train: 0.3473 acc_train: 0.8918 loss_val: 0.3575 acc_val: 0.8950 time: 630.2396s\n",
            "Epoch: 2571 loss_train: 0.3462 acc_train: 0.8932 loss_val: 0.3657 acc_val: 0.8933 time: 630.4988s\n",
            "Epoch: 2572 loss_train: 0.3524 acc_train: 0.8856 loss_val: 0.3566 acc_val: 0.8996 time: 630.7585s\n",
            "Epoch: 2573 loss_train: 0.3445 acc_train: 0.8935 loss_val: 0.3641 acc_val: 0.8954 time: 631.0231s\n",
            "Epoch: 2574 loss_train: 0.3655 acc_train: 0.8900 loss_val: 0.3609 acc_val: 0.8958 time: 631.2706s\n",
            "Epoch: 2575 loss_train: 0.3486 acc_train: 0.8881 loss_val: 0.3622 acc_val: 0.8917 time: 631.5372s\n",
            "Epoch: 2576 loss_train: 0.3605 acc_train: 0.8840 loss_val: 0.3538 acc_val: 0.8988 time: 631.8025s\n",
            "Epoch: 2577 loss_train: 0.3457 acc_train: 0.8930 loss_val: 0.3619 acc_val: 0.8975 time: 632.0533s\n",
            "Epoch: 2578 loss_train: 0.3381 acc_train: 0.8975 loss_val: 0.3631 acc_val: 0.8988 time: 632.3197s\n",
            "Epoch: 2579 loss_train: 0.3537 acc_train: 0.8909 loss_val: 0.3531 acc_val: 0.8967 time: 632.5792s\n",
            "Epoch: 2580 loss_train: 0.3450 acc_train: 0.8922 loss_val: 0.3566 acc_val: 0.8958 time: 632.8315s\n",
            "Epoch: 2581 loss_train: 0.3488 acc_train: 0.8920 loss_val: 0.3646 acc_val: 0.8938 time: 633.0844s\n",
            "Epoch: 2582 loss_train: 0.3439 acc_train: 0.8873 loss_val: 0.3648 acc_val: 0.8954 time: 633.3229s\n",
            "Epoch: 2583 loss_train: 0.3506 acc_train: 0.8879 loss_val: 0.3592 acc_val: 0.8958 time: 633.5805s\n",
            "Epoch: 2584 loss_train: 0.3515 acc_train: 0.8957 loss_val: 0.3562 acc_val: 0.8967 time: 633.8466s\n",
            "Epoch: 2585 loss_train: 0.3560 acc_train: 0.8875 loss_val: 0.3635 acc_val: 0.8938 time: 634.1006s\n",
            "Epoch: 2586 loss_train: 0.3488 acc_train: 0.8824 loss_val: 0.3577 acc_val: 0.8933 time: 634.3564s\n",
            "Epoch: 2587 loss_train: 0.3524 acc_train: 0.8913 loss_val: 0.3552 acc_val: 0.8962 time: 634.6286s\n",
            "Epoch: 2588 loss_train: 0.3502 acc_train: 0.8930 loss_val: 0.3722 acc_val: 0.8912 time: 634.8798s\n",
            "Epoch: 2589 loss_train: 0.3562 acc_train: 0.8816 loss_val: 0.3569 acc_val: 0.8988 time: 635.1302s\n",
            "Epoch: 2590 loss_train: 0.3384 acc_train: 0.8900 loss_val: 0.3638 acc_val: 0.8971 time: 635.3719s\n",
            "Epoch: 2591 loss_train: 0.3578 acc_train: 0.8907 loss_val: 0.3618 acc_val: 0.8967 time: 635.6418s\n",
            "Epoch: 2592 loss_train: 0.3415 acc_train: 0.8938 loss_val: 0.3813 acc_val: 0.8904 time: 635.8893s\n",
            "Epoch: 2593 loss_train: 0.3586 acc_train: 0.8826 loss_val: 0.3564 acc_val: 0.8950 time: 636.1503s\n",
            "Epoch: 2594 loss_train: 0.3430 acc_train: 0.8943 loss_val: 0.3543 acc_val: 0.8971 time: 636.4053s\n",
            "Epoch: 2595 loss_train: 0.3595 acc_train: 0.8930 loss_val: 0.3837 acc_val: 0.8883 time: 636.6782s\n",
            "Epoch: 2596 loss_train: 0.3629 acc_train: 0.8822 loss_val: 0.3630 acc_val: 0.8958 time: 636.9224s\n",
            "Epoch: 2597 loss_train: 0.3536 acc_train: 0.8886 loss_val: 0.3609 acc_val: 0.8971 time: 637.1879s\n",
            "Epoch: 2598 loss_train: 0.3512 acc_train: 0.8944 loss_val: 0.3558 acc_val: 0.8967 time: 637.4047s\n",
            "Epoch: 2599 loss_train: 0.3471 acc_train: 0.8889 loss_val: 0.3689 acc_val: 0.8879 time: 637.6698s\n",
            "Epoch: 2600 loss_train: 0.3517 acc_train: 0.8804 loss_val: 0.3526 acc_val: 0.8975 time: 637.9227s\n",
            "Epoch: 2601 loss_train: 0.3425 acc_train: 0.8920 loss_val: 0.3671 acc_val: 0.8938 time: 638.1671s\n",
            "Epoch: 2602 loss_train: 0.3640 acc_train: 0.8826 loss_val: 0.3764 acc_val: 0.8862 time: 638.3940s\n",
            "Epoch: 2603 loss_train: 0.3519 acc_train: 0.8824 loss_val: 0.3620 acc_val: 0.8908 time: 638.6551s\n",
            "Epoch: 2604 loss_train: 0.3488 acc_train: 0.8856 loss_val: 0.3611 acc_val: 0.8967 time: 638.9309s\n",
            "Epoch: 2605 loss_train: 0.3485 acc_train: 0.8949 loss_val: 0.3790 acc_val: 0.8954 time: 639.1813s\n",
            "Epoch: 2606 loss_train: 0.3687 acc_train: 0.8853 loss_val: 0.3778 acc_val: 0.8875 time: 639.4499s\n",
            "Epoch: 2607 loss_train: 0.3673 acc_train: 0.8793 loss_val: 0.3644 acc_val: 0.8925 time: 639.7094s\n",
            "Epoch: 2608 loss_train: 0.3585 acc_train: 0.8875 loss_val: 0.3549 acc_val: 0.8983 time: 639.9622s\n",
            "Epoch: 2609 loss_train: 0.3417 acc_train: 0.8953 loss_val: 0.3689 acc_val: 0.8933 time: 640.2228s\n",
            "Epoch: 2610 loss_train: 0.3466 acc_train: 0.8897 loss_val: 0.3666 acc_val: 0.8975 time: 640.4652s\n",
            "Epoch: 2611 loss_train: 0.3522 acc_train: 0.8969 loss_val: 0.3589 acc_val: 0.8967 time: 640.7146s\n",
            "Epoch: 2612 loss_train: 0.3482 acc_train: 0.8910 loss_val: 0.3543 acc_val: 0.8967 time: 640.9760s\n",
            "Epoch: 2613 loss_train: 0.3418 acc_train: 0.8903 loss_val: 0.3557 acc_val: 0.8958 time: 641.2367s\n",
            "Epoch: 2614 loss_train: 0.3448 acc_train: 0.8880 loss_val: 0.3571 acc_val: 0.8975 time: 641.5086s\n",
            "Epoch: 2615 loss_train: 0.3420 acc_train: 0.8945 loss_val: 0.3580 acc_val: 0.8996 time: 641.7861s\n",
            "Epoch: 2616 loss_train: 0.3367 acc_train: 0.8981 loss_val: 0.3641 acc_val: 0.8971 time: 642.0360s\n",
            "Epoch: 2617 loss_train: 0.3347 acc_train: 0.8958 loss_val: 0.3607 acc_val: 0.8954 time: 642.2933s\n",
            "Epoch: 2618 loss_train: 0.3443 acc_train: 0.8915 loss_val: 0.3538 acc_val: 0.9012 time: 642.5397s\n",
            "Epoch: 2619 loss_train: 0.3463 acc_train: 0.8942 loss_val: 0.3559 acc_val: 0.8988 time: 642.8105s\n",
            "Epoch: 2620 loss_train: 0.3338 acc_train: 0.8901 loss_val: 0.3583 acc_val: 0.8983 time: 643.0609s\n",
            "Epoch: 2621 loss_train: 0.3524 acc_train: 0.8895 loss_val: 0.3580 acc_val: 0.8971 time: 643.3118s\n",
            "Epoch: 2622 loss_train: 0.3387 acc_train: 0.8913 loss_val: 0.3556 acc_val: 0.8983 time: 643.5612s\n",
            "Epoch: 2623 loss_train: 0.3441 acc_train: 0.8923 loss_val: 0.3531 acc_val: 0.9012 time: 643.8288s\n",
            "Epoch: 2624 loss_train: 0.3362 acc_train: 0.8941 loss_val: 0.3540 acc_val: 0.8983 time: 644.0860s\n",
            "Epoch: 2625 loss_train: 0.3442 acc_train: 0.8896 loss_val: 0.3550 acc_val: 0.8983 time: 644.3594s\n",
            "Epoch: 2626 loss_train: 0.3350 acc_train: 0.8940 loss_val: 0.3555 acc_val: 0.8992 time: 644.6088s\n",
            "Epoch: 2627 loss_train: 0.3537 acc_train: 0.8903 loss_val: 0.3702 acc_val: 0.8925 time: 644.8578s\n",
            "Epoch: 2628 loss_train: 0.3450 acc_train: 0.8871 loss_val: 0.3574 acc_val: 0.8946 time: 645.0741s\n",
            "Epoch: 2629 loss_train: 0.3393 acc_train: 0.8944 loss_val: 0.3565 acc_val: 0.8971 time: 645.3172s\n",
            "Epoch: 2630 loss_train: 0.3408 acc_train: 0.8951 loss_val: 0.3588 acc_val: 0.8962 time: 645.5764s\n",
            "Epoch: 2631 loss_train: 0.3416 acc_train: 0.8889 loss_val: 0.3614 acc_val: 0.8933 time: 645.8356s\n",
            "Epoch: 2632 loss_train: 0.3386 acc_train: 0.8863 loss_val: 0.3629 acc_val: 0.8962 time: 646.0774s\n",
            "Epoch: 2633 loss_train: 0.3574 acc_train: 0.8889 loss_val: 0.3812 acc_val: 0.8904 time: 646.3248s\n",
            "Epoch: 2634 loss_train: 0.3535 acc_train: 0.8878 loss_val: 0.3704 acc_val: 0.8933 time: 646.5754s\n",
            "Epoch: 2635 loss_train: 0.3502 acc_train: 0.8884 loss_val: 0.3643 acc_val: 0.9000 time: 646.8404s\n",
            "Epoch: 2636 loss_train: 0.3697 acc_train: 0.8911 loss_val: 0.3551 acc_val: 0.8958 time: 647.0872s\n",
            "Epoch: 2637 loss_train: 0.3397 acc_train: 0.8887 loss_val: 0.3623 acc_val: 0.8958 time: 647.3354s\n",
            "Epoch: 2638 loss_train: 0.3404 acc_train: 0.8877 loss_val: 0.3593 acc_val: 0.8962 time: 647.5978s\n",
            "Epoch: 2639 loss_train: 0.3521 acc_train: 0.8921 loss_val: 0.3523 acc_val: 0.8996 time: 647.8609s\n",
            "Epoch: 2640 loss_train: 0.3461 acc_train: 0.8873 loss_val: 0.3665 acc_val: 0.8950 time: 648.0918s\n",
            "Epoch: 2641 loss_train: 0.3435 acc_train: 0.8881 loss_val: 0.3576 acc_val: 0.8983 time: 648.3402s\n",
            "Epoch: 2642 loss_train: 0.3401 acc_train: 0.8913 loss_val: 0.3605 acc_val: 0.8954 time: 648.5902s\n",
            "Epoch: 2643 loss_train: 0.3498 acc_train: 0.8900 loss_val: 0.3545 acc_val: 0.8988 time: 648.8409s\n",
            "Epoch: 2644 loss_train: 0.3466 acc_train: 0.8898 loss_val: 0.3582 acc_val: 0.8933 time: 649.0295s\n",
            "Epoch: 2645 loss_train: 0.3457 acc_train: 0.8859 loss_val: 0.3518 acc_val: 0.8996 time: 649.2852s\n",
            "Epoch: 2646 loss_train: 0.3401 acc_train: 0.8926 loss_val: 0.3572 acc_val: 0.8958 time: 649.5276s\n",
            "Epoch: 2647 loss_train: 0.3464 acc_train: 0.8904 loss_val: 0.3607 acc_val: 0.8958 time: 649.7838s\n",
            "Epoch: 2648 loss_train: 0.3355 acc_train: 0.8932 loss_val: 0.3521 acc_val: 0.8975 time: 650.0403s\n",
            "Epoch: 2649 loss_train: 0.3370 acc_train: 0.8912 loss_val: 0.3527 acc_val: 0.9000 time: 650.2950s\n",
            "Epoch: 2650 loss_train: 0.3296 acc_train: 0.8964 loss_val: 0.3546 acc_val: 0.9000 time: 650.5529s\n",
            "Epoch: 2651 loss_train: 0.3407 acc_train: 0.8925 loss_val: 0.3613 acc_val: 0.8967 time: 650.8161s\n",
            "Epoch: 2652 loss_train: 0.3455 acc_train: 0.8898 loss_val: 0.3521 acc_val: 0.8975 time: 651.0601s\n",
            "Epoch: 2653 loss_train: 0.3424 acc_train: 0.8924 loss_val: 0.3505 acc_val: 0.9000 time: 651.3079s\n",
            "Epoch: 2654 loss_train: 0.3318 acc_train: 0.8964 loss_val: 0.3608 acc_val: 0.8929 time: 651.5557s\n",
            "Epoch: 2655 loss_train: 0.3354 acc_train: 0.8860 loss_val: 0.3559 acc_val: 0.9000 time: 651.8207s\n",
            "Epoch: 2656 loss_train: 0.3329 acc_train: 0.8957 loss_val: 0.3593 acc_val: 0.8962 time: 652.0781s\n",
            "Epoch: 2657 loss_train: 0.3542 acc_train: 0.8927 loss_val: 0.3608 acc_val: 0.8967 time: 652.3155s\n",
            "Epoch: 2658 loss_train: 0.3425 acc_train: 0.8924 loss_val: 0.3623 acc_val: 0.8938 time: 652.5638s\n",
            "Epoch: 2659 loss_train: 0.3366 acc_train: 0.8931 loss_val: 0.3496 acc_val: 0.8983 time: 652.8214s\n",
            "Epoch: 2660 loss_train: 0.3308 acc_train: 0.8948 loss_val: 0.3535 acc_val: 0.8988 time: 653.0763s\n",
            "Epoch: 2661 loss_train: 0.3337 acc_train: 0.8934 loss_val: 0.3612 acc_val: 0.8971 time: 653.3260s\n",
            "Epoch: 2662 loss_train: 0.3447 acc_train: 0.8912 loss_val: 0.3525 acc_val: 0.9008 time: 653.5753s\n",
            "Epoch: 2663 loss_train: 0.3415 acc_train: 0.8930 loss_val: 0.3539 acc_val: 0.8983 time: 653.8198s\n",
            "Epoch: 2664 loss_train: 0.3419 acc_train: 0.8904 loss_val: 0.3608 acc_val: 0.8950 time: 654.0812s\n",
            "Epoch: 2665 loss_train: 0.3465 acc_train: 0.8863 loss_val: 0.3530 acc_val: 0.8962 time: 654.3384s\n",
            "Epoch: 2666 loss_train: 0.3423 acc_train: 0.8929 loss_val: 0.3555 acc_val: 0.9004 time: 654.5963s\n",
            "Epoch: 2667 loss_train: 0.3386 acc_train: 0.8937 loss_val: 0.3490 acc_val: 0.8996 time: 654.8498s\n",
            "Epoch: 2668 loss_train: 0.3381 acc_train: 0.8922 loss_val: 0.3589 acc_val: 0.8975 time: 655.1111s\n",
            "Epoch: 2669 loss_train: 0.3403 acc_train: 0.8870 loss_val: 0.3527 acc_val: 0.8992 time: 655.3677s\n",
            "Epoch: 2670 loss_train: 0.3421 acc_train: 0.8916 loss_val: 0.3523 acc_val: 0.8975 time: 655.6271s\n",
            "Epoch: 2671 loss_train: 0.3388 acc_train: 0.8947 loss_val: 0.3496 acc_val: 0.9000 time: 655.8816s\n",
            "Epoch: 2672 loss_train: 0.3307 acc_train: 0.8920 loss_val: 0.3524 acc_val: 0.8996 time: 656.1291s\n",
            "Epoch: 2673 loss_train: 0.3370 acc_train: 0.8946 loss_val: 0.3550 acc_val: 0.8992 time: 656.3795s\n",
            "Epoch: 2674 loss_train: 0.3334 acc_train: 0.8938 loss_val: 0.3520 acc_val: 0.9004 time: 656.6511s\n",
            "Epoch: 2675 loss_train: 0.3338 acc_train: 0.8952 loss_val: 0.3498 acc_val: 0.8996 time: 656.9192s\n",
            "Epoch: 2676 loss_train: 0.3326 acc_train: 0.8938 loss_val: 0.3504 acc_val: 0.9004 time: 657.1683s\n",
            "Epoch: 2677 loss_train: 0.3418 acc_train: 0.8951 loss_val: 0.3636 acc_val: 0.8971 time: 657.4166s\n",
            "Epoch: 2678 loss_train: 0.3376 acc_train: 0.8913 loss_val: 0.3523 acc_val: 0.8992 time: 657.6674s\n",
            "Epoch: 2679 loss_train: 0.3274 acc_train: 0.8940 loss_val: 0.3515 acc_val: 0.9008 time: 657.9195s\n",
            "Epoch: 2680 loss_train: 0.3458 acc_train: 0.8915 loss_val: 0.3608 acc_val: 0.8971 time: 658.1737s\n",
            "Epoch: 2681 loss_train: 0.3423 acc_train: 0.8882 loss_val: 0.3566 acc_val: 0.8971 time: 658.4250s\n",
            "Epoch: 2682 loss_train: 0.3257 acc_train: 0.8975 loss_val: 0.3517 acc_val: 0.9012 time: 658.6730s\n",
            "Epoch: 2683 loss_train: 0.3361 acc_train: 0.8966 loss_val: 0.3590 acc_val: 0.8946 time: 658.9284s\n",
            "Epoch: 2684 loss_train: 0.3496 acc_train: 0.8853 loss_val: 0.3498 acc_val: 0.9004 time: 659.1865s\n",
            "Epoch: 2685 loss_train: 0.3327 acc_train: 0.8942 loss_val: 0.3541 acc_val: 0.8954 time: 659.4441s\n",
            "Epoch: 2686 loss_train: 0.3311 acc_train: 0.8957 loss_val: 0.3538 acc_val: 0.8983 time: 659.6956s\n",
            "Epoch: 2687 loss_train: 0.3312 acc_train: 0.8908 loss_val: 0.3513 acc_val: 0.8975 time: 659.9672s\n",
            "Epoch: 2688 loss_train: 0.3316 acc_train: 0.8916 loss_val: 0.3499 acc_val: 0.8996 time: 660.2131s\n",
            "Epoch: 2689 loss_train: 0.3348 acc_train: 0.8969 loss_val: 0.3623 acc_val: 0.8967 time: 660.4641s\n",
            "Epoch: 2690 loss_train: 0.3369 acc_train: 0.8960 loss_val: 0.3629 acc_val: 0.8942 time: 660.7273s\n",
            "Epoch: 2691 loss_train: 0.3419 acc_train: 0.8863 loss_val: 0.3556 acc_val: 0.8979 time: 660.9650s\n",
            "Epoch: 2692 loss_train: 0.3477 acc_train: 0.8938 loss_val: 0.3642 acc_val: 0.8962 time: 661.2212s\n",
            "Epoch: 2693 loss_train: 0.3448 acc_train: 0.8877 loss_val: 0.3617 acc_val: 0.9000 time: 661.4794s\n",
            "Epoch: 2694 loss_train: 0.3382 acc_train: 0.8947 loss_val: 0.3559 acc_val: 0.8975 time: 661.7284s\n",
            "Epoch: 2695 loss_train: 0.3411 acc_train: 0.8947 loss_val: 0.3571 acc_val: 0.8933 time: 662.0087s\n",
            "Epoch: 2696 loss_train: 0.3498 acc_train: 0.8862 loss_val: 0.3556 acc_val: 0.8962 time: 662.2623s\n",
            "Epoch: 2697 loss_train: 0.3318 acc_train: 0.8920 loss_val: 0.3786 acc_val: 0.8904 time: 662.5169s\n",
            "Epoch: 2698 loss_train: 0.3665 acc_train: 0.8914 loss_val: 0.3782 acc_val: 0.8858 time: 662.7640s\n",
            "Epoch: 2699 loss_train: 0.3639 acc_train: 0.8754 loss_val: 0.3776 acc_val: 0.8888 time: 663.0156s\n",
            "Epoch: 2700 loss_train: 0.3734 acc_train: 0.8784 loss_val: 0.3546 acc_val: 0.8983 time: 663.2635s\n",
            "Epoch: 2701 loss_train: 0.3481 acc_train: 0.8902 loss_val: 0.4021 acc_val: 0.8833 time: 663.5147s\n",
            "Epoch: 2702 loss_train: 0.3827 acc_train: 0.8840 loss_val: 0.3717 acc_val: 0.8871 time: 663.7508s\n",
            "Epoch: 2703 loss_train: 0.3462 acc_train: 0.8851 loss_val: 0.3770 acc_val: 0.8883 time: 663.9968s\n",
            "Epoch: 2704 loss_train: 0.3810 acc_train: 0.8803 loss_val: 0.3577 acc_val: 0.8938 time: 664.2493s\n",
            "Epoch: 2705 loss_train: 0.3523 acc_train: 0.8855 loss_val: 0.3742 acc_val: 0.8933 time: 664.5006s\n",
            "Epoch: 2706 loss_train: 0.3473 acc_train: 0.8914 loss_val: 0.3778 acc_val: 0.8938 time: 664.7573s\n",
            "Epoch: 2707 loss_train: 0.3471 acc_train: 0.8938 loss_val: 0.3668 acc_val: 0.8938 time: 664.9945s\n",
            "Epoch: 2708 loss_train: 0.3436 acc_train: 0.8904 loss_val: 0.3670 acc_val: 0.8900 time: 665.2404s\n",
            "Epoch: 2709 loss_train: 0.3566 acc_train: 0.8799 loss_val: 0.3591 acc_val: 0.8958 time: 665.4891s\n",
            "Epoch: 2710 loss_train: 0.3373 acc_train: 0.8914 loss_val: 0.3611 acc_val: 0.8958 time: 665.7398s\n",
            "Epoch: 2711 loss_train: 0.3350 acc_train: 0.8935 loss_val: 0.3738 acc_val: 0.8950 time: 665.9908s\n",
            "Epoch: 2712 loss_train: 0.3486 acc_train: 0.8908 loss_val: 0.3740 acc_val: 0.9000 time: 666.2455s\n",
            "Epoch: 2713 loss_train: 0.3502 acc_train: 0.8946 loss_val: 0.3603 acc_val: 0.8988 time: 666.4751s\n",
            "Epoch: 2714 loss_train: 0.3422 acc_train: 0.8940 loss_val: 0.3592 acc_val: 0.8929 time: 666.7238s\n",
            "Epoch: 2715 loss_train: 0.3418 acc_train: 0.8875 loss_val: 0.3659 acc_val: 0.8921 time: 666.9839s\n",
            "Epoch: 2716 loss_train: 0.3550 acc_train: 0.8853 loss_val: 0.3556 acc_val: 0.8971 time: 667.2322s\n",
            "Epoch: 2717 loss_train: 0.3436 acc_train: 0.8921 loss_val: 0.3644 acc_val: 0.8992 time: 667.4844s\n",
            "Epoch: 2718 loss_train: 0.3446 acc_train: 0.8954 loss_val: 0.3715 acc_val: 0.8946 time: 667.7376s\n",
            "Epoch: 2719 loss_train: 0.3382 acc_train: 0.8902 loss_val: 0.3639 acc_val: 0.8979 time: 667.9895s\n",
            "Epoch: 2720 loss_train: 0.3508 acc_train: 0.8909 loss_val: 0.3626 acc_val: 0.8983 time: 668.2432s\n",
            "Epoch: 2721 loss_train: 0.3588 acc_train: 0.8896 loss_val: 0.3854 acc_val: 0.8854 time: 668.4904s\n",
            "Epoch: 2722 loss_train: 0.3456 acc_train: 0.8863 loss_val: 0.3593 acc_val: 0.8971 time: 668.7399s\n",
            "Epoch: 2723 loss_train: 0.3312 acc_train: 0.8923 loss_val: 0.3784 acc_val: 0.8988 time: 669.0011s\n",
            "Epoch: 2724 loss_train: 0.3658 acc_train: 0.8893 loss_val: 0.3651 acc_val: 0.8946 time: 669.2456s\n",
            "Epoch: 2725 loss_train: 0.3305 acc_train: 0.8929 loss_val: 0.3682 acc_val: 0.8929 time: 669.4981s\n",
            "Epoch: 2726 loss_train: 0.3508 acc_train: 0.8870 loss_val: 0.3736 acc_val: 0.8983 time: 669.7471s\n",
            "Epoch: 2727 loss_train: 0.3694 acc_train: 0.8865 loss_val: 0.3959 acc_val: 0.8850 time: 669.9984s\n",
            "Epoch: 2728 loss_train: 0.3549 acc_train: 0.8845 loss_val: 0.3935 acc_val: 0.8858 time: 670.2463s\n",
            "Epoch: 2729 loss_train: 0.3679 acc_train: 0.8810 loss_val: 0.4099 acc_val: 0.8962 time: 670.4968s\n",
            "Epoch: 2730 loss_train: 0.4158 acc_train: 0.8831 loss_val: 0.3692 acc_val: 0.8896 time: 670.7522s\n",
            "Epoch: 2731 loss_train: 0.3449 acc_train: 0.8876 loss_val: 0.3702 acc_val: 0.8942 time: 671.0209s\n",
            "Epoch: 2732 loss_train: 0.3506 acc_train: 0.8843 loss_val: 0.3956 acc_val: 0.8950 time: 671.2669s\n",
            "Epoch: 2733 loss_train: 0.4118 acc_train: 0.8816 loss_val: 0.5483 acc_val: 0.8496 time: 671.5166s\n",
            "Epoch: 2734 loss_train: 0.5150 acc_train: 0.8446 loss_val: 0.4056 acc_val: 0.8842 time: 671.7420s\n",
            "Epoch: 2735 loss_train: 0.4117 acc_train: 0.8697 loss_val: 0.5347 acc_val: 0.8375 time: 672.0055s\n",
            "Epoch: 2736 loss_train: 0.5383 acc_train: 0.8201 loss_val: 0.6343 acc_val: 0.7979 time: 672.2743s\n",
            "Epoch: 2737 loss_train: 0.5933 acc_train: 0.7946 loss_val: 0.6539 acc_val: 0.8025 time: 672.5096s\n",
            "Epoch: 2738 loss_train: 0.6679 acc_train: 0.7946 loss_val: 0.4892 acc_val: 0.8313 time: 672.7669s\n",
            "Epoch: 2739 loss_train: 0.4906 acc_train: 0.8246 loss_val: 0.6260 acc_val: 0.8004 time: 673.0215s\n",
            "Epoch: 2740 loss_train: 0.6525 acc_train: 0.7767 loss_val: 0.5952 acc_val: 0.8254 time: 673.2670s\n",
            "Epoch: 2741 loss_train: 0.6065 acc_train: 0.8132 loss_val: 0.6655 acc_val: 0.8196 time: 673.5136s\n",
            "Epoch: 2742 loss_train: 0.6554 acc_train: 0.8165 loss_val: 0.5258 acc_val: 0.8158 time: 673.7740s\n",
            "Epoch: 2743 loss_train: 0.5243 acc_train: 0.8124 loss_val: 0.7714 acc_val: 0.7262 time: 674.0303s\n",
            "Epoch: 2744 loss_train: 0.8205 acc_train: 0.7005 loss_val: 0.8010 acc_val: 0.7454 time: 674.2781s\n",
            "Epoch: 2745 loss_train: 0.7971 acc_train: 0.7432 loss_val: 0.5878 acc_val: 0.8150 time: 674.5343s\n",
            "Epoch: 2746 loss_train: 0.5625 acc_train: 0.8118 loss_val: 0.4881 acc_val: 0.8721 time: 674.7856s\n",
            "Epoch: 2747 loss_train: 0.4901 acc_train: 0.8566 loss_val: 0.7117 acc_val: 0.8054 time: 675.0169s\n",
            "Epoch: 2748 loss_train: 0.7459 acc_train: 0.7808 loss_val: 0.5175 acc_val: 0.8688 time: 675.2572s\n",
            "Epoch: 2749 loss_train: 0.5212 acc_train: 0.8469 loss_val: 0.5489 acc_val: 0.8383 time: 675.5055s\n",
            "Epoch: 2750 loss_train: 0.5511 acc_train: 0.8280 loss_val: 0.4747 acc_val: 0.8508 time: 675.7599s\n",
            "Epoch: 2751 loss_train: 0.4771 acc_train: 0.8456 loss_val: 0.5632 acc_val: 0.8275 time: 676.0229s\n",
            "Epoch: 2752 loss_train: 0.5945 acc_train: 0.8080 loss_val: 0.5194 acc_val: 0.8421 time: 676.2533s\n",
            "Epoch: 2753 loss_train: 0.5421 acc_train: 0.8254 loss_val: 0.4179 acc_val: 0.8800 time: 676.5063s\n",
            "Epoch: 2754 loss_train: 0.4105 acc_train: 0.8755 loss_val: 0.4943 acc_val: 0.8592 time: 676.7558s\n",
            "Epoch: 2755 loss_train: 0.4634 acc_train: 0.8609 loss_val: 0.5286 acc_val: 0.8567 time: 677.0307s\n",
            "Epoch: 2756 loss_train: 0.4908 acc_train: 0.8540 loss_val: 0.4671 acc_val: 0.8750 time: 677.2859s\n",
            "Epoch: 2757 loss_train: 0.4497 acc_train: 0.8637 loss_val: 0.4741 acc_val: 0.8712 time: 677.5398s\n",
            "Epoch: 2758 loss_train: 0.4742 acc_train: 0.8598 loss_val: 0.4641 acc_val: 0.8671 time: 677.7941s\n",
            "Epoch: 2759 loss_train: 0.4618 acc_train: 0.8570 loss_val: 0.4193 acc_val: 0.8788 time: 678.0584s\n",
            "Epoch: 2760 loss_train: 0.4105 acc_train: 0.8692 loss_val: 0.4523 acc_val: 0.8683 time: 678.3162s\n",
            "Epoch: 2761 loss_train: 0.4481 acc_train: 0.8601 loss_val: 0.4510 acc_val: 0.8700 time: 678.5760s\n",
            "Epoch: 2762 loss_train: 0.4570 acc_train: 0.8523 loss_val: 0.4315 acc_val: 0.8750 time: 678.8327s\n",
            "Epoch: 2763 loss_train: 0.4349 acc_train: 0.8671 loss_val: 0.4267 acc_val: 0.8742 time: 679.0952s\n",
            "Epoch: 2764 loss_train: 0.4289 acc_train: 0.8597 loss_val: 0.4261 acc_val: 0.8750 time: 679.3462s\n",
            "Epoch: 2765 loss_train: 0.4170 acc_train: 0.8616 loss_val: 0.4159 acc_val: 0.8817 time: 679.6170s\n",
            "Epoch: 2766 loss_train: 0.4037 acc_train: 0.8690 loss_val: 0.4180 acc_val: 0.8875 time: 679.8744s\n",
            "Epoch: 2767 loss_train: 0.4101 acc_train: 0.8773 loss_val: 0.4266 acc_val: 0.8842 time: 680.1474s\n",
            "Epoch: 2768 loss_train: 0.4194 acc_train: 0.8764 loss_val: 0.4176 acc_val: 0.8879 time: 680.4027s\n",
            "Epoch: 2769 loss_train: 0.4076 acc_train: 0.8834 loss_val: 0.4133 acc_val: 0.8888 time: 680.6350s\n",
            "Epoch: 2770 loss_train: 0.4084 acc_train: 0.8753 loss_val: 0.4070 acc_val: 0.8875 time: 680.8343s\n",
            "Epoch: 2771 loss_train: 0.3937 acc_train: 0.8769 loss_val: 0.3952 acc_val: 0.8875 time: 681.1003s\n",
            "Epoch: 2772 loss_train: 0.3857 acc_train: 0.8760 loss_val: 0.3916 acc_val: 0.8879 time: 681.3562s\n",
            "Epoch: 2773 loss_train: 0.3976 acc_train: 0.8742 loss_val: 0.4001 acc_val: 0.8871 time: 681.6114s\n",
            "Epoch: 2774 loss_train: 0.3992 acc_train: 0.8746 loss_val: 0.3944 acc_val: 0.8888 time: 681.8876s\n",
            "Epoch: 2775 loss_train: 0.3871 acc_train: 0.8798 loss_val: 0.3872 acc_val: 0.8896 time: 682.1668s\n",
            "Epoch: 2776 loss_train: 0.3923 acc_train: 0.8746 loss_val: 0.3891 acc_val: 0.8908 time: 682.4283s\n",
            "Epoch: 2777 loss_train: 0.3915 acc_train: 0.8784 loss_val: 0.3910 acc_val: 0.8933 time: 682.6907s\n",
            "Epoch: 2778 loss_train: 0.3820 acc_train: 0.8835 loss_val: 0.3896 acc_val: 0.8938 time: 682.9239s\n",
            "Epoch: 2779 loss_train: 0.3814 acc_train: 0.8863 loss_val: 0.3894 acc_val: 0.8933 time: 683.1883s\n",
            "Epoch: 2780 loss_train: 0.3771 acc_train: 0.8846 loss_val: 0.3857 acc_val: 0.8929 time: 683.4224s\n",
            "Epoch: 2781 loss_train: 0.3788 acc_train: 0.8824 loss_val: 0.3843 acc_val: 0.8942 time: 683.6791s\n",
            "Epoch: 2782 loss_train: 0.3776 acc_train: 0.8811 loss_val: 0.3776 acc_val: 0.8946 time: 683.9154s\n",
            "Epoch: 2783 loss_train: 0.3731 acc_train: 0.8852 loss_val: 0.3787 acc_val: 0.8962 time: 684.1495s\n",
            "Epoch: 2784 loss_train: 0.3699 acc_train: 0.8871 loss_val: 0.3794 acc_val: 0.8954 time: 684.4016s\n",
            "Epoch: 2785 loss_train: 0.3783 acc_train: 0.8812 loss_val: 0.3764 acc_val: 0.8942 time: 684.6593s\n",
            "Epoch: 2786 loss_train: 0.3688 acc_train: 0.8848 loss_val: 0.3739 acc_val: 0.8954 time: 684.8713s\n",
            "Epoch: 2787 loss_train: 0.3725 acc_train: 0.8812 loss_val: 0.3757 acc_val: 0.8967 time: 685.1231s\n",
            "Epoch: 2788 loss_train: 0.3691 acc_train: 0.8849 loss_val: 0.3798 acc_val: 0.8929 time: 685.3804s\n",
            "Epoch: 2789 loss_train: 0.3655 acc_train: 0.8876 loss_val: 0.3795 acc_val: 0.8946 time: 685.6371s\n",
            "Epoch: 2790 loss_train: 0.3567 acc_train: 0.8866 loss_val: 0.3746 acc_val: 0.8954 time: 685.8920s\n",
            "Epoch: 2791 loss_train: 0.3633 acc_train: 0.8884 loss_val: 0.3734 acc_val: 0.8958 time: 686.1579s\n",
            "Epoch: 2792 loss_train: 0.3627 acc_train: 0.8879 loss_val: 0.3761 acc_val: 0.8929 time: 686.4191s\n",
            "Epoch: 2793 loss_train: 0.3641 acc_train: 0.8838 loss_val: 0.3779 acc_val: 0.8929 time: 686.6892s\n",
            "Epoch: 2794 loss_train: 0.3662 acc_train: 0.8854 loss_val: 0.3701 acc_val: 0.8971 time: 686.9365s\n",
            "Epoch: 2795 loss_train: 0.3705 acc_train: 0.8848 loss_val: 0.3706 acc_val: 0.8983 time: 687.2084s\n",
            "Epoch: 2796 loss_train: 0.3773 acc_train: 0.8848 loss_val: 0.3865 acc_val: 0.8883 time: 687.4683s\n",
            "Epoch: 2797 loss_train: 0.3804 acc_train: 0.8775 loss_val: 0.3723 acc_val: 0.8962 time: 687.7212s\n",
            "Epoch: 2798 loss_train: 0.3676 acc_train: 0.8865 loss_val: 0.3718 acc_val: 0.8946 time: 687.9874s\n",
            "Epoch: 2799 loss_train: 0.3690 acc_train: 0.8876 loss_val: 0.3779 acc_val: 0.8908 time: 688.2483s\n",
            "Epoch: 2800 loss_train: 0.3630 acc_train: 0.8860 loss_val: 0.3778 acc_val: 0.8900 time: 688.5041s\n",
            "Epoch: 2801 loss_train: 0.3670 acc_train: 0.8832 loss_val: 0.3695 acc_val: 0.8954 time: 688.7628s\n",
            "Epoch: 2802 loss_train: 0.3697 acc_train: 0.8826 loss_val: 0.3700 acc_val: 0.8958 time: 689.0205s\n",
            "Epoch: 2803 loss_train: 0.3664 acc_train: 0.8862 loss_val: 0.3816 acc_val: 0.8896 time: 689.2866s\n",
            "Epoch: 2804 loss_train: 0.3665 acc_train: 0.8804 loss_val: 0.3814 acc_val: 0.8879 time: 689.5391s\n",
            "Epoch: 2805 loss_train: 0.3621 acc_train: 0.8849 loss_val: 0.3706 acc_val: 0.8962 time: 689.7890s\n",
            "Epoch: 2806 loss_train: 0.3632 acc_train: 0.8871 loss_val: 0.3674 acc_val: 0.8942 time: 690.0559s\n",
            "Epoch: 2807 loss_train: 0.3465 acc_train: 0.8903 loss_val: 0.3846 acc_val: 0.8879 time: 690.3195s\n",
            "Epoch: 2808 loss_train: 0.3695 acc_train: 0.8818 loss_val: 0.3785 acc_val: 0.8908 time: 690.5915s\n",
            "Epoch: 2809 loss_train: 0.3748 acc_train: 0.8765 loss_val: 0.3743 acc_val: 0.8933 time: 690.8571s\n",
            "Epoch: 2810 loss_train: 0.3700 acc_train: 0.8865 loss_val: 0.3654 acc_val: 0.8962 time: 691.1164s\n",
            "Epoch: 2811 loss_train: 0.3616 acc_train: 0.8871 loss_val: 0.3926 acc_val: 0.8825 time: 691.3740s\n",
            "Epoch: 2812 loss_train: 0.3731 acc_train: 0.8797 loss_val: 0.3812 acc_val: 0.8900 time: 691.6416s\n",
            "Epoch: 2813 loss_train: 0.3574 acc_train: 0.8855 loss_val: 0.3696 acc_val: 0.8954 time: 691.8976s\n",
            "Epoch: 2814 loss_train: 0.3676 acc_train: 0.8870 loss_val: 0.3682 acc_val: 0.8950 time: 692.1577s\n",
            "Epoch: 2815 loss_train: 0.3601 acc_train: 0.8889 loss_val: 0.3820 acc_val: 0.8879 time: 692.4376s\n",
            "Epoch: 2816 loss_train: 0.3616 acc_train: 0.8849 loss_val: 0.3739 acc_val: 0.8917 time: 692.7133s\n",
            "Epoch: 2817 loss_train: 0.3675 acc_train: 0.8823 loss_val: 0.3665 acc_val: 0.8942 time: 692.9699s\n",
            "Epoch: 2818 loss_train: 0.3725 acc_train: 0.8866 loss_val: 0.3646 acc_val: 0.8942 time: 693.2261s\n",
            "Epoch: 2819 loss_train: 0.3597 acc_train: 0.8901 loss_val: 0.3807 acc_val: 0.8908 time: 693.4822s\n",
            "Epoch: 2820 loss_train: 0.3820 acc_train: 0.8776 loss_val: 0.3682 acc_val: 0.8958 time: 693.7328s\n",
            "Epoch: 2821 loss_train: 0.3578 acc_train: 0.8853 loss_val: 0.3699 acc_val: 0.8958 time: 693.9908s\n",
            "Epoch: 2822 loss_train: 0.3765 acc_train: 0.8858 loss_val: 0.3673 acc_val: 0.8958 time: 694.2474s\n",
            "Epoch: 2823 loss_train: 0.3580 acc_train: 0.8881 loss_val: 0.3849 acc_val: 0.8888 time: 694.5031s\n",
            "Epoch: 2824 loss_train: 0.3646 acc_train: 0.8834 loss_val: 0.3657 acc_val: 0.8971 time: 694.7681s\n",
            "Epoch: 2825 loss_train: 0.3558 acc_train: 0.8836 loss_val: 0.3706 acc_val: 0.8954 time: 695.0258s\n",
            "Epoch: 2826 loss_train: 0.3879 acc_train: 0.8781 loss_val: 0.3847 acc_val: 0.8842 time: 695.2819s\n",
            "Epoch: 2827 loss_train: 0.3668 acc_train: 0.8815 loss_val: 0.3973 acc_val: 0.8808 time: 695.5349s\n",
            "Epoch: 2828 loss_train: 0.3793 acc_train: 0.8738 loss_val: 0.3679 acc_val: 0.8958 time: 695.7920s\n",
            "Epoch: 2829 loss_train: 0.3672 acc_train: 0.8887 loss_val: 0.3697 acc_val: 0.8942 time: 696.0505s\n",
            "Epoch: 2830 loss_train: 0.3705 acc_train: 0.8869 loss_val: 0.3856 acc_val: 0.8900 time: 696.3136s\n",
            "Epoch: 2831 loss_train: 0.3723 acc_train: 0.8780 loss_val: 0.3873 acc_val: 0.8871 time: 696.5714s\n",
            "Epoch: 2832 loss_train: 0.3668 acc_train: 0.8793 loss_val: 0.3687 acc_val: 0.8958 time: 696.8286s\n",
            "Epoch: 2833 loss_train: 0.3575 acc_train: 0.8902 loss_val: 0.3659 acc_val: 0.8950 time: 697.0866s\n",
            "Epoch: 2834 loss_train: 0.3666 acc_train: 0.8865 loss_val: 0.3796 acc_val: 0.8904 time: 697.3595s\n",
            "Epoch: 2835 loss_train: 0.3553 acc_train: 0.8851 loss_val: 0.3901 acc_val: 0.8858 time: 697.6179s\n",
            "Epoch: 2836 loss_train: 0.3793 acc_train: 0.8756 loss_val: 0.3657 acc_val: 0.8950 time: 697.8577s\n",
            "Epoch: 2837 loss_train: 0.3670 acc_train: 0.8858 loss_val: 0.3668 acc_val: 0.8938 time: 698.1191s\n",
            "Epoch: 2838 loss_train: 0.3618 acc_train: 0.8904 loss_val: 0.3788 acc_val: 0.8900 time: 698.3766s\n",
            "Epoch: 2839 loss_train: 0.3667 acc_train: 0.8834 loss_val: 0.3724 acc_val: 0.8929 time: 698.6325s\n",
            "Epoch: 2840 loss_train: 0.3520 acc_train: 0.8865 loss_val: 0.3670 acc_val: 0.8921 time: 698.8965s\n",
            "Epoch: 2841 loss_train: 0.3693 acc_train: 0.8877 loss_val: 0.3641 acc_val: 0.8979 time: 699.1382s\n",
            "Epoch: 2842 loss_train: 0.3492 acc_train: 0.8877 loss_val: 0.3768 acc_val: 0.8938 time: 699.3732s\n",
            "Epoch: 2843 loss_train: 0.3635 acc_train: 0.8840 loss_val: 0.3706 acc_val: 0.8946 time: 699.6286s\n",
            "Epoch: 2844 loss_train: 0.3631 acc_train: 0.8823 loss_val: 0.3684 acc_val: 0.8938 time: 699.8800s\n",
            "Epoch: 2845 loss_train: 0.3729 acc_train: 0.8848 loss_val: 0.3652 acc_val: 0.8950 time: 700.1353s\n",
            "Epoch: 2846 loss_train: 0.3616 acc_train: 0.8841 loss_val: 0.3848 acc_val: 0.8875 time: 700.3942s\n",
            "Epoch: 2847 loss_train: 0.3750 acc_train: 0.8776 loss_val: 0.3656 acc_val: 0.8971 time: 700.6500s\n",
            "Epoch: 2848 loss_train: 0.3577 acc_train: 0.8898 loss_val: 0.3780 acc_val: 0.8958 time: 700.9076s\n",
            "Epoch: 2849 loss_train: 0.3705 acc_train: 0.8900 loss_val: 0.3771 acc_val: 0.8921 time: 701.1634s\n",
            "Epoch: 2850 loss_train: 0.3613 acc_train: 0.8855 loss_val: 0.3922 acc_val: 0.8850 time: 701.4225s\n",
            "Epoch: 2851 loss_train: 0.3787 acc_train: 0.8798 loss_val: 0.3671 acc_val: 0.8954 time: 701.6823s\n",
            "Epoch: 2852 loss_train: 0.3676 acc_train: 0.8803 loss_val: 0.3666 acc_val: 0.8958 time: 701.9458s\n",
            "Epoch: 2853 loss_train: 0.3698 acc_train: 0.8857 loss_val: 0.3713 acc_val: 0.8967 time: 702.2198s\n",
            "Epoch: 2854 loss_train: 0.3568 acc_train: 0.8871 loss_val: 0.3787 acc_val: 0.8917 time: 702.4751s\n",
            "Epoch: 2855 loss_train: 0.3680 acc_train: 0.8816 loss_val: 0.3627 acc_val: 0.8938 time: 702.7499s\n",
            "Epoch: 2856 loss_train: 0.3590 acc_train: 0.8899 loss_val: 0.3648 acc_val: 0.8942 time: 703.0115s\n",
            "Epoch: 2857 loss_train: 0.3519 acc_train: 0.8895 loss_val: 0.3684 acc_val: 0.8942 time: 703.2712s\n",
            "Epoch: 2858 loss_train: 0.3549 acc_train: 0.8887 loss_val: 0.3650 acc_val: 0.8950 time: 703.5372s\n",
            "Epoch: 2859 loss_train: 0.3511 acc_train: 0.8858 loss_val: 0.3642 acc_val: 0.8962 time: 703.7931s\n",
            "Epoch: 2860 loss_train: 0.3420 acc_train: 0.8896 loss_val: 0.3619 acc_val: 0.8958 time: 704.0571s\n",
            "Epoch: 2861 loss_train: 0.3556 acc_train: 0.8876 loss_val: 0.3663 acc_val: 0.8954 time: 704.3144s\n",
            "Epoch: 2862 loss_train: 0.3567 acc_train: 0.8848 loss_val: 0.3663 acc_val: 0.8967 time: 704.5725s\n",
            "Epoch: 2863 loss_train: 0.3443 acc_train: 0.8900 loss_val: 0.3647 acc_val: 0.8962 time: 704.8197s\n",
            "Epoch: 2864 loss_train: 0.3451 acc_train: 0.8914 loss_val: 0.3696 acc_val: 0.8942 time: 705.0721s\n",
            "Epoch: 2865 loss_train: 0.3557 acc_train: 0.8852 loss_val: 0.3653 acc_val: 0.8946 time: 705.3303s\n",
            "Epoch: 2866 loss_train: 0.3596 acc_train: 0.8879 loss_val: 0.3629 acc_val: 0.8962 time: 705.5877s\n",
            "Epoch: 2867 loss_train: 0.3507 acc_train: 0.8932 loss_val: 0.3652 acc_val: 0.8950 time: 705.8485s\n",
            "Epoch: 2868 loss_train: 0.3434 acc_train: 0.8890 loss_val: 0.3662 acc_val: 0.8929 time: 706.0983s\n",
            "Epoch: 2869 loss_train: 0.3560 acc_train: 0.8870 loss_val: 0.3628 acc_val: 0.8967 time: 706.3272s\n",
            "Epoch: 2870 loss_train: 0.3515 acc_train: 0.8879 loss_val: 0.3620 acc_val: 0.8954 time: 706.5840s\n",
            "Epoch: 2871 loss_train: 0.3518 acc_train: 0.8870 loss_val: 0.3659 acc_val: 0.8933 time: 706.8425s\n",
            "Epoch: 2872 loss_train: 0.3525 acc_train: 0.8868 loss_val: 0.3626 acc_val: 0.8954 time: 707.0965s\n",
            "Epoch: 2873 loss_train: 0.3473 acc_train: 0.8902 loss_val: 0.3635 acc_val: 0.8971 time: 707.3459s\n",
            "Epoch: 2874 loss_train: 0.3583 acc_train: 0.8895 loss_val: 0.3713 acc_val: 0.8929 time: 707.6076s\n",
            "Epoch: 2875 loss_train: 0.3482 acc_train: 0.8890 loss_val: 0.3756 acc_val: 0.8912 time: 707.8420s\n",
            "Epoch: 2876 loss_train: 0.3630 acc_train: 0.8824 loss_val: 0.3635 acc_val: 0.8946 time: 708.1040s\n",
            "Epoch: 2877 loss_train: 0.3540 acc_train: 0.8899 loss_val: 0.3613 acc_val: 0.8975 time: 708.3611s\n",
            "Epoch: 2878 loss_train: 0.3582 acc_train: 0.8875 loss_val: 0.3726 acc_val: 0.8933 time: 708.6240s\n",
            "Epoch: 2879 loss_train: 0.3576 acc_train: 0.8804 loss_val: 0.3696 acc_val: 0.8942 time: 708.8767s\n",
            "Epoch: 2880 loss_train: 0.3530 acc_train: 0.8878 loss_val: 0.3705 acc_val: 0.8954 time: 709.1290s\n",
            "Epoch: 2881 loss_train: 0.3589 acc_train: 0.8919 loss_val: 0.3601 acc_val: 0.8979 time: 709.3898s\n",
            "Epoch: 2882 loss_train: 0.3496 acc_train: 0.8931 loss_val: 0.3846 acc_val: 0.8850 time: 709.6375s\n",
            "Epoch: 2883 loss_train: 0.3634 acc_train: 0.8809 loss_val: 0.3611 acc_val: 0.8967 time: 709.8894s\n",
            "Epoch: 2884 loss_train: 0.3391 acc_train: 0.8915 loss_val: 0.3642 acc_val: 0.8954 time: 710.1393s\n",
            "Epoch: 2885 loss_train: 0.3506 acc_train: 0.8955 loss_val: 0.3662 acc_val: 0.8958 time: 710.3898s\n",
            "Epoch: 2886 loss_train: 0.3491 acc_train: 0.8848 loss_val: 0.3690 acc_val: 0.8946 time: 710.5798s\n",
            "Epoch: 2887 loss_train: 0.3557 acc_train: 0.8807 loss_val: 0.3583 acc_val: 0.8971 time: 710.8409s\n",
            "Epoch: 2888 loss_train: 0.3486 acc_train: 0.8937 loss_val: 0.3585 acc_val: 0.8992 time: 711.1068s\n",
            "Epoch: 2889 loss_train: 0.3502 acc_train: 0.8932 loss_val: 0.3701 acc_val: 0.8933 time: 711.3617s\n",
            "Epoch: 2890 loss_train: 0.3466 acc_train: 0.8890 loss_val: 0.3624 acc_val: 0.8983 time: 711.6263s\n",
            "Epoch: 2891 loss_train: 0.3449 acc_train: 0.8881 loss_val: 0.3638 acc_val: 0.8954 time: 711.8821s\n",
            "Epoch: 2892 loss_train: 0.3471 acc_train: 0.8913 loss_val: 0.3593 acc_val: 0.8983 time: 712.1449s\n",
            "Epoch: 2893 loss_train: 0.3562 acc_train: 0.8868 loss_val: 0.3730 acc_val: 0.8942 time: 712.3988s\n",
            "Epoch: 2894 loss_train: 0.3524 acc_train: 0.8851 loss_val: 0.3644 acc_val: 0.8946 time: 712.6572s\n",
            "Epoch: 2895 loss_train: 0.3525 acc_train: 0.8870 loss_val: 0.3654 acc_val: 0.8950 time: 712.9088s\n",
            "Epoch: 2896 loss_train: 0.3642 acc_train: 0.8875 loss_val: 0.3685 acc_val: 0.8938 time: 713.1482s\n",
            "Epoch: 2897 loss_train: 0.3502 acc_train: 0.8870 loss_val: 0.3727 acc_val: 0.8933 time: 713.4161s\n",
            "Epoch: 2898 loss_train: 0.3474 acc_train: 0.8877 loss_val: 0.3625 acc_val: 0.8967 time: 713.6735s\n",
            "Epoch: 2899 loss_train: 0.3519 acc_train: 0.8936 loss_val: 0.3591 acc_val: 0.8962 time: 713.9283s\n",
            "Epoch: 2900 loss_train: 0.3609 acc_train: 0.8854 loss_val: 0.3804 acc_val: 0.8892 time: 714.1731s\n",
            "Epoch: 2901 loss_train: 0.3554 acc_train: 0.8837 loss_val: 0.3610 acc_val: 0.8971 time: 714.4233s\n",
            "Epoch: 2902 loss_train: 0.3472 acc_train: 0.8887 loss_val: 0.3636 acc_val: 0.8996 time: 714.6840s\n",
            "Epoch: 2903 loss_train: 0.3515 acc_train: 0.8930 loss_val: 0.3619 acc_val: 0.8954 time: 714.9431s\n",
            "Epoch: 2904 loss_train: 0.3548 acc_train: 0.8860 loss_val: 0.3628 acc_val: 0.8975 time: 715.1986s\n",
            "Epoch: 2905 loss_train: 0.3414 acc_train: 0.8862 loss_val: 0.3621 acc_val: 0.8958 time: 715.4519s\n",
            "Epoch: 2906 loss_train: 0.3422 acc_train: 0.8935 loss_val: 0.3585 acc_val: 0.8992 time: 715.7028s\n",
            "Epoch: 2907 loss_train: 0.3372 acc_train: 0.8943 loss_val: 0.3634 acc_val: 0.8992 time: 715.9691s\n",
            "Epoch: 2908 loss_train: 0.3394 acc_train: 0.8909 loss_val: 0.3625 acc_val: 0.9000 time: 716.2249s\n",
            "Epoch: 2909 loss_train: 0.3481 acc_train: 0.8891 loss_val: 0.3567 acc_val: 0.9017 time: 716.4780s\n",
            "Epoch: 2910 loss_train: 0.3405 acc_train: 0.8944 loss_val: 0.3597 acc_val: 0.9000 time: 716.7427s\n",
            "Epoch: 2911 loss_train: 0.3394 acc_train: 0.8947 loss_val: 0.3590 acc_val: 0.8979 time: 716.9984s\n",
            "Epoch: 2912 loss_train: 0.3491 acc_train: 0.8892 loss_val: 0.3604 acc_val: 0.8992 time: 717.2587s\n",
            "Epoch: 2913 loss_train: 0.3438 acc_train: 0.8911 loss_val: 0.3586 acc_val: 0.8979 time: 717.5107s\n",
            "Epoch: 2914 loss_train: 0.3513 acc_train: 0.8897 loss_val: 0.3601 acc_val: 0.8992 time: 717.7746s\n",
            "Epoch: 2915 loss_train: 0.3472 acc_train: 0.8901 loss_val: 0.3580 acc_val: 0.9000 time: 718.0343s\n",
            "Epoch: 2916 loss_train: 0.3375 acc_train: 0.8915 loss_val: 0.3573 acc_val: 0.9004 time: 718.3092s\n",
            "Epoch: 2917 loss_train: 0.3350 acc_train: 0.8965 loss_val: 0.3606 acc_val: 0.8975 time: 718.5685s\n",
            "Epoch: 2918 loss_train: 0.3420 acc_train: 0.8905 loss_val: 0.3576 acc_val: 0.8979 time: 718.8313s\n",
            "Epoch: 2919 loss_train: 0.3422 acc_train: 0.8922 loss_val: 0.3572 acc_val: 0.9004 time: 719.0508s\n",
            "Epoch: 2920 loss_train: 0.3421 acc_train: 0.8916 loss_val: 0.3571 acc_val: 0.9000 time: 719.3090s\n",
            "Epoch: 2921 loss_train: 0.3434 acc_train: 0.8926 loss_val: 0.3567 acc_val: 0.9000 time: 719.5399s\n",
            "Epoch: 2922 loss_train: 0.3429 acc_train: 0.8930 loss_val: 0.3574 acc_val: 0.9000 time: 719.7959s\n",
            "Epoch: 2923 loss_train: 0.3340 acc_train: 0.8935 loss_val: 0.3594 acc_val: 0.8988 time: 720.0372s\n",
            "Epoch: 2924 loss_train: 0.3405 acc_train: 0.8926 loss_val: 0.3569 acc_val: 0.9004 time: 720.2937s\n",
            "Epoch: 2925 loss_train: 0.3456 acc_train: 0.8953 loss_val: 0.3570 acc_val: 0.9012 time: 720.5227s\n",
            "Epoch: 2926 loss_train: 0.3450 acc_train: 0.8896 loss_val: 0.3558 acc_val: 0.9012 time: 720.7617s\n",
            "Epoch: 2927 loss_train: 0.3450 acc_train: 0.8945 loss_val: 0.3589 acc_val: 0.9004 time: 721.0106s\n",
            "Epoch: 2928 loss_train: 0.3391 acc_train: 0.8903 loss_val: 0.3617 acc_val: 0.8979 time: 721.2771s\n",
            "Epoch: 2929 loss_train: 0.3450 acc_train: 0.8893 loss_val: 0.3577 acc_val: 0.8996 time: 721.5369s\n",
            "Epoch: 2930 loss_train: 0.3314 acc_train: 0.8915 loss_val: 0.3556 acc_val: 0.8992 time: 721.8098s\n",
            "Epoch: 2931 loss_train: 0.3414 acc_train: 0.8901 loss_val: 0.3576 acc_val: 0.9008 time: 722.0624s\n",
            "Epoch: 2932 loss_train: 0.3451 acc_train: 0.8888 loss_val: 0.3561 acc_val: 0.9008 time: 722.3331s\n",
            "Epoch: 2933 loss_train: 0.3412 acc_train: 0.8959 loss_val: 0.3574 acc_val: 0.9000 time: 722.6056s\n",
            "Epoch: 2934 loss_train: 0.3413 acc_train: 0.8926 loss_val: 0.3597 acc_val: 0.8988 time: 722.8701s\n",
            "Epoch: 2935 loss_train: 0.3430 acc_train: 0.8868 loss_val: 0.3555 acc_val: 0.9000 time: 723.1284s\n",
            "Epoch: 2936 loss_train: 0.3357 acc_train: 0.8935 loss_val: 0.3560 acc_val: 0.8992 time: 723.3944s\n",
            "Epoch: 2937 loss_train: 0.3384 acc_train: 0.8931 loss_val: 0.3634 acc_val: 0.8967 time: 723.6603s\n",
            "Epoch: 2938 loss_train: 0.3465 acc_train: 0.8921 loss_val: 0.3579 acc_val: 0.8983 time: 723.9200s\n",
            "Epoch: 2939 loss_train: 0.3375 acc_train: 0.8942 loss_val: 0.3544 acc_val: 0.9008 time: 724.1810s\n",
            "Epoch: 2940 loss_train: 0.3394 acc_train: 0.8954 loss_val: 0.3645 acc_val: 0.8950 time: 724.4338s\n",
            "Epoch: 2941 loss_train: 0.3501 acc_train: 0.8858 loss_val: 0.3561 acc_val: 0.8992 time: 724.6845s\n",
            "Epoch: 2942 loss_train: 0.3458 acc_train: 0.8930 loss_val: 0.3580 acc_val: 0.8988 time: 724.9411s\n",
            "Epoch: 2943 loss_train: 0.3456 acc_train: 0.8957 loss_val: 0.3640 acc_val: 0.8954 time: 725.1708s\n",
            "Epoch: 2944 loss_train: 0.3398 acc_train: 0.8885 loss_val: 0.3576 acc_val: 0.8992 time: 725.4233s\n",
            "Epoch: 2945 loss_train: 0.3444 acc_train: 0.8910 loss_val: 0.3543 acc_val: 0.8958 time: 725.6774s\n",
            "Epoch: 2946 loss_train: 0.3280 acc_train: 0.8986 loss_val: 0.3568 acc_val: 0.8988 time: 725.9386s\n",
            "Epoch: 2947 loss_train: 0.3390 acc_train: 0.8931 loss_val: 0.3592 acc_val: 0.8971 time: 726.1995s\n",
            "Epoch: 2948 loss_train: 0.3344 acc_train: 0.8932 loss_val: 0.3540 acc_val: 0.9004 time: 726.4548s\n",
            "Epoch: 2949 loss_train: 0.3388 acc_train: 0.8960 loss_val: 0.3569 acc_val: 0.9008 time: 726.7082s\n",
            "Epoch: 2950 loss_train: 0.3405 acc_train: 0.8885 loss_val: 0.3573 acc_val: 0.8988 time: 726.9570s\n",
            "Epoch: 2951 loss_train: 0.3372 acc_train: 0.8948 loss_val: 0.3608 acc_val: 0.8954 time: 727.2089s\n",
            "Epoch: 2952 loss_train: 0.3396 acc_train: 0.8909 loss_val: 0.3555 acc_val: 0.8983 time: 727.4672s\n",
            "Epoch: 2953 loss_train: 0.3334 acc_train: 0.8931 loss_val: 0.3539 acc_val: 0.9008 time: 727.7269s\n",
            "Epoch: 2954 loss_train: 0.3353 acc_train: 0.8947 loss_val: 0.3540 acc_val: 0.9029 time: 727.9859s\n",
            "Epoch: 2955 loss_train: 0.3384 acc_train: 0.8923 loss_val: 0.3543 acc_val: 0.8988 time: 728.2393s\n",
            "Epoch: 2956 loss_train: 0.3371 acc_train: 0.8914 loss_val: 0.3558 acc_val: 0.9012 time: 728.4966s\n",
            "Epoch: 2957 loss_train: 0.3325 acc_train: 0.8929 loss_val: 0.3573 acc_val: 0.8979 time: 728.7689s\n",
            "Epoch: 2958 loss_train: 0.3386 acc_train: 0.8886 loss_val: 0.3525 acc_val: 0.9021 time: 729.0176s\n",
            "Epoch: 2959 loss_train: 0.3286 acc_train: 0.8953 loss_val: 0.3539 acc_val: 0.9004 time: 729.2683s\n",
            "Epoch: 2960 loss_train: 0.3398 acc_train: 0.8932 loss_val: 0.3605 acc_val: 0.8967 time: 729.5159s\n",
            "Epoch: 2961 loss_train: 0.3340 acc_train: 0.8878 loss_val: 0.3545 acc_val: 0.8996 time: 729.7825s\n",
            "Epoch: 2962 loss_train: 0.3417 acc_train: 0.8943 loss_val: 0.3527 acc_val: 0.9017 time: 730.0257s\n",
            "Epoch: 2963 loss_train: 0.3360 acc_train: 0.8968 loss_val: 0.3608 acc_val: 0.8958 time: 730.2786s\n",
            "Epoch: 2964 loss_train: 0.3410 acc_train: 0.8886 loss_val: 0.3574 acc_val: 0.9025 time: 730.5281s\n",
            "Epoch: 2965 loss_train: 0.3485 acc_train: 0.8901 loss_val: 0.3573 acc_val: 0.8971 time: 730.7949s\n",
            "Epoch: 2966 loss_train: 0.3449 acc_train: 0.8949 loss_val: 0.3575 acc_val: 0.8996 time: 731.0503s\n",
            "Epoch: 2967 loss_train: 0.3398 acc_train: 0.8911 loss_val: 0.3584 acc_val: 0.8988 time: 731.3000s\n",
            "Epoch: 2968 loss_train: 0.3281 acc_train: 0.8937 loss_val: 0.3528 acc_val: 0.9017 time: 731.5505s\n",
            "Epoch: 2969 loss_train: 0.3430 acc_train: 0.8935 loss_val: 0.3549 acc_val: 0.9000 time: 731.8341s\n",
            "Epoch: 2970 loss_train: 0.3390 acc_train: 0.8940 loss_val: 0.3576 acc_val: 0.8967 time: 732.0908s\n",
            "Epoch: 2971 loss_train: 0.3335 acc_train: 0.8944 loss_val: 0.3534 acc_val: 0.9004 time: 732.3484s\n",
            "Epoch: 2972 loss_train: 0.3360 acc_train: 0.8932 loss_val: 0.3526 acc_val: 0.9000 time: 732.6046s\n",
            "Epoch: 2973 loss_train: 0.3355 acc_train: 0.8946 loss_val: 0.3576 acc_val: 0.8992 time: 732.8563s\n",
            "Epoch: 2974 loss_train: 0.3344 acc_train: 0.8953 loss_val: 0.3556 acc_val: 0.8996 time: 733.1103s\n",
            "Epoch: 2975 loss_train: 0.3433 acc_train: 0.8870 loss_val: 0.3559 acc_val: 0.8996 time: 733.3547s\n",
            "Epoch: 2976 loss_train: 0.3463 acc_train: 0.8931 loss_val: 0.3547 acc_val: 0.8979 time: 733.6173s\n",
            "Epoch: 2977 loss_train: 0.3386 acc_train: 0.8900 loss_val: 0.3613 acc_val: 0.8958 time: 733.8919s\n",
            "Epoch: 2978 loss_train: 0.3456 acc_train: 0.8844 loss_val: 0.3530 acc_val: 0.9017 time: 734.1584s\n",
            "Epoch: 2979 loss_train: 0.3374 acc_train: 0.8932 loss_val: 0.3545 acc_val: 0.9004 time: 734.4092s\n",
            "Epoch: 2980 loss_train: 0.3460 acc_train: 0.8927 loss_val: 0.3620 acc_val: 0.8962 time: 734.6632s\n",
            "Epoch: 2981 loss_train: 0.3405 acc_train: 0.8925 loss_val: 0.3608 acc_val: 0.8967 time: 734.9376s\n",
            "Epoch: 2982 loss_train: 0.3249 acc_train: 0.8968 loss_val: 0.3534 acc_val: 0.9008 time: 735.1961s\n",
            "Epoch: 2983 loss_train: 0.3401 acc_train: 0.8905 loss_val: 0.3533 acc_val: 0.9029 time: 735.4451s\n",
            "Epoch: 2984 loss_train: 0.3299 acc_train: 0.8931 loss_val: 0.3538 acc_val: 0.9004 time: 735.6967s\n",
            "Epoch: 2985 loss_train: 0.3396 acc_train: 0.8932 loss_val: 0.3573 acc_val: 0.8967 time: 735.9554s\n",
            "Epoch: 2986 loss_train: 0.3363 acc_train: 0.8909 loss_val: 0.3571 acc_val: 0.9004 time: 736.2260s\n",
            "Epoch: 2987 loss_train: 0.3392 acc_train: 0.8920 loss_val: 0.3521 acc_val: 0.8992 time: 736.4752s\n",
            "Epoch: 2988 loss_train: 0.3337 acc_train: 0.8918 loss_val: 0.3513 acc_val: 0.9025 time: 736.7233s\n",
            "Epoch: 2989 loss_train: 0.3385 acc_train: 0.8942 loss_val: 0.3540 acc_val: 0.8992 time: 736.9680s\n",
            "Epoch: 2990 loss_train: 0.3464 acc_train: 0.8964 loss_val: 0.3602 acc_val: 0.8967 time: 737.2210s\n",
            "Epoch: 2991 loss_train: 0.3442 acc_train: 0.8896 loss_val: 0.3507 acc_val: 0.9012 time: 737.4789s\n",
            "Epoch: 2992 loss_train: 0.3428 acc_train: 0.8910 loss_val: 0.3576 acc_val: 0.8975 time: 737.7311s\n",
            "Epoch: 2993 loss_train: 0.3310 acc_train: 0.8957 loss_val: 0.3541 acc_val: 0.8996 time: 737.9869s\n",
            "Epoch: 2994 loss_train: 0.3308 acc_train: 0.8947 loss_val: 0.3551 acc_val: 0.9000 time: 738.2483s\n",
            "Epoch: 2995 loss_train: 0.3383 acc_train: 0.8964 loss_val: 0.3535 acc_val: 0.9012 time: 738.4985s\n",
            "Epoch: 2996 loss_train: 0.3463 acc_train: 0.8871 loss_val: 0.3528 acc_val: 0.9029 time: 738.7699s\n",
            "Epoch: 2997 loss_train: 0.3506 acc_train: 0.8921 loss_val: 0.3545 acc_val: 0.9000 time: 739.0335s\n",
            "Epoch: 2998 loss_train: 0.3361 acc_train: 0.8897 loss_val: 0.3566 acc_val: 0.9008 time: 739.2908s\n",
            "Epoch: 2999 loss_train: 0.3387 acc_train: 0.8959 loss_val: 0.3546 acc_val: 0.8988 time: 739.5480s\n",
            "Epoch: 3000 loss_train: 0.3363 acc_train: 0.8926 loss_val: 0.3712 acc_val: 0.8900 time: 739.8088s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 740.2010s\n",
            "GCN(\n",
            "  (gc1): GraphConvolution (767 -> 16) \n",
            "  (gc2): GraphConvolution (16 -> 10) \n",
            ")\n",
            "Test set results: loss= 0.3854 accuracy= 0.8809\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}